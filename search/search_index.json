{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"GeospaNN - Neural networks for geospatial data","text":"<p>Authors: Wentao Zhan (wzhan3@jhu.edu), Abhirup Datta (abhidatta@jhu.edu)</p> <p>A package based on the paper: Neural networks for geospatial data</p> <p>GeospaNN is a formal implementation of NN-GLS, the Neural Networks for geospatial data proposed in Zhan et.al (2023), that explicitly accounts for spatial correlation in the data. The package is developed using PyTorch and under the framework of PyG library. NN-GLS is a geographically-informed Graph Neural Network (GNN) for analyzing large and irregular geospatial data, that combines multi-layer perceptrons, Gaussian processes, and generalized least squares (GLS) loss. NN-GLS offers both regression function estimation and spatial prediction, and can scale up to sample sizes of hundreds of thousands. A  vignette is available at https://github.com/WentaoZhan1998/geospaNN/blob/main/vignette/vignette.pdf. Users are welcome to provide any helpful suggestions and comments.</p>"},{"location":"#contents","title":"Contents","text":"<p>This documentation provides a comprehensive guide to getting started with the project and understanding its features. Overview provides a detailed description to the package. How to Start contains instructions for installation, setup, and an easy running example. Documentation dives into the details of each module, explaining their functions and configurations. Finally, Examples gives practical applications and demonstrations of the package in action.</p> <ol> <li>Overview</li> <li>How to start</li> <li>Documentation</li> <li>Examples</li> </ol>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>Acknowledgement: This work was partially supported by the National Institute of Environmental Health Sciences (NIEHS) under grant R01 ES033739.</p>"},{"location":"Example_linear/","title":"Example linear","text":"<pre><code>%load_ext autoreload\n%autoreload 2\n</code></pre> <pre><code>import rpy2.robjects as robjects\nfrom rpy2.robjects.packages import importr\nBRISC = importr('BRISC')\ndef BRISC_estimation(residual, X, coord):\n    residual_r = robjects.FloatVector(residual)\n    coord_r = robjects.FloatVector(coord.transpose().reshape(-1))\n    coord_r = robjects.r['matrix'](coord_r, ncol=2)\n\n    if X is None:\n        res = BRISC.BRISC_estimation(coord_r, residual_r)\n    else:\n        Xr = robjects.FloatVector(X.transpose().reshape(-1))\n        Xr = robjects.r['matrix'](Xr, ncol=X.shape[1])\n        res = BRISC.BRISC_estimation(coord_r, residual_r, Xr)\n\n    theta_hat = res[9]\n    beta = res[8]\n    beta = np.array(beta)\n    theta_hat = np.array(theta_hat)\n    phi = theta_hat[2]\n    tau_sq = theta_hat[1]\n    sigma_sq = theta_hat[0]\n    theta_hat[1] = phi\n    theta_hat[2] = tau_sq / sigma_sq\n\n    return beta, theta_hat\n</code></pre> <pre><code>import torch\nimport geospaNN\nimport numpy as np\nimport time\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n</code></pre> <pre><code>def f5(X): return (10*np.sin(np.pi*X[:,0]*X[:,1]) + 20*(X[:,2]-0.5)**2 + 10*X[:,3] +5*X[:,4])/6\ndef f1(X): return 5*X + 2\n\nsigma = 1\nphi = 3\ntau = 0.01\ntheta = torch.tensor([sigma, phi / np.sqrt(2), tau])\n\np = 1; funXY = f1\n\nn = 1000\nnn = 20\nbatch_size = 50\n\ntorch.manual_seed(2024)\nX, Y, coord, cov, corerr = geospaNN.Simulation(n, p, nn, funXY, theta, range=[0, 10])\n\nX, Y, coord, _ = geospaNN.spatial_order(X, Y, coord, method = 'max-min')\ndata = geospaNN.make_graph(X, Y, coord, nn)\n\ntorch.manual_seed(2024)\nnp.random.seed(0)\ndata_train, data_val, data_test = geospaNN.split_data(X, Y, coord, neighbor_size = nn, \n                                                   test_proportion = 0.2)\n</code></pre> <pre><code>torch.manual_seed(2024)\nmlp_nn = torch.nn.Sequential(\n    torch.nn.Linear(p, 1)\n)\nnn_model = geospaNN.nn_train(mlp_nn, lr =  0.01, min_delta = 0.001)\ntraining_log = nn_model.train(data_train, data_val, data_test)\ntheta0 = geospaNN.theta_update(torch.tensor([1, 1.5, 0.01]), mlp_nn(data_train.x).squeeze() - data_train.y, data_train.pos, neighbor_size = 20)\nmlp_nngls = torch.nn.Sequential(\n    torch.nn.Linear(p, 1),\n)\nmodel = geospaNN.nngls(p=p, neighbor_size=nn, coord_dimensions=2, mlp=mlp_nngls, theta=torch.tensor(theta0))\nnngls_model = geospaNN.nngls_train(model, lr =  0.01, min_delta = 0.001)\ntraining_log = nngls_model.train(data_train, data_val, data_test,\n                                 Update_init = 10, Update_step = 5)\n</code></pre> <pre><code>Theta updated from\n[1.   1.5  0.01]\nTheta updated from\n[1.2714003  1.89249914 0.11636965]\nto\n[8.28898276 0.19122626 0.11904745]\nTheta updated from\n[8.28898276 0.19122626 0.11904745]\nto\n[6.99168522 0.23228389 0.11687287]\nTheta updated from\n[6.99168522 0.23228389 0.11687287]\nto\n[6.12525639 0.27047986 0.10956298]\nTheta updated from\n[6.12525639 0.27047986 0.10956298]\nto\n[5.26639589 0.32092605 0.10179605]\nTheta updated from\n[5.26639589 0.32092605 0.10179605]\nto\n[4.45536997 0.38688547 0.09374227]\nTheta updated from\n[4.45536997 0.38688547 0.09374227]\nto\n[3.69556335 0.47626894 0.08536871]\nTheta updated from\n[3.69556335 0.47626894 0.08536871]\nto\n[3.02207465 0.59644983 0.07659992]\nTheta updated from\n[3.02207465 0.59644983 0.07659992]\nto\n[2.44732479 0.75884009 0.06718273]\nTheta updated from\n[2.44732479 0.75884009 0.06718273]\nto\n[1.99102496 0.96820214 0.0565409 ]\nTheta updated from\n[1.99102496 0.96820214 0.0565409 ]\nto\n[1.64774011 1.22312618 0.04477572]\nTheta updated from\n[1.64774011 1.22312618 0.04477572]\nto\n[1.41049225 1.49623259 0.03291362]\nTheta updated from\n[1.41049225 1.49623259 0.03291362]\nto\n[1.27416592 1.71684432 0.02340899]\nTheta updated from\n[1.27416592 1.71684432 0.02340899]\nto\n[1.21404291 1.83882309 0.0170868 ]\nTheta updated from\n[1.21404291 1.83882309 0.0170868 ]\nto\n[1.19619055 1.88436302 0.0131272 ]\nTheta updated from\n[1.19619055 1.88436302 0.0131272 ]\nto\n[1.19376909 1.89770586 0.01086118]\nEpoch 00081: reducing learning rate of group 0 to 5.0000e-03.\nINFO: Early stopping\nEnd at epoch84\n</code></pre> <pre><code>X_train = np.concatenate([data_train.x.detach().numpy(), data_val.x.detach().numpy()], axis = 0)\nn_train = X_train.shape[0]\nX_train_int = np.concatenate((X_train, np.repeat(1, n_train).reshape(n_train, 1)), axis=1)\nn_test = data_test.x.shape[0]\nX_test_int = torch.concatenate((data_test.x, torch.from_numpy(np.repeat(1, n_test).reshape(n_test, 1))), axis=1)\nY_train = np.concatenate([data_train.y.detach().numpy(), data_val.y.detach().numpy()], axis = 0)\ncoord_train = np.concatenate([data_train.pos.detach().numpy(), data_val.pos.detach().numpy()], axis = 0)\n</code></pre> <pre><code>torch.manual_seed(2024)\nnp.random.seed(2024)\nbeta, theta_hat_linear = BRISC_estimation(Y_train, X_train_int, coord_train)\ndef model_BRISC(X, edge_index = 0):\n    if isinstance(X, np.ndarray):\n        X = torch.from_numpy(X).float()\n    return(torch.matmul(X, torch.from_numpy(beta).float()))\ntheta_hat_linear\n</code></pre> <pre><code>---------------------------------------- \n    Ordering Coordinates \n----------------------------------------\n    Model description\n----------------------------------------\nBRISC model fit with 800 observations.\n\nNumber of covariates 2 (including intercept if specified).\n\nUsing the exponential spatial correlation model.\n\nUsing 15 nearest neighbors.\n\n\n\nSource not compiled with OpenMP support.\n----------------------------------------\n    Building neighbor index\n----------------------------------------\n    Performing optimization\n----------------------------------------\n    Processing optimizers\n----------------------------------------\n\n\n\n\n\narray([1.16544065, 1.85772445, 0.01273631])\n</code></pre> <pre><code>#### A one line function for prediction for nn-gls model\ntest_predict_nngls = model.predict(data_train, data_test)\n#### Prediction for BRISC linear model\nw_test = geospaNN.krig_pred(torch.from_numpy(Y_train) - model_BRISC(X_train_int), torch.from_numpy(coord_train), data_test.pos, theta_hat_linear)[0]\ntest_predict_BRISC = w_test + model_BRISC(X_test_int)\n\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\naxes[0].scatter(data_test.x.detach().numpy(), data_test.y.detach().numpy(), s = 1, label = 'data')\naxes[0].scatter(data_test.x.detach().numpy(), funXY(data_test.x.detach().numpy()), s = 1, label = 'f(x)')\naxes[0].scatter(data_test.x.detach().numpy(), mlp_nngls(data_test.x).detach().numpy(), s = 1, label = 'estimation')\naxes[0].scatter(data_test.x.detach().numpy(), model_BRISC(X_test_int).detach().numpy(), s = 1, label = 'BRISC')\n#axes[0].scatter(data_test.x.detach().numpy(), test_predict.detach().numpy(), s = 1, label = 'prediction')\naxes[0].set_xlabel('X', fontsize=15)\naxes[0].set_ylabel('Y', fontsize=15)\naxes[0].set_title('Estimation', fontsize=15)\naxes[0].legend(prop={'size': 15})\n\naxes[1].scatter(test_predict_nngls.detach().numpy(), data_test.y.detach().numpy(), s = 1, label = 'Truth vs prediction nngls')\naxes[1].scatter(test_predict_BRISC.detach().numpy(), data_test.y.detach().numpy(), s = 1, label = 'Truth vs prediction BRISC')\naxes[1].scatter(data_test.y.detach().numpy(), data_test.y.detach().numpy(), s = 1, label = 'reference')\naxes[1].set_xlabel(\"Prediction\", fontsize=15)\naxes[1].set_ylabel(\"Truth\", fontsize=15)\naxes[1].set_title('Prediction', fontsize=15)\naxes[1].legend(prop={'size': 15})\nplt.tight_layout()\n</code></pre> <pre><code>epoch = len(training_log[\"val_loss\"])\ntraining_log[\"epoch\"] = list(range(1, epoch + 1))\ntraining_log[\"est_loss\"] = None\ntraining_log = pd.DataFrame(training_log)\n\n\n# Melting the dataframe to make it suitable for seaborn plotting\ntraining_log_melted = training_log[[\"epoch\", \"val_loss\", \"est_loss\"]].melt(id_vars='epoch', var_name='Variable', value_name='Value')\n\n# Finding the color used for the 'metric' in the plot\npalette = sns.color_palette()\nmetric_color = palette[1]  # Assuming 'metric' is the second line in the plot\n\n# Plotting with seaborn\n# Creating two subplots side by side\nfig, axes = plt.subplots(1, 2, figsize=(20, 6))\nsns.lineplot(ax=axes[0], data=training_log_melted, x='epoch', y='Value', hue='Variable', style='Variable', markers=False, dashes=False)\n\naxes[0].set_title('Validation and prediction loss over Epochs (Log Scale) with Benchmark', fontsize=14)\naxes[0].set_xlabel('Epoch', fontsize=15)\naxes[0].set_ylabel('Value (Log Scale)', fontsize=15)\naxes[0].set_yscale('log')\naxes[0].legend(prop={'size': 15})\naxes[0].tick_params(labelsize=14)\naxes[0].grid(True)\n\n# Second plot (sigma, phi, tau)\nkernel_params_melted = training_log[[\"epoch\", \"sigma\", \"phi\", \"tau\"]].melt(id_vars='epoch', var_name='Variable', value_name='Value')\nground_truth = {'sigma': sigma, 'phi': phi/np.sqrt(2), 'tau': tau}\nsns.lineplot(ax=axes[1], data=kernel_params_melted, x='epoch', y='Value', hue='Variable', style='Variable', markers=False, dashes=False, linestyle='dashdot')\npalette = sns.color_palette()\nfor i, (param, gt_value) in enumerate(ground_truth.items()):\n    axes[1].hlines(y=gt_value, xmin=1, xmax=epoch, color=palette[i], linestyle='solid')\n    axes[1].hlines(y=theta_hat_linear[i], xmin=1, xmax=epoch, color=palette[i], linestyle='dashed')\npalette = sns.color_palette()\naxes[1].set_title('Parameter Values over Epochs with Ground Truth', fontsize=14)\naxes[1].set_xlabel('Epoch', fontsize=15)\naxes[1].set_ylabel('Value', fontsize=15)\naxes[1].legend(prop={'size': 15})\naxes[1].tick_params(labelsize=14)\naxes[1].grid(True)\n\nplt.tight_layout()\n</code></pre> <pre><code>\n</code></pre>"},{"location":"Example_realdata/","title":"Example realdata","text":"<pre><code>%load_ext autoreload\n%autoreload 2\n</code></pre> <pre><code>import torch\nimport geospaNN\nimport numpy as np\nimport time\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.image as img \nimport geopandas as gpd\nfrom shapely.geometry import Point\nfrom scipy import spatial, interpolate\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n</code></pre> <pre><code>url = \"https://www2.census.gov/geo/tiger/GENZ2018/shp/cb_2018_us_nation_20m.zip\"\nus = gpd.read_file(url).explode()\nus = us.loc[us.geometry.apply(lambda x: x.exterior.bounds[2])&lt;-60]\n</code></pre> <pre><code>df_covariates = pd.read_csv('./data/covariate0605.csv')\ndf_pm25 = pd.read_csv('./data/pm25_0605.csv')\ndf_pm25 = df_pm25.loc[df_pm25.Latitude &lt; 50]\n\nx_min,y_min,x_max,y_max = np.array([np.min(df_covariates['long']), np.min(df_covariates['lat']),\n    np.max(df_covariates['long']), np.max(df_covariates['lat'])])\narr1 = np.mgrid[x_min:x_max:101j, y_min:y_max:101j]\n\n# extract the x and y coordinates as flat arrays\narr1x = np.ravel(arr1[0])\narr1y = np.ravel(arr1[1])\n# using the X and Y columns, build a dataframe, then the geodataframe\ndf = pd.DataFrame({'X':arr1x, 'Y':arr1y})\ndf['coords'] = list(zip(df['X'], df['Y']))\ndf['coords'] = df['coords'].apply(Point)\n\ngdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(x=df.X, y=df.Y),crs = us.crs)\ninUS = gdf['geometry'].apply(lambda s: s.within(us.geometry.unary_union))\n</code></pre> <pre><code>lonlat_pm25=df_pm25.values[:,[1,2]]\nnear = df_covariates.values[:,[1,2]]\ntree = spatial.KDTree(list(zip(near[:,0].ravel(), near[:,1].ravel())))\nidx = tree.query(lonlat_pm25)[1]\ndf_pm25_mean = df_pm25.assign(neighbor = idx).groupby('neighbor')['PM25'].mean()\nidx_new = df_pm25_mean.index.values\npm25 = df_pm25_mean.values\nz = pm25[:,None]\n\nlon = df_covariates.values[:,1]\nlat = df_covariates.values[:,2]\n\nf = interpolate.Rbf(lon[idx_new], lat[idx_new], z, function = 'inverse')\nx_test = gdf.loc[inUS,:].X\ny_test = gdf.loc[inUS,:].Y\nz_test = f(x_test, y_test)\n</code></pre> <pre><code>plt.clf()\nfig, ax = plt.subplots(figsize=(9, 5))\nc = ax.scatter(x = x_test, y = y_test, s = 10, c = z_test, marker = 's', alpha = 0.7)\nax.plot(np.array(df_pm25['Longitude']), np.array(df_pm25['Latitude']), 'o', c = 'orange', markersize = 4)\nax.set_title('')\nfig.colorbar(c, ax=ax)\nplt.show()\n</code></pre> <pre><code>&lt;Figure size 640x480 with 0 Axes&gt;\n</code></pre> <pre><code>lon = df_covariates.values[:,1]\nlat = df_covariates.values[:,2]\ncovariates = df_covariates.values[:,3:]\nnormalized_lon = (lon-min(lon))/(max(lon)-min(lon))\nnormalized_lat = (lat-min(lat))/(max(lat)-min(lat))\nnormalized_x_test = (x_test-min(lon))/(max(lon)-min(lon))\nnormalized_y_test = (y_test-min(lat))/(max(lat)-min(lat))\n\ns_obs = np.vstack((normalized_lon[idx_new],normalized_lat[idx_new])).T\nX = covariates[idx_new,:]\nnormalized_X = X\nfor i in range(X.shape[1]):\n    normalized_X[:,i] = (X[:,i]-min(X[:,i]))/(max(X[:,i])-min(X[:,i]))\n\nX = normalized_X\nY = z.reshape(-1)\ncoord = s_obs\n#columns = ['precipitation', 'temperature', 'air pressure', 'relative humidity', 'U-wind', 'V-wind',\n#           'PM 2.5', 'longitude', 'latitude']\n#df = pd.DataFrame(data=data, index=range(data.shape[0]), columns=columns)\n#df.to_csv('./data/Normalized_PM2.5_20190605.csv')\n</code></pre> <pre><code>data_PM25 = pd.read_csv(\"./data/Normalized_PM2.5_20190605.csv\")\ndata_PM25\n</code></pre> Unnamed: 0 precipitation temperature air pressure relative humidity U-wind V-wind PM 2.5 longitude latitude 0 0 0.008044 0.362296 0.887664 0.774197 0.868530 0.781498 5.020834 0.980311 0.906268 1 1 0.005516 0.355305 0.882153 0.751742 0.864206 0.770715 3.837500 0.983093 0.889762 2 2 0.000000 0.335323 0.928359 0.714189 0.697080 0.813224 2.041666 0.974238 0.814722 3 3 0.000000 0.338579 0.954218 0.690767 0.625266 0.868161 3.669444 0.976951 0.798275 4 4 0.002528 0.293827 0.893599 0.685830 0.688808 0.842395 1.020833 0.945193 0.800337 ... ... ... ... ... ... ... ... ... ... ... 600 600 0.393932 0.901079 0.972022 0.910279 0.642436 0.469410 5.168750 0.469337 0.101640 601 601 0.000689 0.810329 0.897414 0.539392 0.553265 0.464926 6.041666 0.436702 0.098054 602 602 0.294415 0.882501 0.972022 0.822590 0.642248 0.469353 8.704166 0.468665 0.090378 603 603 0.011492 0.811830 0.965240 0.719028 0.676463 0.512429 8.725000 0.460179 0.035680 604 604 0.256952 0.775792 0.975837 0.802168 0.696516 0.509137 8.213636 0.470595 0.032926 <p>605 rows \u00d7 10 columns</p> <pre><code>X = torch.from_numpy(data_PM25[['precipitation', 'temperature', 'air pressure', 'relative humidity', 'U-wind', 'V-wind']].to_numpy()).float()\nY = torch.from_numpy(data_PM25[['PM 2.5']].to_numpy().reshape(-1)).float()\ncoord = torch.from_numpy(data_PM25[['longitude', 'latitude']].to_numpy()).float()\n\np = X.shape[1]\n\nn = X.shape[0]\nnn = 20\nbatch_size = 50\n\nX, Y, coord, _ = geospaNN.spatial_order(X, Y, coord, method = 'max-min')\ndata = geospaNN.make_graph(X, Y, coord, nn)\n\ntorch.manual_seed(2024)\nnp.random.seed(0)\ndata_train, data_val, data_test = geospaNN.split_data(X, Y, coord, neighbor_size = 20, \n                                                   test_proportion = 0.5)\n</code></pre> <pre><code>start_time = time.time()\nmlp_nn = torch.nn.Sequential(\n    torch.nn.Linear(p, 50),\n    torch.nn.ReLU(),\n    torch.nn.Linear(50, 20),\n    torch.nn.ReLU(),\n    torch.nn.Linear(20, 1),\n)\nnn_model = geospaNN.nn_train(mlp_nn, lr =  0.01, min_delta = 0.001)\ntraining_log = nn_model.train(data_train, data_val, data_test)\n</code></pre> <pre><code>Epoch 00031: reducing learning rate of group 0 to 5.0000e-03.\n</code></pre> <pre><code>theta0 = geospaNN.theta_update(torch.tensor([1, 1.5, 0.01]), mlp_nn(data_train.x).squeeze() - data_train.y, data_train.pos, neighbor_size = 20)\nmlp_nngls = torch.nn.Sequential(\n    torch.nn.Linear(p, 100),\n    torch.nn.ReLU(),\n    torch.nn.Linear(100, 50),\n    torch.nn.ReLU(),\n    torch.nn.Linear(50, 20),\n    torch.nn.ReLU(),\n    torch.nn.Linear(20, 10),\n    torch.nn.ReLU(),\n    torch.nn.Linear(10, 1),\n)\nmodel = geospaNN.nngls(p=p, neighbor_size=nn, coord_dimensions=2, mlp=mlp_nngls, theta=torch.tensor(theta0))\nnngls_model = geospaNN.nngls_train(model, lr =  0.01, min_delta = 0.001)\ntraining_log = nngls_model.train(data_train, data_val, data_test,\n                                 Update_init = 20, Update_step = 10)\nend_time = time.time()\n</code></pre> <pre><code>Theta updated from\n[1.   1.5  0.01]\nEpoch 00020: reducing learning rate of group 0 to 5.0000e-03.\nTheta updated from\n[10.29591688 10.3716713   0.20216037]\nto\n[11.32370202  8.14642528  0.15268137]\nEpoch 00029: reducing learning rate of group 0 to 2.5000e-03.\nTheta updated from\n[11.32370202  8.14642528  0.15268137]\nto\n[10.57182313  8.82062549  0.15966506]\nINFO: Early stopping\nEnd at epoch32\n</code></pre> <pre><code>print(f\"\\rRunning time: {end_time - start_time} seconds\")\n</code></pre> <pre><code>Running time: 7.1364500522613525 seconds\n</code></pre> <pre><code>[test_predict, test_U, test_L] = model.predict(data_train, data_test, CI = True)\nplt.clf()\nplt.scatter(test_predict.detach().numpy(), data_test.y.detach().numpy(), s = 1, label = 'Truth vs prediction')\nplt.scatter(data_test.y.detach().numpy(), data_test.y.detach().numpy(), s = 1, label = 'reference')\nplt.xlabel(\"Prediction\")\nplt.ylabel(\"Truth\")\nplt.legend()\nplt.show()\n</code></pre> <p></p> <pre><code>f_pred = interpolate.CloughTocher2DInterpolator(list(zip(data_test.pos.detach().numpy()[:,0], \n                                                data_test.pos.detach().numpy()[:,1])),\n                                                test_predict.detach().numpy())\nf_U = interpolate.CloughTocher2DInterpolator(list(zip(data_test.pos.detach().numpy()[:,0], \n                                                data_test.pos.detach().numpy()[:,1])),\n                                                test_U.detach().numpy())\nf_L = interpolate.CloughTocher2DInterpolator(list(zip(data_test.pos.detach().numpy()[:,0], \n                                                data_test.pos.detach().numpy()[:,1])),\n                                                test_L.detach().numpy())\nf_true = interpolate.CloughTocher2DInterpolator(list(zip(data_test.pos.detach().numpy()[:,0], \n                                                data_test.pos.detach().numpy()[:,1])),\n                                                data_test.y.detach().numpy())\n\n\ntitles = np.array([['Prediction', 'Truth'], ['Lower confidence bound', 'Upper confidence bound']])\nf_vec = np.array([[f_pred, f_true], [f_L, f_U]])\n\nfig, ax = plt.subplots(2, 2, figsize=(20, 10))\nfor i in range(2):\n    for j in range(2):\n        im = ax[i,j].scatter(x = normalized_x_test, y = normalized_y_test, s = 9, \n                             c = f_vec[i,j](normalized_x_test, normalized_y_test), marker = 's', alpha = 0.7, \n                             vmin=0, vmax=20)\n        ax[i,j].title.set_text(titles[i,j])\n        fig.colorbar(im)\n    #cbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\nax[0,1].plot(data_test.pos.detach().numpy()[:,0], \n           data_test.pos.detach().numpy()[:,1], 'o', c = 'orange', markersize = 4)\nplt.show()\n</code></pre> <p></p> <pre><code>variable_names = ['Precipitation accumulation', 'Air temperature', 'Pressure', 'Relative humidity', 'U-wind', 'V-wind']\ngeospaNN.plot_PDP(model, X, variable_names)\n</code></pre> <p> </p> <p> </p>"},{"location":"Example_simulation/","title":"Example simulation","text":"<pre><code>%load_ext autoreload\n%autoreload 2\n</code></pre> <pre><code>import torch\nimport geospaNN\nimport numpy as np\nimport time\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n</code></pre> <pre><code>class DropoutLayer(torch.nn.Module):\n    def __init__(self, p):\n        super().__init__()\n        self.p = p\n\n    def forward(self, input):\n        if self.training:\n            u1 = torch.rand(*input.shape)&lt;self.p\n            return u1*input\n        else: \n            return input * self.p\n</code></pre> <pre><code>def f5(X): return (10*np.sin(np.pi*X[:,0]*X[:,1]) + 20*(X[:,2]-0.5)**2 + 10*X[:,3] +5*X[:,4])/6\ndef f1(X): return 10 * np.sin(np.pi * X)\n\nsigma = 1\nphi = 3\ntau = 0.01\ntheta = torch.tensor([sigma, phi / np.sqrt(2), tau])\n\np = 1; funXY = f1\n\nn = 1000\nnn = 20\nbatch_size = 50\n\ntorch.manual_seed(2024)\nX, Y, coord, cov, corerr = geospaNN.Simulation(n, p, nn, funXY, theta, range=[0, 10])\n\nX, Y, coord, _ = geospaNN.spatial_order(X, Y, coord, method = 'max-min')\ndata = geospaNN.make_graph(X, Y, coord, nn)\n\ntorch.manual_seed(2024)\nnp.random.seed(0)\ndata_train, data_val, data_test = geospaNN.split_data(X, Y, coord, neighbor_size = nn, \n                                                   test_proportion = 0.2)\n\n</code></pre> <pre><code>torch.manual_seed(2024)\nstart_time = time.time()\nmlp_nn = torch.nn.Sequential(\n    torch.nn.Linear(p, 100),\n    torch.nn.Flatten(), \n    DropoutLayer(0.9),\n    torch.nn.ReLU(),\n    torch.nn.Linear(100, 50),\n    torch.nn.ReLU(),\n    torch.nn.Linear(50, 20),\n    torch.nn.ReLU(),\n    torch.nn.Linear(20, 1),\n)\nnn_model = geospaNN.nn_train(mlp_nn, lr =  0.01, min_delta = 0.001)\ntraining_log = nn_model.train(data_train, data_val, data_test)\ntheta0 = geospaNN.theta_update(torch.tensor([1, 1.5, 0.01]), mlp_nn(data_train.x).squeeze() - data_train.y, data_train.pos, neighbor_size = 20)\nmlp_nngls = torch.nn.Sequential(\n    torch.nn.Linear(p, 100),\n    torch.nn.Flatten(), \n    DropoutLayer(0.9),\n    torch.nn.ReLU(),\n    torch.nn.Linear(100, 50),\n    torch.nn.ReLU(),\n    torch.nn.Linear(50, 20),\n    torch.nn.ReLU(),\n    torch.nn.Linear(20, 10),\n    torch.nn.ReLU(),\n    torch.nn.Linear(10, 1),\n)\nmodel = geospaNN.nngls(p=p, neighbor_size=nn, coord_dimensions=2, mlp=mlp_nngls, theta=torch.tensor(theta0))\nnngls_model = geospaNN.nngls_train(model, lr =  0.01, min_delta = 0.001)\ntraining_log = nngls_model.train(data_train, data_val, data_test,\n                                 Update_init = 10, Update_step = 5)\nend_time = time.time()\n</code></pre> <pre><code>Epoch 00053: reducing learning rate of group 0 to 5.0000e-03.\nINFO: Early stopping\nEnd at epoch56\nTheta updated from\n[1.   1.5  0.01]\nTheta updated from\n[1.23325332 1.91505429 0.16597437]\nto\n[1.24555493 1.88866008 0.02749452]\nTheta updated from\n[1.24555493 1.88866008 0.02749452]\nto\n[2.39995167 0.72656793 0.04760227]\nEpoch 00016: reducing learning rate of group 0 to 5.0000e-03.\nTheta updated from\n[2.39995167 0.72656793 0.04760227]\nto\n[1.33750198 1.46555775 0.05409349]\nEpoch 00023: reducing learning rate of group 0 to 2.5000e-03.\nTheta updated from\n[1.33750198 1.46555775 0.05409349]\nto\n[1.23864962 2.01261422 0.        ]\nEpoch 00030: reducing learning rate of group 0 to 1.2500e-03.\nTheta updated from\n[1.23864962 2.01261422 0.        ]\nto\n[1.2539044  1.80523262 0.01950352]\nINFO: Early stopping\nEnd at epoch33\n</code></pre> <pre><code># Compute benchmark MSE (always predicting the mean)\nbenchmark_preds = torch.full(data_test.y.shape, data_train.y.mean())\nbenchmark_mse = torch.nn.functional.mse_loss(benchmark_preds, data_test.y)\nprint(f'Benchmark MSE: {benchmark_mse:.3f}')\n</code></pre> <pre><code>Benchmark MSE: 10.976\n</code></pre> <pre><code>print(f\"\\rRunning time: {end_time - start_time} seconds\")\n</code></pre> <pre><code>Running time: 20.763789176940918 seconds\n</code></pre> <pre><code>plt.clf()\nplt.scatter(X.detach().numpy(), Y.detach().numpy(), s = 1, label = 'data')\nplt.scatter(X.detach().numpy(), funXY(X.detach().numpy()), s = 1, label = 'f(x)')\nplt.scatter(X.detach().numpy(), mlp_nn(X).detach().numpy(), s = 1, label = 'NN')\nplt.scatter(X.detach().numpy(), mlp_nngls(X).detach().numpy(), s = 1, label = 'NNGLS')\nplt.legend()\nplt.show()\n</code></pre> <pre><code>test_predict = model.predict(data_train, data_test)\n\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\naxes[0].scatter(data_test.x.detach().numpy(), data_test.y.detach().numpy(), s = 1, label = 'data')\naxes[0].scatter(data_test.x.detach().numpy(), funXY(data_test.x.detach().numpy()), s = 1, label = 'f(x)')\naxes[0].scatter(data_test.x.detach().numpy(), mlp_nngls(data_test.x).detach().numpy(), s = 1, label = 'estimation')\naxes[0].scatter(data_test.x.detach().numpy(), test_predict.detach().numpy(), s = 1, label = 'prediction')\naxes[0].set_xlabel('X', fontsize=15)\naxes[0].set_ylabel('Y', fontsize=15)\naxes[0].legend(prop={'size': 15})\n\naxes[1].scatter(test_predict.detach().numpy(), data_test.y.detach().numpy(), s = 1, label = 'Truth vs prediction')\naxes[1].scatter(data_test.y.detach().numpy(), data_test.y.detach().numpy(), s = 1, label = 'reference')\naxes[1].set_xlabel(\"Prediction\", fontsize=15)\naxes[1].set_ylabel(\"Truth\", fontsize=15)\naxes[1].legend(prop={'size': 15})\nplt.tight_layout()\n</code></pre> <pre><code>epoch = len(training_log[\"val_loss\"])\ntraining_log[\"epoch\"] = list(range(1, epoch + 1))\ntraining_log[\"est_loss\"] = None\ntraining_log = pd.DataFrame(training_log)\n\n# Melting the dataframe to make it suitable for seaborn plotting\ntraining_log_melted = training_log[[\"epoch\", \"val_loss\"]].melt(id_vars='epoch', var_name='Variable', value_name='Value')\n\n# Finding the color used for the 'metric' in the plot\npalette = sns.color_palette()\nmetric_color = palette[1]  # Assuming 'metric' is the second line in the plot\n\n# Plotting with seaborn\n# Creating two subplots side by side\nfig, axes = plt.subplots(1, 2, figsize=(20, 6))\nbenchmark_line = axes[0].hlines(y=benchmark_mse, xmin=1, xmax=epoch, color=metric_color, linestyle='--', label='benchmark')\nsns.lineplot(ax=axes[0], data=training_log_melted, x='epoch', y='Value', hue='Variable', style='Variable', markers=False, dashes=False)\n\naxes[0].set_title('Validation and prediction loss over Epochs (Log Scale) with Benchmark', fontsize=14)\naxes[0].set_xlabel('Epoch', fontsize=15)\naxes[0].set_ylabel('Value (Log Scale)', fontsize=15)\naxes[0].set_yscale('log')\naxes[0].legend(prop={'size': 15})\naxes[0].tick_params(labelsize=14)\naxes[0].grid(True)\n\n# Second plot (sigma, phi, tau)\nkernel_params_melted = training_log[[\"epoch\", \"sigma\", \"phi\", \"tau\"]].melt(id_vars='epoch', var_name='Variable', value_name='Value')\nground_truth = {'sigma': sigma, 'phi': phi/np.sqrt(2), 'tau': tau}\nsns.lineplot(ax=axes[1], data=kernel_params_melted, x='epoch', y='Value', hue='Variable', style='Variable', markers=False, dashes=False)\npalette = sns.color_palette()\nfor i, (param, gt_value) in enumerate(ground_truth.items()):\n    axes[1].hlines(y=gt_value, xmin=1, xmax=epoch, color=palette[i], linestyle='--')\naxes[1].set_title('Parameter Values over Epochs with Ground Truth', fontsize=14)\naxes[1].set_xlabel('Epoch', fontsize=15)\naxes[1].set_ylabel('Value', fontsize=15)\naxes[1].legend(prop={'size': 15})\naxes[1].tick_params(labelsize=14)\naxes[1].grid(True)\n\nplt.tight_layout()\n</code></pre> <pre><code>time_df = pd.read_csv(\"./data/running_time.csv\")\ntime_df['Time per epoch'] = time_df['time']/time_df['epoch']\n\nfig, axes = plt.subplots(1, 2, figsize=(20, 6))\nsns.lineplot(ax=axes[0], data=time, x='size', y='Time per epoch', markers=False, dashes=False)\naxes[0].set_title('Running time per epoch vs sample size', fontsize=14)\naxes[0].axline((200, 0.04), slope=1, color='red', label='by slope', linestyle='--')\naxes[0].set_xlabel('Size (Log scale)', fontsize=15)\naxes[0].set_ylabel('Time (Log Scale)', fontsize=15)\naxes[0].set_yscale('log')\naxes[0].set_xscale('log')\naxes[0].legend(prop={'size': 15})\naxes[0].tick_params(labelsize=14)\naxes[0].grid(True)\n\nsns.lineplot(ax=axes[1], data=time, x='size', y='time', markers=False, dashes=False)\naxes[1].set_title('Running time vs sample size', fontsize=14)\naxes[1].axline((200, 1.2), slope=1, color='red', label='by slope', linestyle='--')\naxes[1].set_xlabel('Size (Log scale)', fontsize=15)\naxes[1].set_ylabel('Time (Log Scale)', fontsize=15)\naxes[1].set_yscale('log')\naxes[1].set_xscale('log')\naxes[1].legend(prop={'size': 15})\naxes[1].tick_params(labelsize=14)\naxes[1].grid(True)\n#axes[1].set_yscale('log')\n</code></pre>"},{"location":"Examples/","title":"Examples","text":""},{"location":"Examples/#running-examples","title":"Running examples","text":""},{"location":"Examples/#notes","title":"Notes","text":"<p>Python packages time, pandas, seaborn, geopandas, and matplotlib are required to run the following experiments.</p> <ul> <li>A simple pipeline illsutrating the basic features of the package is available here.</li> <li>Several simulation examples are available, to illustrate the usage of geospaNN for different statistical tasks:<ul> <li>Choice of neural network architectures.</li> <li>Compare with the add-to-spatial-feature approaches.</li> <li>Compare with GAM.</li> </ul> </li> <li>A real data experiment is shown here.</li> </ul> <p>In the real data experiment, the PM2.5 data is collected from the U.S. Environmental Protection Agency datasets for each state are collected and bound together to obtain 'pm25_2022.csv'. daily PM2.5 files are subsets of 'pm25_2022.csv' produced by 'realdata_preprocess.py'. One can skip the preprocessing and use the daily files directory.</p> <p>The meteorological data is collected from the National Centers for Environmental Prediction's (NCEP) North American Regional Reanalysis (NARR) product. The '.nc' (netCDF) files should be downloaded from the website and saved in the root directory to run 'realdata_preprocess.py'. Otherwise, one may skip the preprocessing and use covariate files directly.</p>"},{"location":"Overview/","title":"Overview","text":"<p>The Python package geospaNN stands for 'geospatial Neural Networks', where we implement NN-GLS,  neural networks tailored for analysis of geospatial data that explicitly accounts for spatial dependence (Zhan et.al, 2023).  Geospatial data naturally exhibits spatial correlation or dependence and traditional geostatistical analysis often relies on  model-based approaches to handle the spatial dependency, treating the spatial outcome y(s) as a linear regression on covariates x(s) and  modeling dependency through the spatially correlated errors.  For example, using Gaussian processes (GP) to model dependent errors,  simple techniques like kriging can provide powerful prediction performance by properly aggregating the neighboring information.  On the other hand, artificial Neural Networks (NN), one of the most popular machine learning approaches, could be used to estimate non-linear regression functions.  However, common neural networks like multi-layer perceptrons (MLP) does not incorporate correlation among data units.</p> <p>Our package geospaNN takes the advantages from both perspectives and provides an efficient tool for geospatial data analysis.  In NN-GLS, an MLP is used to model the non-linear regression function while a GP is used to model the spatial dependence.  The resulting loss function then becomes a generalized least squares (GLS) loss informed by the GP covariance matrix,  thereby explicitly incorporating spatial correlation into the neural network optimization.  The idea mimics the extension of ordinary least squares (OLS) loss to GLS loss in linear regression for dependent data.</p> <p>Zhan and Datta, 2023 shows that neural networks with GLS loss can be represented as a graph neural network,  with the GP covariances guiding the neighborhood aggregation on the output layer.  Thus NN-GLS is implemented in geospaNN with the framework of Graph Neural Networks (GNN), and is highly generalizable.  (The implementation of geospaNN' uses the 'torch_geom' module.)</p> <p>geospaNN provides an estimate of regression function \ud835\udc53(\ud835\udc65) as well as accurate spatial predictions using Gaussian process (kriging),  and thus constitutes a complete geospatial analysis pipeline.  To accelerate the training process for the GP, geospaNN approximates the working correlation structure using  Nearest Neighbor Gaussian Process (NNGP) (Datta et al., 2016) which makes it suitable for larger datasets towards a size of 1 million.</p> <p></p>"},{"location":"Overview/#citation","title":"Citation","text":"<p>Please cite the following paper when you use geospaNN:</p> <p>Zhan, Wentao, and Abhirup Datta. 2024. \u201cNeural Networks for Geospatial Data.\u201d Journal of the American Statistical Association, June, 1\u201321. doi:10.1080/01621459.2024.2356293. </p>"},{"location":"Overview/#references","title":"References","text":"<p>Datta, Abhirup, Sudipto Banerjee, Andrew O. Finley, and Alan E. Gelfand. 2016. \u201cHierarchical Nearest-Neighbor Gaussian Process Models for Large Geostatistical Datasets.\u201d Journal of the American Statistical Association 111 (514): 800\u2013812. doi:10.1080/01621459.2015.1044091.</p> <p>Zhan, Wentao, and Abhirup Datta. 2024. \u201cNeural Networks for Geospatial Data.\u201d Journal of the American Statistical Association, June, 1\u201321. doi:10.1080/01621459.2024.2356293.</p>"},{"location":"main/","title":"Main","text":""},{"location":"main/#geospaNN.main.nn_train","title":"<code>nn_train</code>","text":"<p>A wrapper for training the ordinary neural networks (simple MLP).</p> <p>The class wraps up a standard training process for ordinary neural networks. Currently it only works for simple MLPs and will be extended to more complicated settings in the future. For more advanced model, users are recommended to write the training functions manually.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>Module</code> <p>A trainable feed-forward model that returns the output.</p> <code>lr</code> <code>float</code> <p>Learning rate.</p> <code>patience</code> <code>float</code> <p>The patience for the early stopping rule, see train() for more details.</p> <code>patience_cut_lr</code> <code>float</code> <p>The patience for cutting the learning rate, see train() for more details.</p> <code>min_delta</code> <code>float</code> <p>The threshold for terminating the training, see train() for more details.</p> <p>Methods:</p> Name Description <code>train</code> <p>Train the model under a mean-squared loss and the early-stopping rule as follows. If the validation loss does not have a drop greater than min_delta for #patience_cut_lr epoches, reduce the learning rate by 50%. If the validation loss does not have a drop greater than min_delta for #patience epoches, the training process terminates. Since Adam optimizer is used here, cutting the learning rate is unnecessary, but we do find setting \"patience_cut_lr = patience/2\" helps the convergence in many scenarios. We keep this setting as default.</p> Source code in <code>geospaNN/main.py</code> <pre><code>class nn_train():\n    \"\"\"\n    A wrapper for training the ordinary neural networks (simple MLP).\n\n    The class wraps up a standard training process for ordinary neural networks. Currently it only works for simple MLPs\n    and will be extended to more complicated settings in the future. For more advanced model, users are recommended to write\n    the training functions manually.\n\n    Attributes:\n        model (torch.nn.Module):\n            A trainable feed-forward model that returns the output.\n        lr (float):\n            Learning rate.\n        patience (float):\n            The patience for the early stopping rule, see train() for more details.\n        patience_cut_lr (float):\n            The patience for cutting the learning rate, see train() for more details.\n        min_delta (float):\n            The threshold for terminating the training, see train() for more details.\n\n    Methods:\n        train():\n            Train the model under a mean-squared loss and the early-stopping rule as follows.\n            If the validation loss does not have a drop greater than min_delta for #patience_cut_lr epoches,\n            reduce the learning rate by 50%.\n            If the validation loss does not have a drop greater than min_delta for #patience epoches,\n            the training process terminates.\n            Since Adam optimizer is used here, cutting the learning rate is unnecessary, but we do find setting \"patience_cut_lr =\n            patience/2\" helps the convergence in many scenarios. We keep this setting as default.\n    \"\"\"\n    def __init__(\n            self,\n            model: torch.nn.Module,\n            lr: Optional[float] =  0.01,\n            patience: Optional[int] = 10,\n            patience_cut_lr: Optional[float] = None,\n            min_delta: float = 0.001\n    ):\n        if patience_cut_lr is None:\n            patience_cut_lr = int(patience/2)\n        self.model = model\n        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n        self.lr_scheduler = LRScheduler(self.optimizer, patience=patience_cut_lr, factor=0.5)\n        self.early_stopping = EarlyStopping(patience=patience, min_delta=min_delta)\n\n    def train(self,\n              data_train: torch_geometric.data.Data,\n              data_val: torch_geometric.data.Data,\n              data_test: Optional[torch_geometric.data.Data] = None,\n              batch_size: Optional[int] = None,\n              epoch_num: Optional[int] = 100,\n              seed: Optional[int] = torch.randint(0, 2024, (1,))\n              ) -&gt; list:\n        \"\"\"Train the neural networks model.\n\n        Parameters:\n            data_train:\n                Training data containing x, y and spatial coordinates, can be the output of split_data() or make_graph().\n            data_val:\n                validation data containing x, y and spatial coordinates, can be the output of split_data() or make_graph().\n            data_test:\n                Testing data containing x, y and spatial coordinates, can be the output of split_data() or make_graph().\n                If not specified, data_train is used for testing.\n            batch_size:\n                Individual size of mini-batches that data_train is split into.\n            epoch_num:\n                Maximum number of epoches allowed.\n            seed:\n                Random seed for data splitting.\n\n        Returns:\n            training_log:\n                A list contains the validation loss, estimation loss.\n        \"\"\"\n        if batch_size is None:\n            batch_size = int(data_train.x.shape[0]/10)\n        if data_test is None:\n            data_test = data_train\n        torch.manual_seed(seed)\n        train_loader = split_loader(data_train, batch_size)\n        training_log = {'val_loss': [], 'est_loss': [], 'sigma': [], 'phi': [], 'tau': []}\n        for epoch in range(epoch_num):\n            # Train for one epoch\n            self.model.train()\n\n            for batch_idx, batch in enumerate(train_loader):\n                self.optimizer.zero_grad()\n                est = self.model(batch.x).squeeze()\n                loss = torch.nn.functional.mse_loss(est, batch.y)\n                loss.backward()\n                self.optimizer.step()\n            # Compute estimations on held-out test set\n            self.model.eval()\n            val_est = self.model(data_val.x).squeeze()\n            val_loss = torch.nn.functional.mse_loss(val_est, data_val.y).item()\n            self.lr_scheduler(val_loss)\n            self.early_stopping(val_loss)\n            if self.early_stopping.early_stop:\n                print('End at epoch' + str(epoch))\n                break\n            training_log[\"val_loss\"].append(val_loss)\n            test_est = self.model(data_test.x).squeeze()\n            est_loss = torch.nn.functional.mse_loss(test_est, data_test.y).item()\n            training_log[\"est_loss\"].append(est_loss)\n\n        return training_log\n</code></pre>"},{"location":"main/#geospaNN.main.nn_train.train","title":"<code>train(data_train, data_val, data_test=None, batch_size=None, epoch_num=100, seed=torch.randint(0, 2024, (1)))</code>","text":"<p>Train the neural networks model.</p> <p>Parameters:</p> Name Type Description Default <code>data_train</code> <code>Data</code> <p>Training data containing x, y and spatial coordinates, can be the output of split_data() or make_graph().</p> required <code>data_val</code> <code>Data</code> <p>validation data containing x, y and spatial coordinates, can be the output of split_data() or make_graph().</p> required <code>data_test</code> <code>Optional[Data]</code> <p>Testing data containing x, y and spatial coordinates, can be the output of split_data() or make_graph(). If not specified, data_train is used for testing.</p> <code>None</code> <code>batch_size</code> <code>Optional[int]</code> <p>Individual size of mini-batches that data_train is split into.</p> <code>None</code> <code>epoch_num</code> <code>Optional[int]</code> <p>Maximum number of epoches allowed.</p> <code>100</code> <code>seed</code> <code>Optional[int]</code> <p>Random seed for data splitting.</p> <code>randint(0, 2024, (1))</code> <p>Returns:</p> Name Type Description <code>training_log</code> <code>list</code> <p>A list contains the validation loss, estimation loss.</p> Source code in <code>geospaNN/main.py</code> <pre><code>def train(self,\n          data_train: torch_geometric.data.Data,\n          data_val: torch_geometric.data.Data,\n          data_test: Optional[torch_geometric.data.Data] = None,\n          batch_size: Optional[int] = None,\n          epoch_num: Optional[int] = 100,\n          seed: Optional[int] = torch.randint(0, 2024, (1,))\n          ) -&gt; list:\n    \"\"\"Train the neural networks model.\n\n    Parameters:\n        data_train:\n            Training data containing x, y and spatial coordinates, can be the output of split_data() or make_graph().\n        data_val:\n            validation data containing x, y and spatial coordinates, can be the output of split_data() or make_graph().\n        data_test:\n            Testing data containing x, y and spatial coordinates, can be the output of split_data() or make_graph().\n            If not specified, data_train is used for testing.\n        batch_size:\n            Individual size of mini-batches that data_train is split into.\n        epoch_num:\n            Maximum number of epoches allowed.\n        seed:\n            Random seed for data splitting.\n\n    Returns:\n        training_log:\n            A list contains the validation loss, estimation loss.\n    \"\"\"\n    if batch_size is None:\n        batch_size = int(data_train.x.shape[0]/10)\n    if data_test is None:\n        data_test = data_train\n    torch.manual_seed(seed)\n    train_loader = split_loader(data_train, batch_size)\n    training_log = {'val_loss': [], 'est_loss': [], 'sigma': [], 'phi': [], 'tau': []}\n    for epoch in range(epoch_num):\n        # Train for one epoch\n        self.model.train()\n\n        for batch_idx, batch in enumerate(train_loader):\n            self.optimizer.zero_grad()\n            est = self.model(batch.x).squeeze()\n            loss = torch.nn.functional.mse_loss(est, batch.y)\n            loss.backward()\n            self.optimizer.step()\n        # Compute estimations on held-out test set\n        self.model.eval()\n        val_est = self.model(data_val.x).squeeze()\n        val_loss = torch.nn.functional.mse_loss(val_est, data_val.y).item()\n        self.lr_scheduler(val_loss)\n        self.early_stopping(val_loss)\n        if self.early_stopping.early_stop:\n            print('End at epoch' + str(epoch))\n            break\n        training_log[\"val_loss\"].append(val_loss)\n        test_est = self.model(data_test.x).squeeze()\n        est_loss = torch.nn.functional.mse_loss(test_est, data_test.y).item()\n        training_log[\"est_loss\"].append(est_loss)\n\n    return training_log\n</code></pre>"},{"location":"main/#geospaNN.main.nngls_train","title":"<code>nngls_train</code>","text":"<p>A wrapper for training the NN-GLS model.</p> <p>The class wraps up the training process for NN-GLS. We assume simple MLP is used for the upper body of the model. NN-GLS allows for more complicated network structures before the final decorrelation step. However, for more advanced structures, finer tuning on the hyperparameters is often needed. Users are recommended to write the training functions manually in that case.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>Module</code> <p>A trainable feed-forward model that returns the output.</p> <code>lr</code> <code>float</code> <p>Learning rate.</p> <code>patience</code> <code>int</code> <p>The patience for the early stopping rule, see train() for more details.</p> <code>patience_cut_lr</code> <code>float</code> <p>The patience for cutting the learning rate, see train() for more details.</p> <code>min_delta</code> <code>float</code> <p>The threshold for terminating the training, see train() for more details.</p> <p>Methods:</p> Name Description <code>train</code> <p>Same as nn_train.train(), train the model under a mean-squared loss and the early-stopping rule as follows. If the validation loss does not have a drop greater than min_delta for #patience_cut_lr epoches, reduce the learning rate by 50%. If the validation loss does not have a drop greater than min_delta for #patience epoches, the training process terminates. Since Adam optimizer is used here, cutting the learning rate is unnecessary, but we do find setting \"patience_cut_lr = patience/2\" helps the convergence in many scenarios. We keep this setting as default.</p> Source code in <code>geospaNN/main.py</code> <pre><code>class nngls_train():\n    \"\"\"\n    A wrapper for training the NN-GLS model.\n\n    The class wraps up the training process for NN-GLS. We assume simple MLP is used for the upper body of the model.\n    NN-GLS allows for more complicated network structures before the final decorrelation step.\n    However, for more advanced structures, finer tuning on the hyperparameters is often needed.\n    Users are recommended to write the training functions manually in that case.\n\n    Attributes:\n        model (torch.nn.Module):\n            A trainable feed-forward model that returns the output.\n        lr (float):\n            Learning rate.\n        patience (int):\n            The patience for the early stopping rule, see train() for more details.\n        patience_cut_lr (float):\n            The patience for cutting the learning rate, see train() for more details.\n        min_delta (float):\n            The threshold for terminating the training, see train() for more details.\n\n    Methods:\n        train():\n            Same as nn_train.train(), train the model under a mean-squared loss and the early-stopping rule as follows.\n            If the validation loss does not have a drop greater than min_delta for #patience_cut_lr epoches,\n            reduce the learning rate by 50%.\n            If the validation loss does not have a drop greater than min_delta for #patience epoches,\n            the training process terminates.\n            Since Adam optimizer is used here, cutting the learning rate is unnecessary, but we do find setting \"patience_cut_lr =\n            patience/2\" helps the convergence in many scenarios. We keep this setting as default.\n    \"\"\"\n    def __init__(\n            self,\n            model: torch.nn.Module,\n            lr: Optional[float] =  0.01,\n            patience: Optional[int] = 10,\n            patience_cut_lr: Optional[int] = None,\n            min_delta: float = 0.001\n    ):\n        if patience_cut_lr is None:\n            patience_cut_lr = int(patience/2)\n        self.model = model\n        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n        self.lr_scheduler = LRScheduler(self.optimizer, patience=patience_cut_lr, factor=0.5)\n        self.early_stopping = EarlyStopping(patience=patience, min_delta=min_delta)\n\n    def theta_update(self,\n                     w: torch.tensor,\n                     data: torch_geometric.data.Data\n                     ): #### Can be replaced by directly using theta_update?\n        \"\"\"Update the spatial parameters using maximum likelihood.\n\n        This is a wrapper for theta_update() within the training module. See help(geospaNN.theta_update) for more details.\n\n        Parameters:\n            w:\n                Length n observations of the spatial random effect without any fixed effect.\n            data:\n                The data.pos object should contain a nxd coordinates matrix.\n\n        Returns:\n            Update self.model.theta by the new estimation.\n        \"\"\"\n        theta_new = theta_update(w,\n                                 data.pos,\n                                 self.model.theta,\n                                 self.model.neighbor_size,\n                                 )\n\n        state_dict = self.model.state_dict()\n        state_dict['theta'] = torch.from_numpy(theta_new)\n        self.model.load_state_dict(state_dict)\n        print('to')\n        print(theta_new)\n\n    def train(self,\n              data_train: torch_geometric.data.Data,\n              data_val: torch_geometric.data.Data,\n              data_test: Optional[torch_geometric.data.Data] = None,\n              batch_size: Optional[int] = None,\n              epoch_num: Optional[int] = 100,\n              Update_init: Optional[int] = 0,\n              Update_step: Optional[int] = 1,\n              seed: Optional[int] = torch.randint(0, 2024, (1,)),\n              vignette: Optional[bool] = False\n              ) -&gt; list:\n        \"\"\"Train NN-GLS.\n\n        Parameters:\n            data_train:\n                Training data containing x, y and spatial coordinates, can be the output of split_data() or make_graph().\n            data_val:\n                validation data containing x, y and spatial coordinates, can be the output of split_data() or make_graph().\n            data_test:\n                Testing data containing x, y and spatial coordinates, can be the output of split_data() or make_graph().\n                If not specified, data_train is used for testing.\n            batch_size:\n                Individual size of mini-batches that data_train is split into.\n            epoch_num:\n                Maximum number of epoches allowed.\n            Update_init:\n                Initial epoch to start spatial parameter updating. The aim here is to allow a 'burn-in' period for NN-GLS's\n                fexed-effect estimation to converge. Default value is 0.\n            Update_step:\n                The spatial parameters will be updated every #Update_step epoches. The default value is 1.\n            seed:\n                Random seed for data splitting.\n\n        Returns:\n            training_log:\n                A list contains the validation loss, estimation loss.\n        \"\"\"\n        if batch_size is None:\n            batch_size = int(data_train.x.shape[0]/10)\n        torch.manual_seed(seed)\n        train_loader = split_loader(data_train, batch_size)\n        training_log = {'val_loss': [], 'est_loss': [], 'sigma': [], 'phi': [], 'tau': []}\n        for epoch in range(epoch_num):\n            # Train for one epoch\n            w = data_train.y - self.model.estimate(data_train.x)\n            self.model.train()\n            self.model.theta.requires_grad = False\n            if (epoch &gt;= Update_init) &amp; (epoch % Update_step == 0):\n                self.theta_update(w, data_train)\n\n            for batch_idx, batch in enumerate(train_loader):\n                if vignette:\n                    print(batch_idx)\n                self.optimizer.zero_grad()\n                decorrelated_preds, decorrelated_targets, est = self.model(batch)\n                loss = torch.nn.functional.mse_loss(decorrelated_preds[:batch_size], decorrelated_targets[:batch_size])\n                loss.backward()\n                self.optimizer.step()\n            # Compute estimations on held-out test set\n            self.model.eval()\n            val_est = self.model.estimate(data_val.x)\n            val_loss = torch.nn.functional.mse_loss(val_est, data_val.y).item()\n            self.lr_scheduler(val_loss)\n            self.early_stopping(val_loss)\n            if self.early_stopping.early_stop:\n                print('End at epoch' + str(epoch))\n                break\n            training_log[\"val_loss\"].append(val_loss)\n            training_log[\"sigma\"].append(self.model.theta[0].item())\n            training_log[\"phi\"].append(self.model.theta[1].item())\n            training_log[\"tau\"].append(self.model.theta[2].item())\n            if data_test is not None:\n                test_est = self.model.estimate(data_test.x)\n                est_loss = torch.nn.functional.mse_loss(test_est, data_test.y).item()\n                training_log[\"est_loss\"].append(est_loss)\n\n        return training_log\n</code></pre>"},{"location":"main/#geospaNN.main.nngls_train.theta_update","title":"<code>theta_update(w, data)</code>","text":"<p>Update the spatial parameters using maximum likelihood.</p> <p>This is a wrapper for theta_update() within the training module. See help(geospaNN.theta_update) for more details.</p> <p>Parameters:</p> Name Type Description Default <code>w</code> <code>tensor</code> <p>Length n observations of the spatial random effect without any fixed effect.</p> required <code>data</code> <code>Data</code> <p>The data.pos object should contain a nxd coordinates matrix.</p> required <p>Returns:</p> Type Description <p>Update self.model.theta by the new estimation.</p> Source code in <code>geospaNN/main.py</code> <pre><code>def theta_update(self,\n                 w: torch.tensor,\n                 data: torch_geometric.data.Data\n                 ): #### Can be replaced by directly using theta_update?\n    \"\"\"Update the spatial parameters using maximum likelihood.\n\n    This is a wrapper for theta_update() within the training module. See help(geospaNN.theta_update) for more details.\n\n    Parameters:\n        w:\n            Length n observations of the spatial random effect without any fixed effect.\n        data:\n            The data.pos object should contain a nxd coordinates matrix.\n\n    Returns:\n        Update self.model.theta by the new estimation.\n    \"\"\"\n    theta_new = theta_update(w,\n                             data.pos,\n                             self.model.theta,\n                             self.model.neighbor_size,\n                             )\n\n    state_dict = self.model.state_dict()\n    state_dict['theta'] = torch.from_numpy(theta_new)\n    self.model.load_state_dict(state_dict)\n    print('to')\n    print(theta_new)\n</code></pre>"},{"location":"main/#geospaNN.main.nngls_train.train","title":"<code>train(data_train, data_val, data_test=None, batch_size=None, epoch_num=100, Update_init=0, Update_step=1, seed=torch.randint(0, 2024, (1)), vignette=False)</code>","text":"<p>Train NN-GLS.</p> <p>Parameters:</p> Name Type Description Default <code>data_train</code> <code>Data</code> <p>Training data containing x, y and spatial coordinates, can be the output of split_data() or make_graph().</p> required <code>data_val</code> <code>Data</code> <p>validation data containing x, y and spatial coordinates, can be the output of split_data() or make_graph().</p> required <code>data_test</code> <code>Optional[Data]</code> <p>Testing data containing x, y and spatial coordinates, can be the output of split_data() or make_graph(). If not specified, data_train is used for testing.</p> <code>None</code> <code>batch_size</code> <code>Optional[int]</code> <p>Individual size of mini-batches that data_train is split into.</p> <code>None</code> <code>epoch_num</code> <code>Optional[int]</code> <p>Maximum number of epoches allowed.</p> <code>100</code> <code>Update_init</code> <code>Optional[int]</code> <p>Initial epoch to start spatial parameter updating. The aim here is to allow a 'burn-in' period for NN-GLS's fexed-effect estimation to converge. Default value is 0.</p> <code>0</code> <code>Update_step</code> <code>Optional[int]</code> <p>The spatial parameters will be updated every #Update_step epoches. The default value is 1.</p> <code>1</code> <code>seed</code> <code>Optional[int]</code> <p>Random seed for data splitting.</p> <code>randint(0, 2024, (1))</code> <p>Returns:</p> Name Type Description <code>training_log</code> <code>list</code> <p>A list contains the validation loss, estimation loss.</p> Source code in <code>geospaNN/main.py</code> <pre><code>def train(self,\n          data_train: torch_geometric.data.Data,\n          data_val: torch_geometric.data.Data,\n          data_test: Optional[torch_geometric.data.Data] = None,\n          batch_size: Optional[int] = None,\n          epoch_num: Optional[int] = 100,\n          Update_init: Optional[int] = 0,\n          Update_step: Optional[int] = 1,\n          seed: Optional[int] = torch.randint(0, 2024, (1,)),\n          vignette: Optional[bool] = False\n          ) -&gt; list:\n    \"\"\"Train NN-GLS.\n\n    Parameters:\n        data_train:\n            Training data containing x, y and spatial coordinates, can be the output of split_data() or make_graph().\n        data_val:\n            validation data containing x, y and spatial coordinates, can be the output of split_data() or make_graph().\n        data_test:\n            Testing data containing x, y and spatial coordinates, can be the output of split_data() or make_graph().\n            If not specified, data_train is used for testing.\n        batch_size:\n            Individual size of mini-batches that data_train is split into.\n        epoch_num:\n            Maximum number of epoches allowed.\n        Update_init:\n            Initial epoch to start spatial parameter updating. The aim here is to allow a 'burn-in' period for NN-GLS's\n            fexed-effect estimation to converge. Default value is 0.\n        Update_step:\n            The spatial parameters will be updated every #Update_step epoches. The default value is 1.\n        seed:\n            Random seed for data splitting.\n\n    Returns:\n        training_log:\n            A list contains the validation loss, estimation loss.\n    \"\"\"\n    if batch_size is None:\n        batch_size = int(data_train.x.shape[0]/10)\n    torch.manual_seed(seed)\n    train_loader = split_loader(data_train, batch_size)\n    training_log = {'val_loss': [], 'est_loss': [], 'sigma': [], 'phi': [], 'tau': []}\n    for epoch in range(epoch_num):\n        # Train for one epoch\n        w = data_train.y - self.model.estimate(data_train.x)\n        self.model.train()\n        self.model.theta.requires_grad = False\n        if (epoch &gt;= Update_init) &amp; (epoch % Update_step == 0):\n            self.theta_update(w, data_train)\n\n        for batch_idx, batch in enumerate(train_loader):\n            if vignette:\n                print(batch_idx)\n            self.optimizer.zero_grad()\n            decorrelated_preds, decorrelated_targets, est = self.model(batch)\n            loss = torch.nn.functional.mse_loss(decorrelated_preds[:batch_size], decorrelated_targets[:batch_size])\n            loss.backward()\n            self.optimizer.step()\n        # Compute estimations on held-out test set\n        self.model.eval()\n        val_est = self.model.estimate(data_val.x)\n        val_loss = torch.nn.functional.mse_loss(val_est, data_val.y).item()\n        self.lr_scheduler(val_loss)\n        self.early_stopping(val_loss)\n        if self.early_stopping.early_stop:\n            print('End at epoch' + str(epoch))\n            break\n        training_log[\"val_loss\"].append(val_loss)\n        training_log[\"sigma\"].append(self.model.theta[0].item())\n        training_log[\"phi\"].append(self.model.theta[1].item())\n        training_log[\"tau\"].append(self.model.theta[2].item())\n        if data_test is not None:\n            test_est = self.model.estimate(data_test.x)\n            est_loss = torch.nn.functional.mse_loss(test_est, data_test.y).item()\n            training_log[\"est_loss\"].append(est_loss)\n\n    return training_log\n</code></pre>"},{"location":"model/","title":"Model","text":"<p>This part of the project documentation focuses on an information-oriented approach. Use it as a reference for the technical implementation of the <code>geospaNN</code> package code.</p>"},{"location":"model/#geospaNN.model.InverseCovMat","title":"<code>InverseCovMat</code>","text":"<p>               Bases: <code>Module</code></p> <p>A feed-forward layer that returns inverses of nearest neighbor covariance matrices. For neighbor size k, and batch size b, the feed-forward layer will return a bxkxk tensor, where the ith kxk matrix is the inverse of neighbor covariance matrix cov(N(i), N(i)). ...</p> <p>Attributes:</p> Name Type Description <code>neighbor_size</code> <code>int</code> <p>Size of nearest neighbor used for NNGP approximation. i.e. k in the documentation.</p> <code>coord_dimension</code> <code>int</code> <p>Dimension of the coordinates, i.e. d in the documentation.</p> <code>theta</code> <code>tuple[float, float, float]</code> <p>theta[0], theta[1], theta[2] represent sigma^2, phi, tau in the exponential covariance family.</p> <p>Methods:</p> Name Description <code>forward</code> Source code in <code>geospaNN/model.py</code> <pre><code>class InverseCovMat(torch.nn.Module):\n    \"\"\"\n    A feed-forward layer that returns inverses of nearest neighbor covariance matrices. For neighbor size k, and batch size b,\n    the feed-forward layer will return a bxkxk tensor, where the ith kxk matrix is the inverse of neighbor covariance matrix\n    cov(N(i), N(i)).\n    ...\n\n    Attributes:\n        neighbor_size (int):\n            Size of nearest neighbor used for NNGP approximation. i.e. k in the documentation.\n\n        coord_dimension (int):\n            Dimension of the coordinates, i.e. d in the documentation.\n\n        theta (tuple[float, float, float]):\n            theta[0], theta[1], theta[2] represent sigma^2, phi, tau in the exponential covariance family.\n\n    Methods:\n        forward():\n    \"\"\"\n    def __init__(self, neighbor_size, coord_dimension, theta):\n        super(InverseCovMat, self).__init__()\n        self.neighbor_size = neighbor_size\n        self.coord_dimension = coord_dimension\n        self.theta = theta\n\n    def forward(self, neighbor_positions, edge_list):\n        batch_size = neighbor_positions.shape[0]\n        neighbor_positions = neighbor_positions.reshape(-1, self.neighbor_size, self.coord_dimension)\n        neighbor_positions1 = neighbor_positions.unsqueeze(1)\n        neighbor_positions2 = neighbor_positions.unsqueeze(2)\n        dists = torch.sqrt(torch.sum((neighbor_positions1 - neighbor_positions2) ** 2, axis=-1))\n        cov = make_cov_full(dists, self.theta, nuggets=True)  # have to add nuggets (resolved)\n        # cov_final = self.theta[0]*torch.eye(self.neighbor_size).repeat(batch_size, 1, 1)\n        # for i in range(batch_size):\n        #    cov_final[i, edge_list[i].reshape(1, -1, 1), edge_list[i].reshape(1, 1, -1)] = \\\n        #        cov[i, edge_list[i].reshape(1, -1, 1), edge_list[i].reshape(1, 1, -1)]\n        # inv_cov_final = torch.linalg.inv(cov_final)\n        inv_cov_final = torch.linalg.inv(cov)\n        return inv_cov_final\n</code></pre>"},{"location":"model/#geospaNN.model.NeighborCovVec","title":"<code>NeighborCovVec</code>","text":"<p>               Bases: <code>MessagePassing</code></p> <p>A message-passing layer that returns covariance vectors for a single batch. For neighbor size k, and batch size b, the message-passing layer will return a bxp tensor, where the ith row is the covariance vector cov(i, N(i)). ...</p> <p>Attributes:</p> Name Type Description <code>neighbor_size</code> <code>int</code> <p>Size of nearest neighbor used. i.e. k in the documentation.</p> <code>theta</code> <code>tuple[float, float, float]</code> <p>theta[0], theta[1], theta[2] represent sigma^2, phi, tau in the exponential covariance family.</p> <p>Methods:</p> Name Description <code>forward</code> <code>message</code> Source code in <code>geospaNN/model.py</code> <pre><code>class NeighborCovVec(MessagePassing):\n    \"\"\"\n    A message-passing layer that returns covariance vectors for a single batch. For neighbor size k, and batch size b, the\n    message-passing layer will return a bxp tensor, where the ith row is the covariance vector cov(i, N(i)).\n    ...\n\n    Attributes:\n        neighbor_size (int):\n            Size of nearest neighbor used. i.e. k in the documentation.\n\n        theta (tuple[float, float, float]):\n            theta[0], theta[1], theta[2] represent sigma^2, phi, tau in the exponential covariance family.\n\n    Methods:\n        forward():\n\n        message():\n    \"\"\"\n    def __init__(self, neighbor_size, theta):\n        super().__init__(aggr=\"sum\")\n        self.neighbor_size = neighbor_size\n        self.theta = theta\n\n    def forward(self, pos, edge_index, edge_attr, batch_size):\n        return self.propagate(edge_index, pos=pos, edge_attr=edge_attr)[range(batch_size), :]\n\n    def message(self, pos_i, pos_j, edge_attr):\n        num_edges = edge_attr.shape[0]\n        msg = torch.zeros(num_edges, self.neighbor_size)\n        col_idc = edge_attr.flatten().int()\n        row_idc = torch.tensor(range(num_edges)).int()\n        msg[row_idc, col_idc] = make_cov_full(distance(pos_i - pos_j, torch.zeros(1, 2)), self.theta).squeeze()\n        return msg\n</code></pre>"},{"location":"model/#geospaNN.model.NeighborInfo","title":"<code>NeighborInfo</code>","text":"<p>               Bases: <code>MessagePassing</code></p> <p>A message-passing layer that collect the output of the nearest neighborhood. For neighbor size k, batch size b, the message-passing layer will return a bxk tensor, where the ith row is the output from the size-p nearest neighborhood of location i. ...</p> <p>Attributes:</p> Name Type Description <code>neighbor_size</code> <code>int</code> <p>Size of nearest neighbor used for NNGP approximation. i.e. k in the documentation.</p> <p>Methods:</p> Name Description <code>forward</code> <code>message</code> Source code in <code>geospaNN/model.py</code> <pre><code>class NeighborInfo(MessagePassing):\n    \"\"\"\n    A message-passing layer that collect the output of the nearest neighborhood. For neighbor size k, batch size b,\n    the message-passing layer will return a bxk tensor, where the ith row is the\n    output from the size-p nearest neighborhood of location i.\n    ...\n\n    Attributes:\n        neighbor_size (int):\n            Size of nearest neighbor used for NNGP approximation. i.e. k in the documentation.\n\n    Methods:\n        forward():\n\n        message():\n    \"\"\"\n    def __init__(self, neighbor_size):\n        super().__init__(aggr=\"sum\")\n        self.neighbor_size = neighbor_size\n\n    def forward(self, y, edge_index, edge_attr, batch_size):\n        out = self.propagate(edge_index, y=y.reshape(-1, 1), edge_attr=edge_attr)[range(batch_size), :]\n        return out\n\n    def message(self, y_j, edge_attr):\n        num_edges = edge_attr.shape[0]\n        msg = torch.zeros(num_edges, self.neighbor_size).double()\n        col_idc = edge_attr.flatten().int()\n        row_idc = torch.tensor(range(num_edges)).int()\n        msg[row_idc, col_idc] = y_j.squeeze().double()\n        return msg\n</code></pre>"},{"location":"model/#geospaNN.model.NeighborPositions","title":"<code>NeighborPositions</code>","text":"<p>               Bases: <code>MessagePassing</code></p> <p>A message-passing layer that collect the coordinates of the nearest neighborhood. For neighbor size k, batch size b, and coordinates' dimension d, the message-passing layer will return a bx(k*d) tensor, where the ith row is the concatenation of k d-dimensional coordinates representing the k-nearest neighborhood of location i. ...</p> <p>Attributes:</p> Name Type Description <code>neighbor_size</code> <code>int</code> <p>Size of nearest neighbor used for NNGP approximation. i.e. k in the documentation.</p> <code>coord_dimension</code> <code>int</code> <p>Dimension of the coordinates, i.e. d in the documentation.</p> <p>Methods:</p> Name Description <code>forward</code> <code>message</code> Source code in <code>geospaNN/model.py</code> <pre><code>class NeighborPositions(MessagePassing):\n    \"\"\"\n    A message-passing layer that collect the coordinates of the nearest neighborhood. For neighbor size k, batch size b,\n    and coordinates' dimension d, the message-passing layer will return a bx(k*d) tensor, where the ith row is the\n    concatenation of k d-dimensional coordinates representing the k-nearest neighborhood of location i.\n    ...\n\n    Attributes:\n        neighbor_size (int):\n            Size of nearest neighbor used for NNGP approximation. i.e. k in the documentation.\n\n        coord_dimension (int):\n            Dimension of the coordinates, i.e. d in the documentation.\n\n    Methods:\n        forward():\n\n        message():\n    \"\"\"\n    def __init__(self, neighbor_size, coord_dimensions):\n        super().__init__(aggr=\"max\")\n        self.neighbor_size = neighbor_size\n        self.coord_dimensions = coord_dimensions\n\n    def forward(self, pos, edge_index, edge_attr, batch_size):\n        positions = self.propagate(edge_index, pos=pos, edge_attr=edge_attr)[range(batch_size), :]\n        zero_index = torch.where(positions == -torch.inf)\n        nz_index = torch.where(positions &gt; -torch.inf)\n        positions[zero_index] = (torch.rand(zero_index[0].shape) + 1) * 10000 * \\\n                                (positions[nz_index].max() - positions[nz_index].min())\n        return positions\n\n    def message(self, pos_j, edge_attr):\n        num_edges = edge_attr.shape[0]\n        msg = torch.zeros(num_edges, self.neighbor_size * self.coord_dimensions)\n        msg[:,:] = -torch.inf\n        col_idc = edge_attr.flatten() * self.coord_dimensions\n        row_idc = torch.tensor(range(num_edges)).int()\n        msg[\n            row_idc.unsqueeze(1), col_idc.unsqueeze(1) + torch.tensor(range(self.coord_dimensions))\n        ] = pos_j\n        return msg\n</code></pre>"},{"location":"model/#geospaNN.model.nngls","title":"<code>nngls</code>","text":"<p>               Bases: <code>Module</code></p> <p>A feed-forward module implementing the NN-GLS algorithm from Zhan et.al 2023. Where the outputs and responses are spatially decorrelated using NNGP approximation proposed by Datta et.al 2016. The decorrelation is implemented by using the message passing (neighborhood aggregation) framework from troch_geometric package.  The aggregation only happens on the output layer, while the main body, i.e. the multi-layer perceptron, allows for flexible choice. ...</p> <p>Attributes:</p> Name Type Description <code>p</code> <code>int</code> <p>Number of features for prediction.</p> <code>neighbor_size</code> <code>int</code> <p>Size of nearest neighbor used for NNGP approximation. i.e. k in the documentation.</p> <code>coord_dimension</code> <code>int</code> <p>Dimension of the coordinates, i.e. d in the documentation.</p> <code>mlp</code> <code>Module</code> <p>Prespecified multi-layer perceptron that takes nxp covariates matrix as the input and nx1 vector as the output. Allows techniques like dropout.</p> <code>compute_covariance_vectors</code> <code>MessagePassing</code> <p>A message-passing layer returns the covariance vector between points and their neighbors. See NeighborCovVec() for more details.</p> <code>compute_inverse_cov_matrices</code> <code>module</code> <p>A feed-forward layer computing the inverses of neighborhood in a vectorized form. See InverseCovMat() for more details.</p> <code>gather_neighbor_positions</code> <code>MessagePassing</code> <p>A message-passing layer that collects the positions of the neighbors in a compact form. See NeighborPositions() for more details.</p> <code>gather_neighbor_outputs</code> <code>MessagePassing</code> <p>Similar to gather_neighbor_positions, the function collects the scalar output (or other quantities) of the neighbors in a compact form. See NeighborInfo() for more details.</p> <p>Methods:</p> Name Description <code>forward</code> <p>Take mini-batch as input and returns a tuple of the [decorrelated response, decorrelated output, original output]. The outcomes are used in the training process defined in nngls_train().</p> <code>estimate</code> <p>Return the estimation of the non-spatial effect with any covariates X. The input X must be of size nxp, where p is the number of features.</p> <code>predict</code> <p>Apply kriging prediction on the testing dataset based on the estimated spatial effect on the training dataset.</p> <p>See Also: nngls_train : Training class for NN-GLS model.  Datta, Abhirup, et al. \"Hierarchical nearest-neighbor Gaussian process models for large geostatistical datasets.\" Journal of the American Statistical Association 111.514 (2016): 800-812.  Datta, Abhirup. \"Sparse nearest neighbor Cholesky matrices in spatial statistics.\" arXiv preprint arXiv:2102.13299 (2021).  ZZhan, Wentao, and Abhirup Datta. 2024. \u201cNeural Networks for Geospatial Data.\u201d Journal of the American Statistical Association, June, 1\u201321. doi:10.1080/01621459.2024.2356293.</p> Source code in <code>geospaNN/model.py</code> <pre><code>class nngls(torch.nn.Module):\n    \"\"\"\n    A feed-forward module implementing the NN-GLS algorithm from Zhan et.al 2023. Where the outputs and responses are\n    spatially decorrelated using NNGP approximation proposed by Datta et.al 2016. The decorrelation is implemented by\n    using the message passing (neighborhood aggregation) framework from troch_geometric package. \n    The aggregation only happens on the output layer, while the main body, i.e. the multi-layer perceptron, allows for\n    flexible choice.\n    ...\n\n    Attributes:\n        p (int):\n            Number of features for prediction.\n        neighbor_size (int):\n            Size of nearest neighbor used for NNGP approximation. i.e. k in the documentation.\n        coord_dimension (int):\n            Dimension of the coordinates, i.e. d in the documentation.\n        mlp (torch.nn.Module):\n            Prespecified multi-layer perceptron that takes nxp covariates matrix as the input and nx1 vector as the output.\n            Allows techniques like dropout.\n        compute_covariance_vectors (MessagePassing):\n            A message-passing layer returns the covariance vector between points and their neighbors. See NeighborCovVec()\n            for more details.\n        compute_inverse_cov_matrices (nn.module):\n            A feed-forward layer computing the inverses of neighborhood in a vectorized form. See InverseCovMat()\n            for more details.\n        gather_neighbor_positions (MessagePassing):\n            A message-passing layer that collects the positions of the neighbors in a compact form. See NeighborPositions()\n            for more details.\n        gather_neighbor_outputs (MessagePassing):\n            Similar to gather_neighbor_positions, the function collects the scalar output (or other quantities) of the neighbors\n            in a compact form. See NeighborInfo() for more details.\n\n    Methods:\n        forward():\n            Take mini-batch as input and returns a tuple of the [decorrelated response, decorrelated output, original output].\n            The outcomes are used in the training process defined in nngls_train().\n        estimate():\n            Return the estimation of the non-spatial effect with any covariates X. The input X must be of size nxp, where p\n            is the number of features.\n        predict():\n            Apply kriging prediction on the testing dataset based on the estimated spatial effect on the training dataset.\n\n\n    See Also:\n    nngls_train : Training class for NN-GLS model. \\\n\n    Datta, Abhirup, et al. \"Hierarchical nearest-neighbor Gaussian process models for large geostatistical datasets.\"\n    Journal of the American Statistical Association 111.514 (2016): 800-812. \\\n\n    Datta, Abhirup. \"Sparse nearest neighbor Cholesky matrices in spatial statistics.\"\n    arXiv preprint arXiv:2102.13299 (2021). \\\n\n    ZZhan, Wentao, and Abhirup Datta. 2024. \u201cNeural Networks for Geospatial Data.\u201d\n    Journal of the American Statistical Association, June, 1\u201321. doi:10.1080/01621459.2024.2356293.\\\n    \"\"\"\n    def __init__(\n            self,\n            p: int,\n            neighbor_size: int,\n            coord_dimensions: int,\n            mlp: torch.nn.Module,\n            theta: tuple[float, float, float]\n    ):\n        super(nngls, self).__init__()\n        self.p = p\n        self.neighbor_size = neighbor_size\n        self.coord_dimensions = coord_dimensions\n        self.theta = torch.nn.Parameter(torch.Tensor(theta))  # split to accelerate?\n        self.compute_covariance_vectors = NeighborCovVec(neighbor_size, self.theta)\n        self.compute_inverse_cov_matrices = InverseCovMat(\n            neighbor_size, coord_dimensions, self.theta\n        )\n        self.gather_neighbor_positions = NeighborPositions(neighbor_size, coord_dimensions)\n        self.gather_neighbor_outputs = NeighborInfo(neighbor_size)\n\n        # Simple MLP to map features to scalars\n        self.mlp = mlp\n\n    def forward(self, batch):\n        \"\"\"Feed-forward step with spatially decorrelated output.\n\n        Parameters:\n            batch: torch_geometric.data.Data\n                A mini-batch of the data contains the x, y, coordinates, and the indexs of edges connecting the nearest neighbors.\n                The mini-batch object can be created by the function split_loader().\n\n        Returns:\n            y_decor: torch.Tensor\n                A decorrelated response vector computed from data.y.\n            o_decor: torch.Tensor\n                A decorrelated output vector computed from the output of the multi-layer perceptron self.mlp(batch.x).\n            o: torch.Tensor\n                The original output vector computed of the multi-layer perceptron.\n        \"\"\"\n        if torch_geometric.__version__ &gt;= '2.4.0':\n            keys = batch.keys()\n        else:\n            keys = batch.keys\n        if 'batch_size' not in keys: #### use batch.keys() in higher version of torch_geom\n            batch.batch_size = batch.x.shape[0]  #### can improve\n        batch = edit_batch(batch)\n        Cov_i_Ni = self.compute_covariance_vectors(batch.pos, batch.edge_index, batch.edge_attr, batch.batch_size)\n        coord_neighbor = self.gather_neighbor_positions(batch.pos, batch.edge_index, batch.edge_attr, batch.batch_size)\n        Inv_Cov_Ni_Ni = self.compute_inverse_cov_matrices(coord_neighbor, batch.edge_list)\n\n        B_i = torch.matmul(Inv_Cov_Ni_Ni, Cov_i_Ni.unsqueeze(2)).squeeze()\n        F_i = self.theta[0]*(1 + self.theta[2]) - torch.sum(B_i * Cov_i_Ni, dim=1)\n\n        y_neighbor = self.gather_neighbor_outputs(batch.y, batch.edge_index, batch.edge_attr, batch.batch_size)\n        y_decor = (batch.y[range(batch.batch_size)] - torch.sum(y_neighbor * B_i, dim=1)) / torch.sqrt(F_i)\n        o = self.mlp(batch.x).squeeze().reshape(-1)\n        o_neighbor = self.gather_neighbor_outputs(o, batch.edge_index, batch.edge_attr, batch.batch_size)\n        o_decor = (o[range(batch.batch_size)] - torch.sum(o_neighbor * B_i, dim=1)) / torch.sqrt(F_i)\n\n        return y_decor, o_decor, o\n\n    def estimate(self, X: torch.Tensor\n                 ) -&gt; torch.Tensor:\n        \"\"\"Estimate the non-spatial effect with covariates X,\n\n        Parameters:\n            X:\n                A nxp matrix where p is the number of features.\n\n        Returns:\n            estimation\n        \"\"\"\n        assert X.shape[1] == self.p\n        with torch.no_grad():\n            return self.mlp(X).squeeze()\n\n    def predict(self,\n                data_train: torch_geometric.data.Data,\n                data_test: torch_geometric.data.Data,\n                CI:Optional[bool] = False, **kwargs\n                ):\n        \"\"\"Kriging prediction on a test dataset.\n\n        The function provides spatial prediction with the following steps.\n        1: Apply the multi-layer perceptron on the training data for a fixed effect estimation.\n        2: Compute the training residual as the estimated spatial effect (#### to implement: and estimate the spatial parameters).\n        3: Use NNGP-approximated-kriging to predict the spatial effect and it's confidence interval.\n        See krig_pred() for more details.\n        4: Provide the overall prediction by combining the spatial effect prediction and fixed effect estimation.\n\n        Parameters:\n            data_train:\n                Training data containing x, y and spatial coordinates, can be the output of split_data() or make_graph().\n            data_test:\n                Testing data containing x, y and spatial coordinates, can be the output of split_data() or make_graph().\n            CI:\n                A boolean value indicating whether to provide the 95% confidence intervals. (#### confidence level to add)\n\n        Returns:\n            if CI is True:\n                tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n                A tuple contains the prediction, confidence upper bound and confidence lower bound.\n            else:\n                torch.Tensor:\n                only contains the prediction.\n\n        See Also:\n            krig_pred: Kriging prediction (Gaussian process regression) with confidence interval. \\\n\n            Zhan, Wentao, and Abhirup Datta. 2024. \u201cNeural Networks for Geospatial Data.\u201d\n            Journal of the American Statistical Association, June, 1\u201321. doi:10.1080/01621459.2024.2356293.\n        \"\"\"\n        with torch.no_grad():\n            w_train = data_train.y - self.estimate(data_train.x)\n            if CI:\n                w_test, w_u, w_l = krig_pred(w_train, data_train.pos, data_test.pos, self.theta, **kwargs)\n                estimation_test = self.estimate(data_test.x)\n                return [estimation_test + w_test, estimation_test + w_u, estimation_test + w_l]\n            else:\n                w_test, _, _ = krig_pred(w_train, data_train.pos, data_test.pos, self.theta, **kwargs)\n                estimation_test = self.estimate(data_test.x)\n                return estimation_test + w_test\n</code></pre>"},{"location":"model/#geospaNN.model.nngls.estimate","title":"<code>estimate(X)</code>","text":"<p>Estimate the non-spatial effect with covariates X,</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Tensor</code> <p>A nxp matrix where p is the number of features.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>estimation</p> Source code in <code>geospaNN/model.py</code> <pre><code>def estimate(self, X: torch.Tensor\n             ) -&gt; torch.Tensor:\n    \"\"\"Estimate the non-spatial effect with covariates X,\n\n    Parameters:\n        X:\n            A nxp matrix where p is the number of features.\n\n    Returns:\n        estimation\n    \"\"\"\n    assert X.shape[1] == self.p\n    with torch.no_grad():\n        return self.mlp(X).squeeze()\n</code></pre>"},{"location":"model/#geospaNN.model.nngls.forward","title":"<code>forward(batch)</code>","text":"<p>Feed-forward step with spatially decorrelated output.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <p>torch_geometric.data.Data A mini-batch of the data contains the x, y, coordinates, and the indexs of edges connecting the nearest neighbors. The mini-batch object can be created by the function split_loader().</p> required <p>Returns:</p> Name Type Description <code>y_decor</code> <p>torch.Tensor A decorrelated response vector computed from data.y.</p> <code>o_decor</code> <p>torch.Tensor A decorrelated output vector computed from the output of the multi-layer perceptron self.mlp(batch.x).</p> <code>o</code> <p>torch.Tensor The original output vector computed of the multi-layer perceptron.</p> Source code in <code>geospaNN/model.py</code> <pre><code>def forward(self, batch):\n    \"\"\"Feed-forward step with spatially decorrelated output.\n\n    Parameters:\n        batch: torch_geometric.data.Data\n            A mini-batch of the data contains the x, y, coordinates, and the indexs of edges connecting the nearest neighbors.\n            The mini-batch object can be created by the function split_loader().\n\n    Returns:\n        y_decor: torch.Tensor\n            A decorrelated response vector computed from data.y.\n        o_decor: torch.Tensor\n            A decorrelated output vector computed from the output of the multi-layer perceptron self.mlp(batch.x).\n        o: torch.Tensor\n            The original output vector computed of the multi-layer perceptron.\n    \"\"\"\n    if torch_geometric.__version__ &gt;= '2.4.0':\n        keys = batch.keys()\n    else:\n        keys = batch.keys\n    if 'batch_size' not in keys: #### use batch.keys() in higher version of torch_geom\n        batch.batch_size = batch.x.shape[0]  #### can improve\n    batch = edit_batch(batch)\n    Cov_i_Ni = self.compute_covariance_vectors(batch.pos, batch.edge_index, batch.edge_attr, batch.batch_size)\n    coord_neighbor = self.gather_neighbor_positions(batch.pos, batch.edge_index, batch.edge_attr, batch.batch_size)\n    Inv_Cov_Ni_Ni = self.compute_inverse_cov_matrices(coord_neighbor, batch.edge_list)\n\n    B_i = torch.matmul(Inv_Cov_Ni_Ni, Cov_i_Ni.unsqueeze(2)).squeeze()\n    F_i = self.theta[0]*(1 + self.theta[2]) - torch.sum(B_i * Cov_i_Ni, dim=1)\n\n    y_neighbor = self.gather_neighbor_outputs(batch.y, batch.edge_index, batch.edge_attr, batch.batch_size)\n    y_decor = (batch.y[range(batch.batch_size)] - torch.sum(y_neighbor * B_i, dim=1)) / torch.sqrt(F_i)\n    o = self.mlp(batch.x).squeeze().reshape(-1)\n    o_neighbor = self.gather_neighbor_outputs(o, batch.edge_index, batch.edge_attr, batch.batch_size)\n    o_decor = (o[range(batch.batch_size)] - torch.sum(o_neighbor * B_i, dim=1)) / torch.sqrt(F_i)\n\n    return y_decor, o_decor, o\n</code></pre>"},{"location":"model/#geospaNN.model.nngls.predict","title":"<code>predict(data_train, data_test, CI=False, **kwargs)</code>","text":"<p>Kriging prediction on a test dataset.</p> <p>The function provides spatial prediction with the following steps. 1: Apply the multi-layer perceptron on the training data for a fixed effect estimation. 2: Compute the training residual as the estimated spatial effect (#### to implement: and estimate the spatial parameters). 3: Use NNGP-approximated-kriging to predict the spatial effect and it's confidence interval. See krig_pred() for more details. 4: Provide the overall prediction by combining the spatial effect prediction and fixed effect estimation.</p> <p>Parameters:</p> Name Type Description Default <code>data_train</code> <code>Data</code> <p>Training data containing x, y and spatial coordinates, can be the output of split_data() or make_graph().</p> required <code>data_test</code> <code>Data</code> <p>Testing data containing x, y and spatial coordinates, can be the output of split_data() or make_graph().</p> required <code>CI</code> <code>Optional[bool]</code> <p>A boolean value indicating whether to provide the 95% confidence intervals. (#### confidence level to add)</p> <code>False</code> <p>Returns:</p> Name Type Description <p>if CI is True: tuple[torch.Tensor, torch.Tensor, torch.Tensor]: A tuple contains the prediction, confidence upper bound and confidence lower bound.</p> <code>else</code> <p>torch.Tensor: only contains the prediction.</p> See Also <p>krig_pred: Kriging prediction (Gaussian process regression) with confidence interval.  Zhan, Wentao, and Abhirup Datta. 2024. \u201cNeural Networks for Geospatial Data.\u201d Journal of the American Statistical Association, June, 1\u201321. doi:10.1080/01621459.2024.2356293.</p> Source code in <code>geospaNN/model.py</code> <pre><code>def predict(self,\n            data_train: torch_geometric.data.Data,\n            data_test: torch_geometric.data.Data,\n            CI:Optional[bool] = False, **kwargs\n            ):\n    \"\"\"Kriging prediction on a test dataset.\n\n    The function provides spatial prediction with the following steps.\n    1: Apply the multi-layer perceptron on the training data for a fixed effect estimation.\n    2: Compute the training residual as the estimated spatial effect (#### to implement: and estimate the spatial parameters).\n    3: Use NNGP-approximated-kriging to predict the spatial effect and it's confidence interval.\n    See krig_pred() for more details.\n    4: Provide the overall prediction by combining the spatial effect prediction and fixed effect estimation.\n\n    Parameters:\n        data_train:\n            Training data containing x, y and spatial coordinates, can be the output of split_data() or make_graph().\n        data_test:\n            Testing data containing x, y and spatial coordinates, can be the output of split_data() or make_graph().\n        CI:\n            A boolean value indicating whether to provide the 95% confidence intervals. (#### confidence level to add)\n\n    Returns:\n        if CI is True:\n            tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n            A tuple contains the prediction, confidence upper bound and confidence lower bound.\n        else:\n            torch.Tensor:\n            only contains the prediction.\n\n    See Also:\n        krig_pred: Kriging prediction (Gaussian process regression) with confidence interval. \\\n\n        Zhan, Wentao, and Abhirup Datta. 2024. \u201cNeural Networks for Geospatial Data.\u201d\n        Journal of the American Statistical Association, June, 1\u201321. doi:10.1080/01621459.2024.2356293.\n    \"\"\"\n    with torch.no_grad():\n        w_train = data_train.y - self.estimate(data_train.x)\n        if CI:\n            w_test, w_u, w_l = krig_pred(w_train, data_train.pos, data_test.pos, self.theta, **kwargs)\n            estimation_test = self.estimate(data_test.x)\n            return [estimation_test + w_test, estimation_test + w_u, estimation_test + w_l]\n        else:\n            w_test, _, _ = krig_pred(w_train, data_train.pos, data_test.pos, self.theta, **kwargs)\n            estimation_test = self.estimate(data_test.x)\n            return estimation_test + w_test\n</code></pre>"},{"location":"model/#geospaNN.model.linear_gls","title":"<code>linear_gls(data_train, neighbor_size=20)</code>","text":"<p>Spatial linear mixed model for geospatial data</p> <p>Find the upper triangular matrix B and diagonal matrix F such that (I-B)'F^{-1}(I-B) appriximate the precision matrix (inverse of the covariance matrix). The level of approximation increase with the neighbor size. When using the full neighbor, the NNGP appriximation degrade to the Cholesky decomposition. (see https://arxiv.org/abs/2102.13299 for more details.)</p> <p>Parameters:</p> Name Type Description Default <code>data_train</code> <code>Data</code> <pre><code>Training data containing x, y and spatial coordinates, can be the output of split_data() or make_graph().\n</code></pre> required <code>neighbor_size</code> <code>Optional[int]</code> <pre><code>Number of nearest neighbors used for NNGP approximation, default value is 20.\n</code></pre> <code>20</code> <p>Returns:</p> Name Type Description <code>model</code> <code>nngls</code> <p>nngls A model in the class of nngls, with additional attribute model.var containing the estimated variance of linear coefficients</p> See Also <p>Saha, Arkajyoti, and Abhirup Datta. \"BRISC: bootstrap for rapid inference on spatial covariances.\" Stat 7.1 (2018): e184.</p> Source code in <code>geospaNN/model.py</code> <pre><code>def linear_gls(data_train: torch_geometric.data.Data,\n               neighbor_size: Optional[int] = 20\n               ) -&gt; nngls:\n    \"\"\"Spatial linear mixed model for geospatial data\n\n    Find the upper triangular matrix B and diagonal matrix F such that (I-B)'F^{-1}(I-B) appriximate the precision matrix\n    (inverse of the covariance matrix). The level of approximation increase with the neighbor size. When using the full neighbor,\n    the NNGP appriximation degrade to the Cholesky decomposition. (see https://arxiv.org/abs/2102.13299 for more details.)\n\n    Parameters:\n        data_train:\n                Training data containing x, y and spatial coordinates, can be the output of split_data() or make_graph().\n        neighbor_size:\n                Number of nearest neighbors used for NNGP approximation, default value is 20.\n\n    Returns:\n        model: nngls\n            A model in the class of nngls, with additional attribute model.var containing the estimated\n            variance of linear coefficients\n\n    See Also:\n        Saha, Arkajyoti, and Abhirup Datta. \"BRISC: bootstrap for rapid inference on spatial covariances.\"\n        Stat 7.1 (2018): e184.\n    \"\"\"\n    x = torch.concat([torch.ones(data_train.x.shape[0], 1), data_train.x],axis=1)\n    beta, theta_hat_BRISC = BRISC_estimation(data_train.y.detach().numpy(),\n                                             x.detach().numpy(),\n                                             data_train.pos.detach().numpy())\n    def mlp_BRISC(X):\n        return beta[0] + torch.Tensor(beta[1:]) * X\n\n    model = nngls(p=data_train.x.shape[1], neighbor_size=15, coord_dimensions=2, mlp=mlp_BRISC, theta=torch.tensor(theta_hat_BRISC))\n    cov = make_cov(data_train.pos, theta_hat_BRISC.theta, neighbor_size = neighbor_size)\n    Z = cov.decorrelate(x)\n    model.var = torch.linalg.pinv(Z.T @ Z)\n    return model\n</code></pre>"},{"location":"reference/","title":"Documentation","text":"<p>This is a full documentation for geospaNN package, the functions are devided into three Modules.</p> <ol> <li>Module utils contains most of the utility function used to build the package. Most of the functions works around NNGP approximation to accerte the computation.</li> <li>Module model contains model class for NN-GLS as well as normal Neural Networks. Also, graph convolution architectures are defined in this module.</li> <li>Module main contains major wrappers for the training process.</li> </ol>"},{"location":"start/","title":"How to start","text":""},{"location":"start/#installation","title":"Installation","text":""},{"location":"start/#create-and-enter-virtual-environment-recommended","title":"Create and enter virtual environment (recommended)","text":"<p>Step 1: If you haven't installed anaconda on your machine, refer to this doc, follow the instruction,  and install the right version.</p> <p>Step 2: Create the conda virtual environment. Refer to this doc. Example:</p> <pre><code># bash\nconda create -n [name of your environment] python=3.10\n</code></pre> <p>Step 3: Enter the virtual environment by running:</p> <pre><code># bash\nconda activate [name of your environment]\n</code></pre> <p>Step 4: In the current version of geospaNN, to use the R-package BRISC  for spatial parameter estimation (through rpy2), we need R installed in the environment. In order to install R, simply run:</p> <pre><code># bash\nconda install r-base\n</code></pre> <p>If you already have native R installed, it's also possible to manually initialize R for rpy2.  See here for more details.</p>"},{"location":"start/#manual-dependency-installation","title":"Manual dependency installation","text":"<p>(Currently) to install the development version of the package, a pre-installed PyTorch and PyG libraries are needed. We provide options to install PyG libraries using conda and pip.</p>"},{"location":"start/#option-1-using-conda","title":"Option 1: Using Conda","text":"<p>For conda, installation in the following order is recommended. It may take around 10 minutes for conda to solve the environment for pytorch-sparse. The following chunk has been tested in a python 3.10 environment.</p> <pre><code>#bash\nconda install pytorch torchvision -c pytorch\nconda install pyg -c pyg        \nconda install pytorch-sparse -c pyg \n</code></pre>"},{"location":"start/#option-2-using-pip","title":"Option 2: Using pip","text":"<p>For pip, installation in the following order is recommended to avoid any compilation issue. It may take around 15 minutes to finish the installation. The following chunk has been tested in a python 3.10 environment.</p> <pre><code># bash\npip install numpy==1.26 --no-cache-dir\npip install torch==2.0.0 --no-cache-dir\npip install torch-scatter -f https://data.pyg.org/whl/torch-2.0.0.html --no-cache-dir\npip install torch-sparse -f https://data.pyg.org/whl/torch-2.0.0.html --no-cache-dir\npip install torch-cluster -f https://data.pyg.org/whl/torch-2.0.0.html --no-cache-dir\npip install torch_geometric --no-cache-dir\n</code></pre>"},{"location":"start/#main-installation","title":"Main installation","text":"<p>Once PyTorch and PyG are successfully installed, use the following command in the terminal for the latest version (version 11/2024):</p> <pre><code>pip install https://github.com/WentaoZhan1998/geospaNN/archive/main.zip\n</code></pre> <p>To install the pypi version, use the following command in the terminal (version 1/2024):</p> <pre><code>pip install geospaNN\n</code></pre>"},{"location":"start/#an-easy-running-sample","title":"An easy running sample:","text":"<p>First, run python in the terminal:</p> <pre><code>python\n</code></pre> <p>import the modules and set up the parameters</p> <ol> <li>Define the Friedman's function, and specify the dimension of input covariates.</li> <li>Set the parameters for the spatial process.</li> <li>Set the hyperparameters of the data.</li> </ol> <pre><code>import torch\nimport geospaNN\nimport numpy as np\n\n# 1. \ndef f5(X): return (10*np.sin(np.pi*X[:,0]*X[:,1]) + 20*(X[:,2]-0.5)**2 + 10*X[:,3] +5*X[:,4])/6\n\np = 5; funXY = f5\n\n# 2.\nsigma = 1\nphi = 3/np.sqrt(2)\ntau = 0.01\ntheta = torch.tensor([sigma, phi, tau])\n\n# 3.\nn = 1000            # Size of the simulated sample.\nnn = 20             # Neighbor size used for NNGP.\nbatch_size = 50     # Batch size for training the neural networks.\n</code></pre> <p>Next, simulate and split the data.</p> <ol> <li>Simulate the spatially correlated data with spatial coordinates randomly sampled on a [0, 10]^2 squared domain.</li> <li>Order the spatial locations by max-min ordering.</li> <li>Build the nearest neighbor graph, as a torch_geometric.data.Data object.</li> <li>Split data into training, validation, testing sets.</li> </ol> <pre><code># 1.\ntorch.manual_seed(2024)\nX, Y, coord, cov, corerr = geospaNN.Simulation(n, p, nn, funXY, theta, range=[0, 10])\n\n# 2.\nX, Y, coord, _ = geospaNN.spatial_order(X, Y, coord, method = 'max-min')\n\n# 3.\ndata = geospaNN.make_graph(X, Y, coord, nn)\n\n# 4.\ndata_train, data_val, data_test = geospaNN.split_data(X, Y, coord, neighbor_size=20,\n                                                   test_proportion=0.2)\n</code></pre> <p>Compose the mlp structure and train easily.</p> <ol> <li>Define the mlp structure (torch.nn) to use.</li> <li>Define the NN-GLS corresponding model.</li> <li>Define the NN-GLS training class with learning rate and tolerance.</li> <li>Train the model.</li> </ol> <pre><code># 1.             \nmlp = torch.nn.Sequential(\n    torch.nn.Linear(p, 50),\n    torch.nn.ReLU(),\n    torch.nn.Linear(50, 20),\n    torch.nn.ReLU(),\n    torch.nn.Linear(20, 10),\n    torch.nn.ReLU(),\n    torch.nn.Linear(10, 1),\n)\n\n# 2.\nmodel = geospaNN.nngls(p=p, neighbor_size=nn, coord_dimensions=2, mlp=mlp, theta=torch.tensor([1.5, 5, 0.1]))\n\n# 3.\nnngls_model = geospaNN.nngls_train(model, lr =  0.01, min_delta = 0.001)\n\n# 4.\ntraining_log = nngls_model.train(data_train, data_val, data_test,\n                                 Update_init = 10, Update_step = 10)\n</code></pre> <p>Estimation from the model. The variable is a torch.Tensor object of the same dimension</p> <pre><code>train_estimate = model.estimate(data_train.x)\n</code></pre> <p>Kriging prediction from the model. The first variable is supposed to be the data used for training, and the second  variable a torch_geometric.data.Data object which can be composed by geospaNN.make_graph()'.</p> <pre><code>test_predict = model.predict(data_train, data_test)\n</code></pre>"},{"location":"utils/","title":"Utils","text":""},{"location":"utils/#geospaNN.utils.DropoutLayer","title":"<code>DropoutLayer</code>","text":"<p>               Bases: <code>Module</code></p> <p>Customized dropout layer where the nodes values are dropped with probability p.</p> <p>Attributes:</p> Name Type Description <code>p</code> <code>float</code> <p>The drop probability</p> Source code in <code>geospaNN/utils.py</code> <pre><code>class DropoutLayer(torch.nn.Module):\n    \"\"\"\n    Customized dropout layer where the nodes values are dropped with probability p.\n\n    Attributes:\n        p (float):\n            The drop probability\n    \"\"\"\n    def __init__(self, p: float):\n        super().__init__()\n        self.p = p\n\n    def forward(self, input):\n        if self.training:\n            u1 = (np.random.rand(*input.shape)&lt;self.p) / self.p\n            u1 *= u1\n            return u1\n        else:\n            input *= self.p\n</code></pre>"},{"location":"utils/#geospaNN.utils.EarlyStopping","title":"<code>EarlyStopping</code>","text":"<p>Early stopping to stop the training when the loss does not improve after certain epochs.</p> <p>Attributes:</p> Name Type Description <code>patience</code> <code>int</code> <p>How many epochs to wait before stopping when loss is not improving</p> <code>min_delta</code> <code>float</code> <p>Minimum difference between new loss and old loss for new loss to be considered as an improvement</p> Source code in <code>geospaNN/utils.py</code> <pre><code>class EarlyStopping():\n    \"\"\"\n    Early stopping to stop the training when the loss does not improve after\n    certain epochs.\n\n    Attributes:\n        patience (int):\n            How many epochs to wait before stopping when loss is not improving\n        min_delta (float):\n            Minimum difference between new loss and old loss for new loss to be considered as an improvement\n    \"\"\"\n    def __init__(self,\n                 patience: Optional[int] = 5,\n                 min_delta: Optional[float] = 0):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.counter = 0\n        self.best_loss = None\n        self.early_stop = False\n    def __call__(self, val_loss):\n        if self.best_loss == None:\n            self.best_loss = val_loss\n        elif self.best_loss - val_loss &gt; self.min_delta or math.isnan(self.best_loss):\n            self.best_loss = val_loss\n            # reset counter if validation loss improves\n            self.counter = 0\n        elif self.best_loss - val_loss &lt; self.min_delta:\n            self.counter += 1\n            #print(f\"INFO: Early stopping counter {self.counter} of {self.patience}\")\n            if self.counter &gt;= self.patience:\n                print('INFO: Early stopping')\n                self.early_stop = True\n</code></pre>"},{"location":"utils/#geospaNN.utils.LRScheduler","title":"<code>LRScheduler</code>","text":"<p>Learning rate scheduler. If the validation loss does not decrease for the given number of <code>patience</code> epochs, then the learning rate will decrease by by given <code>factor</code>.</p> <p>Attributes:</p> Name Type Description <code>optimizer</code> <p>the optimizer we are using</p> <code>patience</code> <code>int</code> <p>How many epochs to wait before updating the lr. Default being 5.</p> <code>min_lr</code> <code>float</code> <p>Least lr value to reduce to while updating.</p> <code>factor</code> <code>float</code> <p>Factor by which the lr should be updated, i.e. new_lr = old_lr * factor.</p> Source code in <code>geospaNN/utils.py</code> <pre><code>class LRScheduler():\n    \"\"\"\n    Learning rate scheduler. If the validation loss does not decrease for the\n    given number of `patience` epochs, then the learning rate will decrease by\n    by given `factor`.\n\n    Attributes:\n        optimizer:\n            the optimizer we are using\n        patience (int):\n            How many epochs to wait before updating the lr. Default being 5.\n        min_lr (float):\n            Least lr value to reduce to while updating.\n        factor (float):\n            Factor by which the lr should be updated, i.e. new_lr = old_lr * factor.\n    \"\"\"\n    def __init__(\n        self,\n            optimizer,\n            patience: Optional[int] = 5,\n            min_lr: Optional[float] = 1e-6,\n            factor: Optional[float] = 0.5\n    ):\n        self.optimizer = optimizer\n        self.patience = patience\n        self.min_lr = min_lr\n        self.factor = factor\n        self.lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n                self.optimizer,\n                mode='min',\n                patience=self.patience,\n                factor=self.factor,\n                min_lr=self.min_lr,\n                verbose=True\n            )\n    def __call__(self, val_loss):\n        self.lr_scheduler.step(val_loss)\n</code></pre>"},{"location":"utils/#geospaNN.utils.NNGP_cov","title":"<code>NNGP_cov</code>","text":"<p>               Bases: <code>Sparse_B</code></p> <p>A subclass of Sparse_B designed for the decorrelation using NNGP. The whole object is an NNGP approximation of the inverse square-root of a covariance matrix \\Sigma. i.d. F^{-1/2}(I-B) ~ \\Sigma^{-1/2} ...</p> <p>Attributes:</p> Name Type Description <code>F_diag</code> <p>torch.Tensor A vector of length n representing the diagonal matrix F.</p> <p>Methods:</p> Name Description <code>correlate</code> <p>Approximately correlate the matrix X to \\Sigma^{1/2}X.</p> <code>decorrelate</code> <p>Approximately decorrelate the matrix X to \\Sigma^{-1/2}X.</p> Source code in <code>geospaNN/utils.py</code> <pre><code>class NNGP_cov(Sparse_B):\n    \"\"\"\n    A subclass of Sparse_B designed for the decorrelation using NNGP. The whole object is an NNGP approximation of the\n    inverse square-root of a covariance matrix \\Sigma. i.d. F^{-1/2}(I-B) ~ \\Sigma^{-1/2}\n    ...\n\n    Attributes:\n        F_diag : torch.Tensor\n            A vector of length n representing the diagonal matrix F.\n\n    Methods:\n        correlate(x):\n            Approximately correlate the matrix X to \\Sigma^{1/2}X.\n\n        decorrelate(x):\n            Approximately decorrelate the matrix X to \\Sigma^{-1/2}X.\n    \"\"\"\n    def __init__(self, B, F_diag, Ind_list):\n        super().__init__(B = B, Ind_list = Ind_list)\n        assert len(F_diag) == B.shape[0]\n        self.F_diag = F_diag\n\n    def correlate(self, x: torch.Tensor\n                  ) -&gt; torch.Tensor:\n        \"\"\"\n        Approximately correlate the matrix X to \\Sigma^{1/2}X by calculating (I_B)^{-1}F^{1/2}X.\n\n        Parameters:\n            x : Input tensor X\n\n        Returns:\n            x_cor: Correlated X\n        \"\"\"\n        assert x.shape[0] == self.n\n        return self.invmul(torch.sqrt(self.F_diag) * x)\n\n    def decorrelate(self, x: torch.Tensor\n                    ) -&gt; torch.Tensor:\n        \"\"\"\n        Approximately decorrelate the matrix X to \\Sigma^{-1/2}X by calculating F^{-1/2}(I_B)X.\n\n        Parameters:\n            x : Input tensor X\n\n        Returns:\n            x_decor: Decorrelated X\n        \"\"\"\n        assert x.shape[0] == self.n\n        return torch.sqrt(torch.reciprocal(self.F_diag)).reshape(-1,1)*self.matmul(x)\n</code></pre>"},{"location":"utils/#geospaNN.utils.NNGP_cov.correlate","title":"<code>correlate(x)</code>","text":"<p>Approximately correlate the matrix X to \\Sigma^{1/2}X by calculating (I_B)^{-1}F^{1/2}X.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <p>Input tensor X</p> required <p>Returns:</p> Name Type Description <code>x_cor</code> <code>Tensor</code> <p>Correlated X</p> Source code in <code>geospaNN/utils.py</code> <pre><code>def correlate(self, x: torch.Tensor\n              ) -&gt; torch.Tensor:\n    \"\"\"\n    Approximately correlate the matrix X to \\Sigma^{1/2}X by calculating (I_B)^{-1}F^{1/2}X.\n\n    Parameters:\n        x : Input tensor X\n\n    Returns:\n        x_cor: Correlated X\n    \"\"\"\n    assert x.shape[0] == self.n\n    return self.invmul(torch.sqrt(self.F_diag) * x)\n</code></pre>"},{"location":"utils/#geospaNN.utils.NNGP_cov.decorrelate","title":"<code>decorrelate(x)</code>","text":"<p>Approximately decorrelate the matrix X to \\Sigma^{-1/2}X by calculating F^{-1/2}(I_B)X.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <p>Input tensor X</p> required <p>Returns:</p> Name Type Description <code>x_decor</code> <code>Tensor</code> <p>Decorrelated X</p> Source code in <code>geospaNN/utils.py</code> <pre><code>def decorrelate(self, x: torch.Tensor\n                ) -&gt; torch.Tensor:\n    \"\"\"\n    Approximately decorrelate the matrix X to \\Sigma^{-1/2}X by calculating F^{-1/2}(I_B)X.\n\n    Parameters:\n        x : Input tensor X\n\n    Returns:\n        x_decor: Decorrelated X\n    \"\"\"\n    assert x.shape[0] == self.n\n    return torch.sqrt(torch.reciprocal(self.F_diag)).reshape(-1,1)*self.matmul(x)\n</code></pre>"},{"location":"utils/#geospaNN.utils.Sparse_B","title":"<code>Sparse_B</code>","text":"<p>A sparse representation of the lower-triangular neighboring nxn matrix B_dense, where each row contains at most p non-zero values.</p> <p>Attributes:</p> Name Type Description <code>B</code> <p>nxp array contains all non-zero values in B_dense.</p> <code>n</code> <p>The number of samples.</p> <code>neighbor_size</code> <p>i.e. k in the documentation, the largest number of non-zero values in each row of B_dense.</p> <code>Ind_list</code> <p>The nxp index array indicating the location where values in B was in B_dense. For example, the [i,j]'s index is k means that B_dense[i,k] = B[i,j].</p> <p>Methods:</p> Name Description <code>to_numpy</code> <p>Transform B to np.array</p> <code>to_tensor</code> <p>Transform B to torch.Tensor</p> <code>matmul</code> <p>Calculate the matrix product of B_dense[idx,:] and X</p> <code>invmul</code> <p>Calculate the matrix-vector product of B_dense^{-1} and y. Only used when the diagonal of B_dense is constantly 1.</p> <code>Fmul</code> <p>Return a new Sparse_B object where B_dense is replaced by the matrix product of F * B_dense. F_diag is the vector representation of the nxn diagonal matrix F.</p> <code>to_dense</code> <p>Return the dense form of B_dense as an np.array object.</p> Source code in <code>geospaNN/utils.py</code> <pre><code>class Sparse_B():\n    \"\"\"\n    A sparse representation of the lower-triangular neighboring nxn matrix B_dense,\n    where each row contains at most p non-zero values.\n\n    Attributes:\n        B :\n            nxp array contains all non-zero values in B_dense.\n        n : \n            The number of samples.\n        neighbor_size :\n            i.e. k in the documentation, the largest number of non-zero values in each row of B_dense.\n        Ind_list :\n            The nxp index array indicating the location where values in B was in B_dense. For example, the [i,j]'s index is k\n            means that B_dense[i,k] = B[i,j].\n\n    Methods:\n        to_numpy():\n            Transform B to np.array\n\n        to_tensor():\n            Transform B to torch.Tensor\n\n        matmul(X, idx = None):\n            Calculate the matrix product of B_dense[idx,:] and X\n\n        invmul(y):\n            Calculate the matrix-vector product of B_dense^{-1} and y. Only used when the diagonal of B_dense is constantly 1.\n\n        Fmul(F_diag):\n            Return a new Sparse_B object where B_dense is replaced by the matrix product of F * B_dense.\n            F_diag is the vector representation of the nxn diagonal matrix F.\n\n        to_dense():\n            Return the dense form of B_dense as an np.array object.\n    \"\"\"\n    def __init__(self,\n                 B: torch.Tensor | np.array,\n                 Ind_list: np.array):\n        self.B = B\n        self.n = B.shape[0]\n        self.neighbor_size = B.shape[1]\n        self.Ind_list = Ind_list.astype(int)\n\n    def to_numpy(self):\n        if torch.is_tensor(self.B):\n           self.B = self.B.detach().numpy()\n        return self\n\n    def to_tensor(self):\n        if isinstance(self.B, np.ndarray):\n            self.B = torch.from_numpy(self.B).float()\n        return self\n\n    def matmul(self, X, idx = None):\n        if idx == None: idx = np.array(range(self.n))\n        if torch.is_tensor(X):\n            self.to_tensor()\n            if X.dim() == 1:\n                result = torch.empty((len(idx)))\n                for k in range(len(idx)):\n                    i = idx[k]\n                    ind = self.Ind_list[i, :][self.Ind_list[i, :] &gt;= 0]\n                    result[k] = torch.dot(self.B[i, range(len(ind))].reshape(-1), X[ind])\n            elif X.dim() == 2:\n                result = torch.empty((len(idx), X.shape[1]))\n                for k in range(len(idx)):\n                    i = idx[k]\n                    ind = self.Ind_list[i, :][self.Ind_list[i, :] &gt;= 0]\n                    #result[i,:] = np.dot(self.B[i, range(len(ind))].reshape(-1), C_Ni[ind, :])\n                    result[k,:] = torch.matmul(self.B[i, range(len(ind))].reshape(-1), X[ind,:])\n#            result = torch.empty((len(idx)))\n#            for k in range(len(idx)):\n#                i = idx[k]\n#                ind = self.Ind_list[i,:][self.Ind_list[i,:] &gt;= 0]\n#                result[k] = torch.dot(self.B[i,range(len(ind))].squeeze(),X[ind])\n        elif isinstance(X, np.ndarray):\n            self.to_numpy()\n            if np.ndim(X) == 1:\n                result = np.empty((len(idx)))\n                for k in range(len(idx)):\n                    i = idx[k]\n                    ind = self.Ind_list[i, :][self.Ind_list[i, :] &gt;= 0]\n                    result[k] = np.dot(self.B[i, range(len(ind))].reshape(-1), X[ind])\n            elif np.ndim(X) == 2:\n                result = np.empty((len(idx), X.shape[1]))\n                for k in range(len(idx)):\n                    i = idx[k]\n                    ind = self.Ind_list[i, :][self.Ind_list[i, :] &gt;= 0]\n                    #result[i,:] = np.dot(self.B[i, range(len(ind))].reshape(-1), C_Ni[ind, :])\n                    result[k,:] = np.dot(self.B[i, range(len(ind))].reshape(-1), X[ind,:])\n        return result\n\n    def invmul(self, y):\n        if isinstance(y, np.ndarray):\n            y = torch.from_numpy(y).float()\n        y = y.reshape(-1)\n        assert self.n == y.shape[0]\n        x = y[0].unsqueeze(0)\n        assert (self.B[:,0] == torch.ones(self.n)).all(), 'Only applies to I-B matrix'\n        Indlist = self.Ind_list[:, 1:]\n        B = -self.B[:, 1:]\n        for i in range(1, self.n):\n            ind = Indlist[i, :]\n            id = ind &gt;= 0\n            if sum(id) == 0:\n                x = torch.cat((x, y[i].unsqueeze(0)), dim = -1)\n            else:\n                x = torch.cat((x, (y[i] + torch.dot(x[ind[id]], B[i, :][id])).unsqueeze(0)), dim = -1)\n        return x\n\n    def Fmul(self, F_diag):\n        res = Sparse_B(self.B.copy(), self.Ind_list.copy())\n        for i in range(self.n):\n            res.B[i,:] = F_diag[i]*self.B[i,:]\n        return res\n\n    def to_dense(self):\n        B = np.zeros((self.n, self.n))\n        for i in range(self.n):\n            ind = self.Ind_list[i, :][self.Ind_list[i, :] &gt;= 0]\n            if len(ind) == 0:\n                continue\n            B[i, ind] = self.B[i,range(len(ind))]\n        return B\n</code></pre>"},{"location":"utils/#geospaNN.utils.Simulation","title":"<code>Simulation(n, p, neighbor_size, fx, theta, coord=None, range=[0, 1], X_pattern='uniform', sparse=True)</code>","text":"<p>Simulate spatial data</p> <p>Simulate the spatial data based on the following model: Y(s) = f(X(s)) + w(s) + delta(s), where s are the spatial locations, X(s) is the spatial covariates, w(s) is the spatial effect (correlated noise), and delta(s) is the i.i.d random noise (nuggets).</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Sample size.</p> required <code>p</code> <code>int</code> <p>Dimension of covariates.</p> required <code>fx</code> <code>Callable</code> <p>Function for the covariates' effect.</p> required <code>theta</code> <code>tuple[float, float, float]</code> <p>theta[0], theta[1], theta[2] represent sigma^2, phi, tau in the exponential covariance family, used for generating spatial random effect.</p> required <code>coord</code> <code>Optional[tensor]</code> <p>A nxd tensor as the locations where to simulate the data, if not specified, randomly sample from [range[0], range[1]]^2 square.</p> <code>None</code> <code>range</code> <code>tuple[float, float]</code> <p>A tuple [a, b] as the range of spatial locations. The spatial coordinates are sampled uniformly from [a, b]^2.</p> <code>[0, 1]</code> <code>sparse</code> <code>Optional[bool]</code> <p>To be implemented. Mainly interact with the rmvn() function.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>X</code> <code>Tensor</code> <p>torch.Tensor nxp array sampled uniformly from [0,1] as the covariates.</p> <code>Y</code> <code>Tensor</code> <p>torch.Tensor Length n vector as the observations consists of fixed covariate effect, spatial random effect, and random noise.</p> <code>coord</code> <code>Tensor</code> <p>torch.Tensor Simulated spatial locations.</p> <code>cov</code> <code>Tensor | NNGP_cov</code> <p>Covariance matrix based on the simulated coordinates.</p> <code>corerr</code> <code>Tensor</code> <p>Simulated spatial random effect.</p> Source code in <code>geospaNN/utils.py</code> <pre><code>def Simulation(n: int, p:int,\n               neighbor_size: int,\n               fx: Callable,\n               theta: tuple[float, float, float],\n               coord: Optional[torch.tensor] = None,\n               range: tuple[float, float] = [0,1],\n               X_pattern: Optional[str] = \"uniform\",\n               sparse: Optional[bool] = True\n               ) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor | NNGP_cov, torch.Tensor]:\n    \"\"\"Simulate spatial data\n\n    Simulate the spatial data based on the following model: Y(s) = f(X(s)) + w(s) + delta(s),\n    where s are the spatial locations, X(s) is the spatial covariates, w(s) is the spatial effect (correlated noise),\n    and delta(s) is the i.i.d random noise (nuggets).\n\n    Parameters:\n        n:\n            Sample size.\n        p:\n            Dimension of covariates.\n        fx:\n            Function for the covariates' effect.\n        theta:\n            theta[0], theta[1], theta[2] represent sigma^2, phi, tau in the exponential covariance family,\n            used for generating spatial random effect.\n        coord:\n            A nxd tensor as the locations where to simulate the data, if not specified, randomly sample from [range[0], range[1]]^2\n            square.\n        range:\n            A tuple [a, b] as the range of spatial locations. The spatial coordinates are sampled uniformly from [a, b]^2.\n        sparse:\n            To be implemented. Mainly interact with the rmvn() function.\n\n    Returns:\n        X: torch.Tensor\n            nxp array sampled uniformly from [0,1] as the covariates.\n        Y: torch.Tensor\n            Length n vector as the observations consists of fixed covariate effect, spatial random effect, and random noise.\n        coord: torch.Tensor\n            Simulated spatial locations.\n        cov:\n            Covariance matrix based on the simulated coordinates.\n        corerr:\n            Simulated spatial random effect.\n    \"\"\"\n    if coord is None:\n        coord = (range[1] - range[0]) * torch.rand(n, 2) + range[0]\n    sigma_sq, phi, tau = theta\n    tau_sq = tau * sigma_sq\n\n    cov = make_cov(coord, theta, neighbor_size)\n    if X_pattern is \"uniform\":\n        X = torch.rand(n, p)\n    elif X_pattern is \"correlated\":\n        _, _, _, _, X = Simulation(n*p, 1, neighbor_size, fx,\n                                   torch.tensor([theta[0], 5*theta[1], theta[2]]), range=[0, 1])\n        X = X.reshape(-1, p)\n        X = (X - X.min()) / (X.max() - X.min())\n    corerr = rmvn(torch.zeros(n), cov, sparse)\n    Y = fx(X).reshape(-1) + corerr + torch.sqrt(tau_sq) * torch.randn(n)\n\n    return X, Y, coord, cov, corerr\n</code></pre>"},{"location":"utils/#geospaNN.utils.distance","title":"<code>distance(coord1, coord2)</code>","text":"<p>Distance matrix between two sets of points</p> <p>Calculate the pairwise distance between two sets of locations.</p> <p>Parameters:</p> Name Type Description Default <code>coord1</code> <code>Tensor</code> <p>The nxd coordinates array for the first set.</p> required <code>coord12</code> <p>The nxd coordinates array for the second set.</p> required <p>Returns:</p> Name Type Description <code>dist</code> <code>Tensor</code> <p>The distance matrix.</p> Source code in <code>geospaNN/utils.py</code> <pre><code>def distance(coord1: torch.Tensor,\n             coord2: torch.Tensor\n             ) -&gt; torch.Tensor:\n    \"\"\"Distance matrix between two sets of points\n\n    Calculate the pairwise distance between two sets of locations.\n\n    Parameters:\n        coord1:\n            The nxd coordinates array for the first set.\n        coord12:\n            The nxd coordinates array for the second set.\n\n    Returns:\n        dist:\n            The distance matrix.\n    \"\"\"\n    if coord1.ndim == 1:\n        m = 1\n        coord1 = coord1.unsqueeze(0)\n    else:\n        m = coord1.shape[0]\n    if coord2.ndim == 1:\n        n = 1\n        coord2 = coord2.unsqueeze(0)\n    else:\n        n = coord2.shape[0]\n\n    #### Can improve (resolved)\n    coord1 = coord1.unsqueeze(0)\n    coord2 = coord2.unsqueeze(1)\n    dists = torch.sqrt(torch.sum((coord1 - coord2) ** 2, axis=-1))\n    return dists\n</code></pre>"},{"location":"utils/#geospaNN.utils.distance_np","title":"<code>distance_np(coord1, coord2)</code>","text":"<p>The numpy version of distance()</p> <p>Calculate the pairwise distance between two sets of locations.</p> <p>Parameters:</p> Name Type Description Default <code>coord1</code> <code>array</code> <p>The nxd coordinates array for the first set.</p> required <code>coord2</code> <code>array</code> <p>The nxd coordinates array for the second set.</p> required <p>Returns:</p> Name Type Description <code>dists</code> <code>array</code> <p>The distance matrix.</p> See Also <p>distance : Distance matrix between two sets of points</p> Source code in <code>geospaNN/utils.py</code> <pre><code>def distance_np(coord1: np.array,\n                coord2: np.array\n                ) -&gt; np.array:\n    \"\"\"The numpy version of distance()\n\n    Calculate the pairwise distance between two sets of locations.\n\n    Parameters:\n        coord1:\n            The nxd coordinates array for the first set.\n        coord2:\n            The nxd coordinates array for the second set.\n\n    Returns:\n        dists:\n            The distance matrix.\n\n    See Also:\n        distance : Distance matrix between two sets of points\n    \"\"\"\n    m = coord1.shape[0]\n    n = coord2.shape[0]\n    #### Can improve (resolved)\n    coord1 = coord1\n    coord2 = coord2\n    dists = np.zeros((m, n))\n    for i in range(m):\n        dists[i, :] = np.sqrt(np.sum((coord1[i] - coord2) ** 2, axis=1))\n    return dists\n</code></pre>"},{"location":"utils/#geospaNN.utils.krig_pred","title":"<code>krig_pred(w_train, coord_train, coord_test, theta, neighbor_size=20, q=0.95)</code>","text":"<p>Kriging prediction (Gaussian process regression) with confidence interval.</p> <p>Kriging prediction on testing locations based on the observations on the training locations. The kriging procedure assumes the observations are sampled from a Gaussian process, which is paramatrized here to have an exponential covariance structure using theta = [sigma^2, phi, tau]. NNGP appriximation is involved for efficient computation of matrix inverse. The conditional variance (kriging variance) is used to build the confidence interval using the quantiles (a/2, 1-a/2). (see https://arxiv.org/abs/2304.09157, section 4.3 for more details.)</p> <p>Parameters:</p> Name Type Description Default <code>w_train</code> <code>Tensor</code> <p>Training observations of the spatial random effect without any fixed effect.</p> required <code>coord_train</code> <code>Tensor</code> <p>Spatial coordinates of the training observations.</p> required <code>coord_test</code> <code>Tensor</code> <p>Spatial coordinates of the locations for prediction</p> required <code>theta</code> <code>tuple[float, float, float]</code> <p>theta[0], theta[1], theta[2] represent sigma^2, phi, tau in the exponential covariance family.</p> required <code>neighbor_size</code> <code>Optional[int]</code> <p>The number of nearest neighbors used for NNGP approximation. Default being 20.</p> <code>20</code> <code>q</code> <code>Optional[float]</code> <p>Confidence coverage for the prediction interval. Default being 0.95.</p> <code>0.95</code> <p>Returns:</p> Name Type Description <code>w_test</code> <code>Tensor</code> <p>torch.Tensor The kriging prediction.</p> <code>pred_U</code> <code>Tensor</code> <p>torch.Tensor Confidence upper bound.</p> <code>pred_L</code> <code>Tensor</code> <p>torch.Tensor Confidence lower bound.</p> See Also <p>Zhan, Wentao, and Abhirup Datta. 2024. \u201cNeural Networks for Geospatial Data.\u201d Journal of the American Statistical Association, June, 1\u201321. doi:10.1080/01621459.2024.2356293.</p> Source code in <code>geospaNN/utils.py</code> <pre><code>def krig_pred(w_train: torch.Tensor,\n              coord_train: torch.Tensor,\n              coord_test: torch.Tensor,\n              theta: tuple[float, float, float],\n              neighbor_size: Optional[int] = 20,\n              q: Optional[float] = 0.95\n              ) -&gt; torch.Tensor:\n    \"\"\"Kriging prediction (Gaussian process regression) with confidence interval.\n\n    Kriging prediction on testing locations based on the observations on the training locations. The kriging procedure\n    assumes the observations are sampled from a Gaussian process, which is paramatrized here to have an exponential covariance\n    structure using theta = [sigma^2, phi, tau]. NNGP appriximation is involved for efficient computation of matrix inverse.\n    The conditional variance (kriging variance) is used to build the confidence interval using the quantiles (a/2, 1-a/2).\n    (see https://arxiv.org/abs/2304.09157, section 4.3 for more details.)\n\n    Parameters:\n        w_train:\n            Training observations of the spatial random effect without any fixed effect.\n        coord_train:\n            Spatial coordinates of the training observations.\n        coord_test:\n            Spatial coordinates of the locations for prediction\n        theta:\n            theta[0], theta[1], theta[2] represent sigma^2, phi, tau in the exponential covariance family.\n        neighbor_size:\n            The number of nearest neighbors used for NNGP approximation. Default being 20.\n        q:\n            Confidence coverage for the prediction interval. Default being 0.95.\n\n    Returns:\n        w_test: torch.Tensor\n            The kriging prediction.\n        pred_U: torch.Tensor\n            Confidence upper bound.\n        pred_L: torch.Tensor\n            Confidence lower bound.\n\n    See Also:\n        Zhan, Wentao, and Abhirup Datta. 2024. \u201cNeural Networks for Geospatial Data.\u201d\n        Journal of the American Statistical Association, June, 1\u201321. doi:10.1080/01621459.2024.2356293.\n    \"\"\"\n    sigma_sq, phi, tau = theta\n    tau_sq = tau * sigma_sq\n    n_test = coord_test.shape[0]\n\n    rank = make_rank(coord_train, neighbor_size, coord_test)\n\n    w_test = torch.zeros(n_test)\n    sigma_test = (sigma_sq + tau_sq) * torch.ones(n_test)\n    for i in range(n_test):\n        ind = rank[i, :]\n        cov_sub = make_cov_full(distance(coord_train[ind, :], coord_train[ind, :]), theta, nuggets=True)\n        cov_vec = make_cov_full(distance(coord_train[ind, :], coord_test[i, :]), theta, nuggets=False).reshape(-1)\n        bi = torch.linalg.solve(cov_sub, cov_vec)\n        w_test[i] = torch.dot(bi.T, w_train[ind]).squeeze()\n        sigma_test[i] = sigma_test[i] - torch.dot(bi.reshape(-1), cov_vec)\n    p = scipy.stats.norm.ppf((1 + q) / 2, loc=0, scale=1)\n    sigma_test = torch.sqrt(sigma_test)\n    pred_U = w_test + p * sigma_test\n    pred_L = w_test - p * sigma_test\n\n    return w_test, pred_U, pred_L\n</code></pre>"},{"location":"utils/#geospaNN.utils.make_bf","title":"<code>make_bf(coord, theta, neighbor_size=20)</code>","text":"<p>Obtain NNGP approximation with implicit covariance matrix</p> <p>Find the upper triangular matrix B and diagonal matrix F such that (I-B)'F^{-1}(I-B) appriximate the precision matrix (inverse of the covariance matrix). (see https://arxiv.org/abs/2102.13299 for more details.) Here only coordinates and spatial parameters are needed to represent the exponential covariance implicitly, thus being more memory-efficient than make_bf_from_cov.</p> <p>Parameters:</p> Name Type Description Default <code>coord</code> <code>Tensor</code> <p>The nxd covariate array.</p> required <code>neighbor_size</code> <code>Optional[int]</code> <p>The number of nearest neighbors used used for NNGP approximation. Default being 20.</p> <code>20</code> <code>theta</code> <code>tuple[float, float, float]</code> <p>theta[0], theta[1], theta[2] represent sigma^2, phi, tau in the exponential covariance family.</p> required <p>Returns:</p> Name Type Description <code>I_B</code> <code>Sparse_B</code> <p>The sparse representation of I-B.</p> <code>F</code> <code>Tensor</code> <p>The vector representation of the diagonal matrix.</p> See Also <p>make_bf_from_cov : Obtain NNGP approximation of a covariance matrix  Datta, Abhirup. \"Sparse nearest neighbor Cholesky matrices in spatial statistics.\" arXiv preprint arXiv:2102.13299 (2021).</p> Source code in <code>geospaNN/utils.py</code> <pre><code>def make_bf(coord: torch.Tensor,  #### could add a make_bf from cov (resolved)\n            theta: tuple[float, float, float],\n            neighbor_size: Optional[int] = 20,\n            ) -&gt; Tuple[Sparse_B, torch.Tensor]:\n    \"\"\"Obtain NNGP approximation with implicit covariance matrix\n\n    Find the upper triangular matrix B and diagonal matrix F such that (I-B)'F^{-1}(I-B) appriximate the precision matrix\n    (inverse of the covariance matrix). (see https://arxiv.org/abs/2102.13299 for more details.) Here only coordinates and\n    spatial parameters are needed to represent the exponential covariance implicitly,\n    thus being more memory-efficient than make_bf_from_cov.\n\n    Parameters:\n        coord:\n            The nxd covariate array.\n        neighbor_size:\n            The number of nearest neighbors used used for NNGP approximation. Default being 20.\n        theta:\n            theta[0], theta[1], theta[2] represent sigma^2, phi, tau in the exponential covariance family.\n\n    Returns:\n        I_B:\n            The sparse representation of I-B.\n        F:\n            The vector representation of the diagonal matrix.\n\n    See Also:\n        make_bf_from_cov : Obtain NNGP approximation of a covariance matrix \\\n\n        Datta, Abhirup. \"Sparse nearest neighbor Cholesky matrices in spatial statistics.\"\n        arXiv preprint arXiv:2102.13299 (2021).\n    \"\"\"\n    n = coord.shape[0]\n    rank = make_rank(coord, neighbor_size)\n    B = torch.zeros((n, neighbor_size))\n    ind_list = np.zeros((n, neighbor_size)).astype(int) - 1\n    F = torch.zeros(n)\n    for i in range(n):\n        F[i] = make_cov_full(torch.tensor([0]), theta, nuggets = True)\n        ind = rank[i, :][rank[i, :] &lt;= i]\n        if len(ind) == 0:\n            continue\n        cov_sub = make_cov_full(distance(coord[ind, :], coord[ind, :]), theta, nuggets = True)\n        if torch.linalg.matrix_rank(cov_sub) == cov_sub.shape[0]:\n            cov_vec = make_cov_full(distance(coord[ind, :], coord[i, :]), theta).reshape(-1)\n            #### nuggets is not specified since its off-diagonal\n            bi = torch.linalg.solve(cov_sub, cov_vec)\n            B[i, range(len(ind))] = bi\n            ind_list[i, range(len(ind))] = ind\n            F[i] = F[i] - torch.inner(cov_vec, bi)\n\n    I_B = Sparse_B(torch.concatenate([torch.ones((n, 1)), -B], axis=1),\n                   np.concatenate([np.arange(0, n).reshape(n, 1), ind_list], axis = 1))\n\n    return I_B, F\n</code></pre>"},{"location":"utils/#geospaNN.utils.make_bf_from_cov","title":"<code>make_bf_from_cov(cov, neighbor_size)</code>","text":"<p>Obtain NNGP approximation of a covariance matrix</p> <p>Find the upper triangular matrix B and diagonal matrix F such that (I-B)'F^{-1}(I-B) appriximate the precision matrix (inverse of the covariance matrix). The level of approximation increase with the neighbor size. When using the full neighbor, the NNGP appriximation degrade to the Cholesky decomposition. (see https://arxiv.org/abs/2102.13299 for more details.)</p> <p>Parameters:</p> Name Type Description Default <code>cov</code> <code>Tensor</code> <p>The nxn covariance matrix.</p> required <code>neighbor_size</code> <code>int</code> <p>The number of nearest neighbors used for NNGP approximation.</p> required <p>Returns:</p> Name Type Description <code>I_B</code> <code>Sparse_B</code> <p>Sparse_B The sparse representation of I-B.</p> <code>F</code> <code>Sparse_B</code> <p>torch.Tensor The vector representation of the diagonal matrix.</p> See Also <p>make_bf : Obtain NNGP approximation with implicit covariance matrix  Datta, Abhirup, et al. \"Hierarchical nearest-neighbor Gaussian process models for large geostatistical datasets.\" Journal of the American Statistical Association 111.514 (2016): 800-812.</p> <p>Datta, Abhirup. \"Sparse nearest neighbor Cholesky matrices in spatial statistics.\" arXiv preprint arXiv:2102.13299 (2021).</p> Source code in <code>geospaNN/utils.py</code> <pre><code>def make_bf_from_cov(cov: torch.Tensor,\n                     neighbor_size: int,\n                     ) -&gt; Sparse_B:\n    \"\"\"Obtain NNGP approximation of a covariance matrix\n\n    Find the upper triangular matrix B and diagonal matrix F such that (I-B)'F^{-1}(I-B) appriximate the precision matrix\n    (inverse of the covariance matrix). The level of approximation increase with the neighbor size. When using the full neighbor,\n    the NNGP appriximation degrade to the Cholesky decomposition. (see https://arxiv.org/abs/2102.13299 for more details.)\n\n    Parameters:\n        cov:\n            The nxn covariance matrix.\n        neighbor_size:\n            The number of nearest neighbors used for NNGP approximation.\n\n    Returns:\n        I_B: Sparse_B\n            The sparse representation of I-B.\n        F: torch.Tensor\n            The vector representation of the diagonal matrix.\n\n    See Also:\n        make_bf : Obtain NNGP approximation with implicit covariance matrix \\\n\n        Datta, Abhirup, et al. \"Hierarchical nearest-neighbor Gaussian process models for large geostatistical datasets.\"\n        Journal of the American Statistical Association 111.514 (2016): 800-812.\n\n        Datta, Abhirup. \"Sparse nearest neighbor Cholesky matrices in spatial statistics.\"\n        arXiv preprint arXiv:2102.13299 (2021).\n    \"\"\"\n    n = cov.shape[0]\n    B = torch.zeros((n, neighbor_size))\n    ind_list = np.zeros((n, neighbor_size)).astype(int) - 1\n    F = torch.zeros(n)\n    rank = np.argsort(-cov, axis=-1) #### consider replace using make_rank?\n    rank = rank[:, 1:(neighbor_size + 1)]\n    for i in range(n):\n        F[i] = cov.diag()[i]\n        ind = rank[i, :][rank[i, :] &lt;= i]\n        if len(ind) == 0:\n            continue\n        cov_sub = cov[ind.reshape(-1, 1), ind.reshape(1, -1)]\n        if torch.linalg.matrix_rank(cov_sub) == cov_sub.shape[0]:\n            cov_vec = cov[ind, i].reshape(-1)\n            bi = torch.linalg.solve(cov_sub, cov_vec)\n            B[i, range(len(ind))] = bi\n            ind_list[i, range(len(ind))] = ind\n            F[i] = F[i] - torch.inner(cov_vec, bi)\n\n    I_B = Sparse_B(torch.concatenate([torch.ones((n, 1)), -B], axis=1),\n                   np.concatenate([np.arange(0, n).reshape(n, 1), ind_list], axis=1))\n\n    return I_B, F #### Shall we return NNGP_cov instead?\n</code></pre>"},{"location":"utils/#geospaNN.utils.make_cov","title":"<code>make_cov(coord, theta, NNGP=True, neighbor_size=20)</code>","text":"<p>Compose covariance matrix.</p> <p>Compose a covariance matrix in the exponential covariance family using the coordinates and spatial parameters. NNGP approximation is introduced for efficient representation. (see https://arxiv.org/abs/2102.13299 for more details.)</p> <p>Parameters:</p> Name Type Description Default <code>coord</code> <code>Tensor</code> <p>The nxd covariate array.</p> required <code>theta</code> <code>tuple[float, float, float]</code> <p>theta[0], theta[1], theta[2] represent sigma^2, phi, tau in the exponential covariance family.</p> required <code>NNGP</code> <code>Optional[bool]</code> <p>Whether use NNGP approximation (recommended and used by default).</p> <code>True</code> <code>neighbor_size</code> <code>Optional[int]</code> <p>Number of nearest neighbors used for NNGP approximation, default value is 20.</p> <code>20</code> <p>Returns:</p> Name Type Description <code>cov</code> <code>Tensor</code> <p>A covariance matrix as torch.Tensor (dense representation) or NNGP_cov (sparse representation).</p> See Also <p>Datta, Abhirup. \"Sparse nearest neighbor Cholesky matrices in spatial statistics.\" arXiv preprint arXiv:2102.13299 (2021).</p> Source code in <code>geospaNN/utils.py</code> <pre><code>def make_cov(coord: torch.Tensor,\n             theta: tuple[float, float, float],\n             NNGP: Optional[bool] = True,\n             neighbor_size: Optional[int] = 20\n             ) -&gt; torch.Tensor:\n    \"\"\"Compose covariance matrix.\n\n    Compose a covariance matrix in the exponential covariance family using the coordinates and spatial parameters.\n    NNGP approximation is introduced for efficient representation. (see https://arxiv.org/abs/2102.13299 for more details.)\n\n    Parameters:\n        coord:\n            The nxd covariate array.\n        theta:\n            theta[0], theta[1], theta[2] represent sigma^2, phi, tau in the exponential covariance family.\n        NNGP:\n            Whether use NNGP approximation (recommended and used by default).\n        neighbor_size:\n            Number of nearest neighbors used for NNGP approximation, default value is 20.\n\n    Returns:\n        cov:\n            A covariance matrix as torch.Tensor (dense representation) or NNGP_cov (sparse representation).\n\n    See Also:\n        Datta, Abhirup. \"Sparse nearest neighbor Cholesky matrices in spatial statistics.\"\n        arXiv preprint arXiv:2102.13299 (2021).\n    \"\"\"\n    if not NNGP:\n        dist = distance(coord, coord)\n        cov = make_cov_full(dist, theta, nuggets = True) #### could add a make_bf from cov (resolved)\n        return cov\n    else:\n        I_B, F_diag = make_bf(coord, theta, neighbor_size) #### could merge into one step\n        cov = NNGP_cov(I_B.B, F_diag, I_B.Ind_list)\n        return cov\n</code></pre>"},{"location":"utils/#geospaNN.utils.make_cov_full","title":"<code>make_cov_full(dist, theta, nuggets=False)</code>","text":"<p>Compose covariance matrix from the distance matrix with dense representation.</p> <p>Compose a covariance matrix in the exponential covariance family (other options to be implemented) from the distance matrix. The returned object class depends on the input distance matrix.</p> <p>Parameters:</p> Name Type Description Default <code>dist</code> <code>Tensor | ndarray</code> <p>The nxn distance matrix</p> required <code>theta</code> <code>tuple[float, float, float]</code> <p>theta[0], theta[1], theta[2] represent sigma^2, phi, tau in the exponential covariance family.</p> required <code>nuggets</code> <code>Optional[bool]</code> <p>Whether to include nuggets term in the covariance matrix (added to the diagonal).</p> <code>False</code> <p>Returns:</p> Name Type Description <code>cov</code> <code>Tensor | ndarray</code> <p>A covariance matrix.</p> Source code in <code>geospaNN/utils.py</code> <pre><code>def make_cov_full(dist: torch.Tensor | np.ndarray,\n                  theta: tuple[float, float, float],\n                  nuggets: Optional[bool] = False,\n                  ) -&gt; torch.Tensor | np.ndarray:\n    \"\"\"Compose covariance matrix from the distance matrix with dense representation.\n\n    Compose a covariance matrix in the exponential covariance family (other options to be implemented) from the distance\n    matrix. The returned object class depends on the input distance matrix.\n\n    Parameters:\n        dist:\n            The nxn distance matrix\n        theta:\n            theta[0], theta[1], theta[2] represent sigma^2, phi, tau in the exponential covariance family.\n        nuggets:\n            Whether to include nuggets term in the covariance matrix (added to the diagonal).\n\n    Returns:\n        cov:\n            A covariance matrix.\n    \"\"\"\n    sigma_sq, phi, tau = theta\n    tau_sq = tau * sigma_sq\n    if isinstance(dist, float) or isinstance(dist, int):\n        dist = torch.Tensor(dist)\n        n = 1\n    else:\n        n = dist.shape[-1]\n    if isinstance(dist, torch.Tensor):\n        cov = sigma_sq * torch.exp(-phi * dist)\n    else:\n        cov = sigma_sq * np.exp(-phi * dist)\n    if nuggets:\n        shape_temp = list(cov.shape)[:-2] + [1 ,1]\n        if isinstance(dist, torch.Tensor):\n            cov += tau_sq * torch.eye(n).repeat(*shape_temp).squeeze()\n        else:\n            cov += tau_sq * np.eye(n).squeeze() #### need improvement\n    return cov\n</code></pre>"},{"location":"utils/#geospaNN.utils.make_graph","title":"<code>make_graph(X, Y, coord, neighbor_size=20, Ind_list=None)</code>","text":"<p>Compose the data with graph information to work on.</p> <p>This function connects each node to its nearest neighbors and records the edge indexes in two forms for the downstream graph operations.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Tensor</code> <p>nxp array of the covariates.</p> required <code>Y</code> <code>Tensor</code> <p>Length n vector as the response.</p> required <code>coord</code> <code>Tensor</code> <p>nxd array of the coordinates</p> required <code>neighbor_size</code> <code>Optional[int]</code> <p>The number of nearest neighbors used for NNGP approximation. Default being 20.</p> <code>20</code> <code>Ind_list</code> <code>Optional</code> <p>An optional index list. If provided, ommit the make_rank() step in the function.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>data</code> <code>Data</code> <p>torch_geometric.data.Data             Data that can be processed by the torch_geometric framework.            data.x contains the covariates array,            data.y contains the response vector,            data.pos contains the spatial coordinates,            data.edge_index contains the indexes of form [i,j] where location j is in the nearest neighbor of location i.            data.edge_attr contains the concatenated numbering of the neighbors.            For each location, the numbering is of the form [0, 1, ... , k] where k is the number of the nearest neighbors.            This attribute is mainly used in messaging passing.</p> <p>See Also: make_rank : Compose the nearest neighbor index list based on the coordinates.     torch_geometric.data.Data : A data object describing a homogeneous graph.</p> Source code in <code>geospaNN/utils.py</code> <pre><code>def make_graph(X: torch.Tensor,\n               Y: torch.Tensor,\n               coord: torch.Tensor,\n               neighbor_size: Optional[int] = 20,\n               Ind_list: Optional = None\n               ) -&gt; torch_geometric.data.Data:\n    \"\"\"Compose the data with graph information to work on.\n\n    This function connects each node to its nearest neighbors and records the edge indexes in two forms for the downstream\n    graph operations.\n\n    Parameters:\n        X:\n            nxp array of the covariates.\n        Y:\n            Length n vector as the response.\n        coord:\n            nxd array of the coordinates\n        neighbor_size:\n            The number of nearest neighbors used for NNGP approximation. Default being 20.\n        Ind_list:\n            An optional index list. If provided, ommit the make_rank() step in the function.\n\n    Returns:\n        data: torch_geometric.data.Data \\\n            Data that can be processed by the torch_geometric framework.\\\n            data.x contains the covariates array,\\\n            data.y contains the response vector,\\\n            data.pos contains the spatial coordinates,\\\n            data.edge_index contains the indexes of form [i,j] where location j is in the nearest neighbor of location i.\\\n            data.edge_attr contains the concatenated numbering of the neighbors.\\\n            For each location, the numbering is of the form [0, 1, ... , k] where k is the number of the nearest neighbors.\\\n            This attribute is mainly used in messaging passing.\n\n    See Also:\n    make_rank : Compose the nearest neighbor index list based on the coordinates. \\\n    torch_geometric.data.Data : A data object describing a homogeneous graph.\n    \"\"\"\n    n = X.shape[0]\n    # Compute the edges of the graph\n    edges = []\n    neighbor_idc = []\n    # Initialize the edges, the edges are predefined for the first m + 1 points\n    # Find the m nearest neighbors for each remaining point\n    if Ind_list is None:\n        Ind_list = make_rank(coord, neighbor_size)\n    for i in range(1, n):\n        for j, idx in enumerate(Ind_list[i]):\n            if idx &lt; i:\n                edges.append([idx, i])\n                neighbor_idc.append(j)\n            elif j &gt;= i:\n                break\n\n    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n    edge_attr = torch.tensor(neighbor_idc).reshape(-1, 1)  # denotes the index of the neighbor\n    data = torch_geometric.data.Data(x=X, y=Y, pos=coord, edge_index=edge_index, edge_attr=edge_attr)\n    assert data.validate(raise_on_error=True)\n    return data\n</code></pre>"},{"location":"utils/#geospaNN.utils.make_rank","title":"<code>make_rank(coord, neighbor_size, coord_ref=None)</code>","text":"<p>Compose the nearest neighbor index list based on the coordinates.</p> <p>Find the indexes of nearest neighbors in reference set for each location i in the main set. The index is based on the increasing order of the distances between ith location and the locations in the reference set.</p> <p>Parameters:</p> Name Type Description Default <code>coord</code> <code>Tensor</code> <p>The nxd coordinates array of target locations.</p> required <code>neighbor_size</code> <code>int</code> required <code>coord_ref</code> <code>Optional</code> <p>The n_refxd coordinates array of reference locations. If None, use the target set itself as the reference. (Any location's neighbor does not include itself.)</p> <code>None</code> <p>Returns:</p> Name Type Description <code>rank_list</code> <code>ndarray</code> <p>A nxp array. The ith row is the indexes of the nearest neighbors for the ith location, ordered by the distance.</p> Source code in <code>geospaNN/utils.py</code> <pre><code>def make_rank(coord: torch.Tensor,\n              neighbor_size: int,\n              coord_ref: Optional = None\n              ) -&gt; np.ndarray:\n    \"\"\"Compose the nearest neighbor index list based on the coordinates.\n\n    Find the indexes of nearest neighbors in reference set for each location i in the main set.\n    The index is based on the increasing order of the distances between ith location and the locations in the reference set.\n\n    Parameters:\n        coord:\n            The nxd coordinates array of target locations.\n        neighbor_size:\n        `   Suppose neighbor_size = k, only the top k-nearest indexes will be returned.\n        coord_ref:\n            The n_refxd coordinates array of reference locations. If None, use the target set itself as the reference.\n            (Any location's neighbor does not include itself.)\n\n    Returns:\n        rank_list:\n            A nxp array. The ith row is the indexes of the nearest neighbors for the ith location, ordered by the distance.\n    \"\"\"\n    if coord_ref is None: neighbor_size += 1\n    knn = NearestNeighbors(n_neighbors=neighbor_size)\n    knn.fit(coord)\n    if coord_ref is None:\n        coord_ref = coord\n        rank = knn.kneighbors(coord_ref)[1]\n        return rank[:, 1:]\n    else:\n        rank = knn.kneighbors(coord_ref)[1]\n        return rank[:, 0:]\n</code></pre>"},{"location":"utils/#geospaNN.utils.rmvn","title":"<code>rmvn(mu, cov, sparse=True)</code>","text":"<p>Randomly generate sample from multivariate normal distribution</p> <p>Generate random sample from a multivariate normal distribution with specified mean and covariance.</p> <p>Parameters:</p> Name Type Description Default <code>mu</code> <code>Tensor</code> <p>The additional mean of multivariate normal of length n.</p> required <code>cov</code> <code>Tensor | NNGP_cov</code> required <code>sparse</code> <code>Optional[bool]</code> <p>Designed for sparse representation, not implemented yet.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>sample</code> <code>Tensor</code> <p>A random sample from the multivariate normal distribution.</p> Source code in <code>geospaNN/utils.py</code> <pre><code>def rmvn(mu: torch.Tensor,\n         cov: torch.Tensor | NNGP_cov,\n         sparse: Optional[bool] = True) \\\n        -&gt; torch.Tensor:\n    \"\"\"Randomly generate sample from multivariate normal distribution\n\n    Generate random sample from a multivariate normal distribution with specified mean and covariance.\n\n    Parameters:\n        mu:\n            The additional mean of multivariate normal of length n.\n        cov:\n        `   The nxn covariance matrix. When use torch.Tensor for the dense representation, Cholesky's decomposition is used\n            for correlating the i.i.d normal sample. When use NNGP_cov object for sparse representation, use NNGP to approximate\n            the correlating process. Dense representation is not recommended for large sample size.\n        sparse:\n            Designed for sparse representation, not implemented yet.\n\n    Returns:\n        sample:\n            A random sample from the multivariate normal distribution.\n    \"\"\"\n    n = len(mu) #### Check dimensionality\n    if isinstance(cov, torch.Tensor):\n        if n &gt;= 2000: warnings.warn(\"Too large for cholesky decomposition, please try to use NNGP\")\n        D = torch.linalg.cholesky(cov)\n        res = torch.matmul(torch.randn(1, n), D.t()) + mu\n    elif isinstance(cov, NNGP_cov):\n        if sparse:\n            res = cov.correlate(torch.randn(1, n).reshape(-1)) + mu\n        else:\n            warnings.warn(\"To be implemented.\")\n    else:\n        warnings.warn(\"Covariance matrix should be in the format of torch.Tensor or NNGP_cov.\")\n        return\n\n    return  res.reshape(-1)\n</code></pre>"},{"location":"utils/#geospaNN.utils.spatial_order","title":"<code>spatial_order(X, Y, coord, method='max-min', numpropose=2, seed=2024)</code>","text":"<p>Spatial ordering for data</p> <p>Order the data according to their spatial locations. A spatial ordering is necessary for NNGP to represent a valid spatial process. (Datta et.al 2016) Method 'coord-sum' stands for a simple spatial ordering by the summation of coordinates. Method 'max-min' stands for the max-min ordering based on Euclidean distance. \"Basically, this ordering starts at a point in the center, then adds points one at a time that maximize the minimum distance from all previous points in the ordering.\" (Katzfuss &amp; Guinness 2021)</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Tensor</code> <p>nxp array of the covariates.</p> required <code>Y</code> <code>Tensor</code> <p>Length n vector as the response.</p> required <code>coord</code> <code>Tensor</code> <p>nxd array of the coordinates</p> required <code>method</code> <code>str</code> <p>Method 'coord-sum' stands for a simple spatial ordering by the summation of coordinates. Method 'max-min' stands for the max-min ordering based on Euclidean distance. (Katzfuss &amp; Guinness 2021)</p> <code>'max-min'</code> <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor, Tensor, Tensor]</code> <p>Ordered X, Ordered Y, Ordered coordinates, order</p> See Also <p>Datta, Abhirup, et al. \"Hierarchical nearest-neighbor Gaussian process models for large geostatistical datasets.\" Journal of the American Statistical Association 111.514 (2016): 800-812.         Katzfuss, Matthias &amp; Guinness, Joseph. \"A General Framework for Vecchia Approximations of Gaussian Processes.\" Statist. Sci. 36 (1) 124 - 141, February 2021. https://doi.org/10.1214/19-STS755</p> Source code in <code>geospaNN/utils.py</code> <pre><code>def spatial_order(X: torch.Tensor,\n                  Y: torch.Tensor,\n                  coord: torch.Tensor,\n                  method: str = 'max-min',\n                  numpropose: Optional[int] = 2,\n                  seed: Optional[int] = 2024\n                  ) -&gt; Tuple[torch.Tensor,torch.Tensor,torch.Tensor,torch.Tensor]:\n    \"\"\"Spatial ordering for data\n\n    Order the data according to their spatial locations. A spatial ordering is necessary for NNGP to represent a valid\n    spatial process. (Datta et.al 2016)\n    Method 'coord-sum' stands for a simple spatial ordering by the summation of coordinates.\n    Method 'max-min' stands for the max-min ordering based on Euclidean distance. \"Basically, this ordering starts at a\n    point in the center, then adds points one at a time that maximize the minimum distance from all previous points in\n    the ordering.\" (Katzfuss &amp; Guinness 2021)\n\n    Parameters:\n        X:\n            nxp array of the covariates.\n        Y:\n            Length n vector as the response.\n        coord:\n            nxd array of the coordinates\n        method:\n            Method 'coord-sum' stands for a simple spatial ordering by the summation of coordinates.\n            Method 'max-min' stands for the max-min ordering based on Euclidean distance. (Katzfuss &amp; Guinness 2021)\n\n    Returns:\n        Ordered X, Ordered Y, Ordered coordinates, order\n\n    See Also:\n        Datta, Abhirup, et al. \"Hierarchical nearest-neighbor Gaussian process models for large geostatistical datasets.\"\n        Journal of the American Statistical Association 111.514 (2016): 800-812. \\\n        Katzfuss, Matthias &amp; Guinness, Joseph. \"A General Framework for Vecchia Approximations of Gaussian Processes.\"\n        Statist. Sci. 36 (1) 124 - 141, February 2021. https://doi.org/10.1214/19-STS755\n    \"\"\"\n    n = coord.shape[0]\n    if n &gt;= 10000 and method == 'max-min':\n        warnings.warn(\"Too large for max-min ordering, switch to 'coord-sum' ordering!\")\n        method = 'coord-sum'\n\n    d = coord.shape[1]\n    random.seed(seed)\n    if method == 'max-min':\n        remaininginds = list(range(n))\n        orderinds = torch.zeros(n, dtype=torch.int)\n        distmp = distance(coord, coord.mean(dim = 0)).reshape(-1)\n        ordermp = torch.argsort(distmp)\n        orderinds[0] = ordermp[0]\n        remaininginds.remove(orderinds[0])\n        for j in range(1,n):\n            randinds = random.sample(remaininginds, min(numpropose, len(remaininginds)))\n            distarray = distance(coord[orderinds[0:j],:], coord[torch.tensor(randinds),:])\n            bestind = torch.argmax(distarray.min(axis = 1).values)\n            orderinds[j] = randinds[bestind]\n            remaininginds.remove(orderinds[j])\n    elif method == 'coord-sum':\n        orderinds = torch.argsort(coord.sum(axis = 1))\n    else:\n        warnings.warn(\"Keep the order\")\n        orderinds = torch.tensor(range(n))\n\n    return X[orderinds,:], Y[orderinds], coord[orderinds,:], orderinds\n</code></pre>"},{"location":"utils/#geospaNN.utils.split_data","title":"<code>split_data(X, Y, coord, neighbor_size=20, val_proportion=0.2, test_proportion=0.2)</code>","text":"<p>Split the data into training, validation, and testing parts and add the graph information respectively.</p> <p>This function split the data with a user specified proportions and build the graph information. The output are the data sets that can be directly processed within the torch_geomtric framework.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Tensor</code> <p>nxp array of the covariates.</p> required <code>Y</code> <code>Tensor</code> <p>Length n vector as the response.</p> required <code>coord</code> <code>Tensor</code> <p>nxd array of the coordinates</p> required <code>neighbor_size</code> <code>Optional[int]</code> <p>The number of nearest neighbors used for NNGP approximation. Default being 20.</p> <code>20</code> <code>val_proportion</code> <code>float</code> <p>The proportion of training set splitted as validation set.</p> <code>0.2</code> <code>test_proportion</code> <code>float</code> <p>The proportion of whole data splitted as testing set.</p> <code>0.2</code> <p>Returns:</p> Type Description <code>tuple[Data, Data, Data]</code> <p>data_train, data_val, data_test</p> See Also <p>make_graph : Compose the data with graph information to work on.         torch_geometric.data.Data : A data object describing a homogeneous graph.</p> Source code in <code>geospaNN/utils.py</code> <pre><code>def split_data(X: torch.Tensor,\n               Y: torch.Tensor,\n               coord: torch.Tensor,\n               neighbor_size: Optional[int] = 20,\n               val_proportion: float = 0.2,\n               test_proportion: float = 0.2\n               ) -&gt; tuple[torch_geometric.data.Data, torch_geometric.data.Data, torch_geometric.data.Data]:\n    \"\"\"Split the data into training, validation, and testing parts and add the graph information respectively.\n\n    This function split the data with a user specified proportions and build the graph information. The output are the\n    data sets that can be directly processed within the torch_geomtric framework.\n\n    Parameters:\n        X:\n            nxp array of the covariates.\n        Y:\n            Length n vector as the response.\n        coord:\n            nxd array of the coordinates\n        neighbor_size:\n            The number of nearest neighbors used for NNGP approximation. Default being 20.\n        val_proportion:\n            The proportion of training set splitted as validation set.\n        test_proportion:\n            The proportion of whole data splitted as testing set.\n\n    Returns:\n        data_train, data_val, data_test\n\n    See Also:\n        make_graph : Compose the data with graph information to work on. \\\n        torch_geometric.data.Data : A data object describing a homogeneous graph.\n    \"\"\"\n    X_train_val, X_test, Y_train_val, Y_test, coord_train_val, coord_test = train_test_split(\n        X, Y, coord, test_size = test_proportion\n    )\n\n    X_train, X_val, Y_train, Y_val, coord_train, coord_val = train_test_split(\n        X_train_val, Y_train_val, coord_train_val, test_size = val_proportion/(1 - test_proportion)\n    )\n\n    data_train = make_graph(X_train, Y_train, coord_train, neighbor_size)\n    data_val = make_graph(X_val, Y_val, coord_val, neighbor_size)\n    data_test = make_graph(X_test, Y_test, coord_test, neighbor_size)\n\n    return data_train, data_val, data_test\n</code></pre>"},{"location":"utils/#geospaNN.utils.split_loader","title":"<code>split_loader(data, batch_size=None)</code>","text":"<p>Create mini-batches for GNN training</p> <p>This functions further split a data for mini-batch training of GNNs on large-scale graphs where full-batch training is not feasible. Note that only source nodes are splited into batches, each batch will contain edges originate from those source nodes. There might be interactions among the target nodes across batches.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>Data with graph information (output of split_data() or make_graph()).</p> required <code>batch_size</code> <code>Optional[int]</code> <p>Size of mini-batches, default value being n/20.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>loader</code> <code>DataLoaders</code> <p>A dataloader that can be enumerated for mini-batch training.</p> See Also <p>make_graph : Compose the data with graph information to work on.         split_data : Split the data into training, validation, and testing parts and add the graph information respectively.         torch_geometric.loader : A data loader that performs neighbor sampling as introduced in the </p> <p>Hamilton, Will, Zhitao Ying, and Jure Leskovec. \"Inductive representation learning on large graphs.\" Advances in neural information processing systems 30 (2017).</p> Source code in <code>geospaNN/utils.py</code> <pre><code>def split_loader(data: torch_geometric.data.Data,\n                 batch_size: Optional[int] = None\n                 ) -&gt; torch.DataLoaders:\n    \"\"\"Create mini-batches for GNN training\n\n    This functions further split a data for mini-batch training of GNNs on large-scale graphs\n    where full-batch training is not feasible. Note that only source nodes are splited into batches,\n    each batch will contain edges originate from those source nodes.\n    There might be interactions among the target nodes across batches.\n\n    Parameters:\n        data:\n            Data with graph information (output of split_data() or make_graph()).\n        batch_size:\n            Size of mini-batches, default value being n/20.\n\n    Returns:\n        loader:\n            A dataloader that can be enumerated for mini-batch training.\n\n    See Also:\n        make_graph : Compose the data with graph information to work on. \\\n        split_data : Split the data into training, validation, and testing parts and add the graph information respectively. \\\n        torch_geometric.loader : A data loader that performs neighbor sampling as introduced in the \\\n\n    Hamilton, Will, Zhitao Ying, and Jure Leskovec. \"Inductive representation learning on large graphs.\"\n    Advances in neural information processing systems 30 (2017).\n    \"\"\"\n    if batch_size is None:\n        batch_size  = int(data.x.shape[0]/20)\n    loader = NeighborLoader(data,\n                            input_nodes=torch.tensor(range(data.x.shape[0])),num_neighbors=[-1],\n                            batch_size=batch_size, replace=False, shuffle=True)\n    return loader\n</code></pre>"},{"location":"utils/#geospaNN.utils.theta_update","title":"<code>theta_update(w, coord, theta0=None, neighbor_size=20, BRISC=True, min_tau=0.001)</code>","text":"<p>Update the spatial parameters using maximum likelihood.</p> <p>This function updates the spatial parameters by assuming the observations are from a Gaussian Process with exponential covariance function. Spatial coordinates and initial values of theta are input for building the covariance. By default, L-BFGS-B algorithm is used to optimize the likelihood. Since the likelihood is computed repeatedly, NNGP approximation is used for efficient computation of the log-likelihood, with a default neighbor size being 20. Note that we currently use the Cpp implementation of L-BFGS-B from the R-package BRISC (Saha &amp; Datta 2018) as default, which is faster than the python version. In the future, we will introduce a python wrapper for the Cpp implementation to get free of R component.</p> <p>Parameters:</p> Name Type Description Default <code>theta0</code> <code>Optional[Tensor]</code> <p>Initial values of the spatial parameters. theta[0], theta[1], theta[2] represent sigma^2, phi, tau in the exponential covariance family.</p> <code>None</code> <code>w</code> <code>Tensor</code> <p>Length n observations of the spatial random effect without any fixed effect.</p> required <code>coord</code> <code>Tensor</code> <p>nx2 spatial coordinates of the observations.</p> required <code>neighbor_size</code> <code>Optional[int]</code> <p>The number of nearest neighbors used for NNGP approximation. Default being 20.</p> <code>20</code> <code>BRISC</code> <code>Optional[bool]</code> <p>Whether to use the optimization from BRISC. Default being True. Setting as False will largely increase the running time.</p> <code>True</code> <code>min_tau</code> <code>Optional[float]</code> <p>A minimum value of tau to avoid sinularity issue in matrix inversion. Default being 0.001.</p> <code>0.001</code> <p>Returns:</p> Name Type Description <code>theta_updated</code> <code>array</code> <p>An updated tuple of the spatial paramaters.</p> See Also <p>Datta, Abhirup, et al. \"Hierarchical nearest-neighbor Gaussian process models for large geostatistical datasets.\" Journal of the American Statistical Association 111.514 (2016): 800-812.</p> <p>Zhu, Ciyou, et al. \"Algorithm 778: L-BFGS-B: Fortran subroutines for large-scale bound-constrained optimization.\" ACM Transactions on mathematical software (TOMS) 23.4 (1997): 550-560.</p> <p>Saha, Arkajyoti, and Abhirup Datta. \"BRISC: bootstrap for rapid inference on spatial covariances.\" Stat 7.1 (2018): e184.</p> Source code in <code>geospaNN/utils.py</code> <pre><code>def theta_update(w: torch.Tensor,\n                 coord: torch.Tensor,\n                 theta0: Optional[torch.Tensor] = None,\n                 neighbor_size: Optional[int] = 20,\n                 BRISC: Optional[bool] = True,\n                 min_tau: Optional[float] = 0.001\n                 ) -&gt; np.array:\n    \"\"\"Update the spatial parameters using maximum likelihood.\n\n    This function updates the spatial parameters by assuming the observations are from a Gaussian Process with exponential\n    covariance function. Spatial coordinates and initial values of theta are input for building the covariance.\n    By default, L-BFGS-B algorithm is used to optimize the likelihood.\n    Since the likelihood is computed repeatedly, NNGP approximation is used for efficient computation of the log-likelihood,\n    with a default neighbor size being 20.\n    Note that we currently use the Cpp implementation of L-BFGS-B from the R-package BRISC (Saha &amp; Datta 2018) as default,\n    which is faster than the python version.\n    In the future, we will introduce a python wrapper for the Cpp implementation to get free of R component.\n\n    Parameters:\n        theta0:\n            Initial values of the spatial parameters.\n            theta[0], theta[1], theta[2] represent sigma^2, phi, tau in the exponential covariance family.\n        w:\n            Length n observations of the spatial random effect without any fixed effect.\n        coord:\n            nx2 spatial coordinates of the observations.\n        neighbor_size:\n            The number of nearest neighbors used for NNGP approximation. Default being 20.\n        BRISC:\n            Whether to use the optimization from BRISC. Default being True. Setting as False will largely increase the running time.\n        min_tau:\n            A minimum value of tau to avoid sinularity issue in matrix inversion. Default being 0.001.\n\n    Returns:\n        theta_updated:\n            An updated tuple of the spatial paramaters.\n\n    See Also:\n        Datta, Abhirup, et al. \"Hierarchical nearest-neighbor Gaussian process models for large geostatistical datasets.\"\n        Journal of the American Statistical Association 111.514 (2016): 800-812.\n\n        Zhu, Ciyou, et al. \"Algorithm 778: L-BFGS-B: Fortran subroutines for large-scale bound-constrained optimization.\"\n        ACM Transactions on mathematical software (TOMS) 23.4 (1997): 550-560.\n\n        Saha, Arkajyoti, and Abhirup Datta. \"BRISC: bootstrap for rapid inference on spatial covariances.\"\n        Stat 7.1 (2018): e184.\n    \"\"\"\n    warnings.filterwarnings(\"ignore\")\n    w = w.detach().numpy()\n    coord = coord.detach().numpy()\n\n    if not BRISC:\n        theta = theta0.detach().numpy()\n        if theta0 is None:\n            warnings.warn(\"Theta not initialized, start from [1,1,1].\")\n            theta = np.array([1,1,1])\n        else:\n            print('Theta updated from')\n            print(theta)\n        n_train = w.shape[0]\n        rank = make_rank(coord, neighbor_size)\n        if n_train &lt;= 2000:\n            dist = distance_np(coord, coord)\n\n            def likelihood(theta):\n                sigma, phi, tau = theta\n                cov = sigma * (np.exp(-phi * dist)) + tau * sigma * np.eye(n_train)  # need dist, n\n\n                term1 = 0\n                term2 = 0\n                for i in range(n_train):\n                    ind = rank[i, :][rank[i, :] &lt;= i]\n                    id = np.append(ind, i)\n\n                    sub_cov = cov[ind, :][:, ind]\n                    if np.linalg.det(sub_cov):\n                        bi = np.linalg.solve(cov[ind, :][:, ind], cov[ind, i])\n                    else:\n                        bi = np.zeros(ind.shape)\n                    I_B_i = np.append(-bi, 1)\n                    F_i = cov[i, i] - np.inner(cov[ind, i], bi)\n                    decor_res = np.sqrt(np.reciprocal(F_i)) * np.dot(I_B_i, w[id])\n                    term1 += np.log(F_i)\n                    term2 += decor_res ** 2\n                return (term1 + term2)\n\n        else:\n            def likelihood(theta):\n                sigma, phi, tau = theta\n\n                term1 = 0\n                term2 = 0\n                for i in range(n_train):\n                    ind = rank[i, :][rank[i, :] &lt;= i]\n                    if len(ind) == 0:\n                        F_i = (1 + tau) * sigma\n                        term1 += np.log(F_i)\n                        term2 += w[i] ** 2 / F_i\n                        continue\n\n                    id = np.append(ind, i)\n\n                    sub_dist = distance_np(coord[ind, :], coord[ind, :])\n                    sub_dist_vec = distance_np(coord[ind, :], coord[i, :].reshape(1, -1)).reshape(-1)\n                    sub_cov = sigma * (np.exp(-phi * sub_dist)) + tau * sigma * np.eye(len(ind))\n                    sub_cov_vec = sigma * (np.exp(-phi * sub_dist_vec))\n                    if np.linalg.det(sub_cov):\n                        bi = np.linalg.solve(sub_cov, sub_cov_vec)\n                    else:\n                        bi = np.zeros(ind.shape)\n                    I_B_i = np.append(-bi, 1)\n                    F_i = (1 + tau) * sigma - np.inner(sub_cov_vec, bi)\n                    decor_res = np.sqrt(np.reciprocal(F_i)) * np.dot(I_B_i, w[id])\n                    term1 += np.log(F_i)\n                    term2 += decor_res ** 2\n                return (term1 + term2)\n\n        res = scipy.optimize.minimize(likelihood, theta, method = 'L-BFGS-B',\n                                      bounds = [(0, None), (0, None), (min_tau, None)])\n        print('Theta estimated as')\n        print(res.x)\n        return res.x\n\n    else:\n        _, theta = BRISC_estimation(w, None, coord)\n        theta[2] = max(theta[2], min_tau)\n        print('Theta estimated as')\n        print(theta)\n        return theta\n</code></pre>"},{"location":"utils/#geospaNN.visualize.plot_PDP","title":"<code>plot_PDP(model, X, names=[], save_path='./', save=False, split=False)</code>","text":"<p>Partial dependency plot for model on the data.</p> <p>A Partial Dependence Plot (PDP) is a visualization tool used to illustrate the relationship between a selected feature and the predicted outcome of a machine learning model, while averaging out the effects of other features. This helps to understand the marginal influence of a single feature on the model's predictions in a more interpretable way.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>Usually a model in nngls class. Can take model with .() method that take tensor X as input and predicted scalar value Y as output. (to implement for more models)</p> required <code>X</code> <code>tensor</code> <p>nxp array of the covariates.</p> required <code>names</code> <code>Optional[list]</code> <p>List of names for variable, if not specified, use \"variable 1\" to \"variable p\".</p> <code>[]</code> <code>save_path</code> <code>Optional[str]</code> <p>Directory to save the plot. Defaults to \"./\".</p> <code>'./'</code> <code>save</code> <code>bool</code> <p>Whether to save the PDPs to the working directory. Default False.</p> <code>False</code> <code>split</code> <code>bool</code> <p>Whether to return the PDPs as a list of PartialDependenceDisplay object for single variabls or a whole</p> <code>False</code> <p>Returns:</p> Type Description <p>A sklearn.inspection.PartialDependenceDisplay object contains PDPs for each variable.</p> Source code in <code>geospaNN/visualize.py</code> <pre><code>def plot_PDP(model,\n             X: torch.tensor,\n             names: Optional[list] = [],\n             save_path: Optional[str] = \"./\",\n             save: bool = False,\n             split: bool = False):\n    \"\"\"Partial dependency plot for model on the data.\n\n    A Partial Dependence Plot (PDP) is a visualization tool used to illustrate the relationship between a selected feature\n    and the predicted outcome of a machine learning model, while averaging out the effects of other features.\n    This helps to understand the marginal influence of a single feature on the model's predictions in a more interpretable way.\n\n    Parameters:\n        model:\n            Usually a model in nngls class. Can take model with .() method that take tensor X as input and\n            predicted scalar value Y as output. (to implement for more models)\n        X:\n            nxp array of the covariates.\n        names:\n            List of names for variable, if not specified, use \"variable 1\" to \"variable p\".\n        save_path:\n            Directory to save the plot. Defaults to \"./\".\n        save:\n            Whether to save the PDPs to the working directory. Default False.\n        split:\n            Whether to return the PDPs as a list of PartialDependenceDisplay object for single variabls or a whole\n\n    Returns:\n        A sklearn.inspection.PartialDependenceDisplay object contains PDPs for each variable.\n    \"\"\"\n    X = X.detach().numpy()\n    p = X.shape[1]\n    Est = _PDP_estimator()\n    Est.fit(X, model)\n    if len(names) != p:\n        warnings.warn(\"length of names does not match columns of X, replace by variable index\")\n        names = [f\"variable {i + 1}\" for i in range(p + 1)]\n\n    if not split:\n        figures = PartialDependenceDisplay.from_estimator(estimator=Est, X=X, features=[i for i in range(p)],\n                                                          feature_names=names,\n                                                          percentiles=(0.05, 0.95))\n        if save:\n            plt.savefig(save_path + names[0] + \".png\")\n\n    else:\n        figures = []\n        for k in range(p):\n            res = PartialDependenceDisplay.from_estimator(estimator=Est, X=X, features=[k],\n                                                          feature_names=names[k],\n                                                          percentiles=(0.05, 0.95))\n            plt.subplots_adjust(left=0.1, bottom=0.1, right=0.9, top=0.9,\n                                wspace=0.4, hspace=0.4)\n            if save:\n                plt.savefig(save_path + names[k] + \".png\")\n            figures.append(res)\n\n    return figures\n</code></pre>"},{"location":"utils/#geospaNN.visualize.plot_PDP_list","title":"<code>plot_PDP_list(model_list, model_names, X, feature_names=[], save_path='./', save=False, split=True)</code>","text":"<p>Partial dependency plot for model on the data.</p> <p>A Partial Dependence Plot (PDP) is a visualization tool used to illustrate the relationship between a selected feature and the predicted outcome of a machine learning model, while averaging out the effects of other features. This helps to understand the marginal influence of a single feature on the model's predictions in a more interpretable way.</p> <p>Parameters:</p> Name Type Description Default <code>model_list</code> <code>list</code> <p>A list of models described in plot_PDP.</p> required <code>model_names</code> <code>list</code> <p>A list of model names for PDP visualization, expect the same length as model_list.</p> required <code>X</code> <code>tensor</code> <p>nxp array of the covariates for PDP integration.</p> required <code>feature_names</code> <code>Optional[list]</code> <p>List of names for variable, if not specified, use \"variable 1\" to \"variable p\".</p> <code>[]</code> <code>save_path</code> <code>Optional[str]</code> <p>Directory to save the plot. Defaults to \"./\".</p> <code>'./'</code> <code>save</code> <code>bool</code> <p>Whether to save the PDPs to the working directory. Default False.</p> <code>False</code> <code>split</code> <code>bool</code> <p>Whether to save the PDP's for different features seperately or as one figure. The default is True.</p> <code>True</code> <p>Returns:</p> Type Description <p>A sklearn.inspection.PartialDependenceDisplay object contains PDPs for each variable.</p> Source code in <code>geospaNN/visualize.py</code> <pre><code>def plot_PDP_list(model_list: list,\n                  model_names: list,\n                  X: torch.tensor,\n                  feature_names: Optional[list] = [],\n                  save_path: Optional[str] = \"./\",\n                  save: bool = False,\n                  split: bool = True):\n    \"\"\"Partial dependency plot for model on the data.\n\n    A Partial Dependence Plot (PDP) is a visualization tool used to illustrate the relationship between a selected feature\n    and the predicted outcome of a machine learning model, while averaging out the effects of other features.\n    This helps to understand the marginal influence of a single feature on the model's predictions in a more interpretable way.\n\n    Parameters:\n        model_list:\n            A list of models described in plot_PDP.\n        model_names:\n            A list of model names for PDP visualization, expect the same length as model_list.\n        X:\n            nxp array of the covariates for PDP integration.\n        feature_names:\n            List of names for variable, if not specified, use \"variable 1\" to \"variable p\".\n        save_path:\n            Directory to save the plot. Defaults to \"./\".\n        save:\n            Whether to save the PDPs to the working directory. Default False.\n        split:\n            Whether to save the PDP's for different features seperately or as one figure. The default is True.\n\n    Returns:\n        A sklearn.inspection.PartialDependenceDisplay object contains PDPs for each variable.\n    \"\"\"\n    l = min(len(model_list), len(model_names))\n    if len(model_list) != len(model_names):\n        warnings.warn(\"Number of models different from number of model_names! Use top \" + l + \" ones.\")\n    PDP_list = [plot_PDP(model_list[i], X, feature_names) for i in range(l)]\n    p = X.shape[1]\n    colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n    fig, axes = plt.subplots(p, 1, figsize=(6, 4 * p))  # Adjust height proportionally\n\n    for i in range(l):\n        PDP_list[i].plot(ax=axes, line_kw={\"label\": model_names[i], \"color\": colors[i]})\n\n    if split:\n        for i, ax in enumerate(axes):\n            ax.set_title(f\"Partial Dependence for Feature {i + 1}\")\n            ax.legend()  # Ensure legends show up for each axis\n\n            # Save each axis as a separate figure\n            fig_single, ax_single = plt.subplots(figsize=(6, 4))  # Create a new figure\n            for line in ax.get_lines():  # Copy lines from the original axis\n                ax_single.plot(line.get_xdata(), line.get_ydata(), label=line.get_label())\n\n            ax_single.set_title(ax.get_title())\n            ax_single.set_xlabel(ax.get_xlabel())\n            ax_single.set_ylabel(ax.get_ylabel())\n            ax_single.legend()\n\n            # Save the figure for the current axis\n            if save:\n                fig_single.savefig(save_path + f\"PDP_feature_{i + 1}.png\")\n            plt.close(fig_single)\n    else:\n        axes.set_title(f\"Partial Dependence for Features\")\n        axes.legend()  # Ensure legends show up for each axis\n        fig_single.savefig(save_path + f\"PDP_features.png\")\n</code></pre>"},{"location":"utils/#geospaNN.visualize.plot_log","title":"<code>plot_log(training_log, theta, save_path='./', save=False)</code>","text":"<p>Output visualization for NN-GLS training.</p> <p>This is a simple visualization of the training log from geospaNN.nngls_train.train()</p> <p>Parameters:</p> Name Type Description Default <code>training_log</code> <code>list</code> <pre><code>A list contains vectors of validation loss, spatial parameters sigma, phi, and tau.\nLengths of the vectors must be the same.\n</code></pre> required <code>theta</code> <code>tuple[float, float, float]</code> <p>theta[0], theta[1], theta[2] represent sigma^2, phi, tau in the exponential covariance family.</p> required <code>save_path</code> <code>Optional[str]</code> <p>Directory to save the plot. Defaults to \"./\".</p> <code>'./'</code> <code>save</code> <code>bool</code> <p>Whether to save the PDPs to the working directory. Default False.</p> <code>False</code> Source code in <code>geospaNN/visualize.py</code> <pre><code>def plot_log(training_log: list,\n             theta: tuple[float, float, float],\n             save_path: Optional[str] = \"./\",\n             save: bool = False):\n    \"\"\"Output visualization for NN-GLS training.\n\n    This is a simple visualization of the training log from geospaNN.nngls_train.train()\n\n    Parameters:\n        training_log:\n                A list contains vectors of validation loss, spatial parameters sigma, phi, and tau.\n                Lengths of the vectors must be the same.\n        theta:\n            theta[0], theta[1], theta[2] represent sigma^2, phi, tau in the exponential covariance family.\n        save_path:\n            Directory to save the plot. Defaults to \"./\".\n        save:\n            Whether to save the PDPs to the working directory. Default False.\n    \"\"\"\n    epoch = len(training_log[\"val_loss\"])\n    training_log[\"epoch\"] = list(range(1, epoch + 1))\n    training_log = pd.DataFrame(training_log)\n\n    # Melting the dataframe to make it suitable for seaborn plotting\n    training_log_melted = training_log[[\"epoch\", \"val_loss\"]].melt(id_vars='epoch', var_name='Variable', value_name='Value')\n    # Plotting with seaborn\n    # Creating two subplots side by side\n    fig, axes = plt.subplots(1, 2, figsize=(20, 6))\n    sns.lineplot(ax=axes[0], data=training_log_melted, x='epoch', y='Value', hue='Variable', style='Variable', markers=False, dashes=False)\n\n    axes[0].set_title('Validation and prediction loss over Epochs (Log Scale) with Benchmark', fontsize=14)\n    axes[0].set_xlabel('Epoch', fontsize=15)\n    axes[0].set_ylabel('Value (Log Scale)', fontsize=15)\n    axes[0].set_yscale('log')\n    axes[0].legend(prop={'size': 15})\n    axes[0].tick_params(labelsize=14)\n    axes[0].grid(True)\n\n    # Second plot (sigma, phi, tau)\n    kernel_params_melted = training_log[[\"epoch\", \"sigma\", \"phi\", \"tau\"]].melt(id_vars='epoch', var_name='Variable', value_name='Value')\n    ground_truth = {'sigma': theta[0], 'phi': theta[1], 'tau': theta[2]}\n    sns.lineplot(ax=axes[1], data=kernel_params_melted, x='epoch', y='Value', hue='Variable', style='Variable', markers=False, dashes=False)\n    palette = sns.color_palette()\n    for i, (param, gt_value) in enumerate(ground_truth.items()):\n        axes[1].hlines(y=gt_value, xmin=1, xmax=epoch, color=palette[i], linestyle='--')\n    axes[1].set_title('Parameter Values (log) over Epochs with Ground Truth', fontsize=14)\n    axes[1].set_xlabel('Epoch', fontsize=15)\n    axes[1].set_ylabel('Value', fontsize=15)\n    axes[1].set_yscale('log')\n    axes[1].legend(prop={'size': 15})\n    axes[1].tick_params(labelsize=14)\n    axes[1].grid(True)\n\n    plt.tight_layout()\n    if save:\n        plt.savefig(save_path + \"training_log.png\")\n</code></pre>"},{"location":"utils/#geospaNN.visualize.spatial_plot_surface","title":"<code>spatial_plot_surface(variable, coord, title='Variable', save_path='./', file_name='spatial_surface.png', grid_resolution=1000, method='CloughTocher', cmap='viridis', show=False)</code>","text":"<p>Plots a smooth surface for spatial data.</p> <p>The function interpolates scattered data onto a regular grid and generates a smooth surface plot. The resulting plot is saved as a PNG file.</p> <p>Parameters:</p> Name Type Description Default <code>variable</code> <code>array - like</code> <p>Values to plot, corresponding to the coordinates.</p> required <code>coord</code> <code>array - like</code> <p>Coordinates of shape (n, 2) where each row represents a point (x, y).</p> required <code>title</code> <code>str</code> <p>Title of the plot. Defaults to \"Variable\".</p> <code>'Variable'</code> <code>save_path</code> <code>str</code> <p>Directory to save the plot. Defaults to \"./\".</p> <code>'./'</code> <code>file_name</code> <code>str</code> <p>Name of the saved plot file. Defaults to \"spatial_surface.png\".</p> <code>'spatial_surface.png'</code> <code>grid_resolution</code> <code>int</code> <p>Resolution of the interpolation grid. Higher values produce finer surfaces. Defaults to 100.</p> <code>1000</code> <code>cmap</code> <code>str</code> <p>Colormap to use for the plot. Defaults to 'viridis'.</p> <code>'viridis'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the lengths of <code>variable</code> and <code>coord</code> do not match.</p> <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>The function saves the plot to a file and does not return any value.</p> Example <p>import numpy as np coord = np.random.uniform(0, 10, (50, 2)) variable = np.sin(coord[:, 0]) + np.cos(coord[:, 1]) spatial_plot_surface(variable, coord, title=\"Example Plot\", grid_resolution=200)</p> Source code in <code>geospaNN/visualize.py</code> <pre><code>def spatial_plot_surface(variable: np.array,\n                         coord: np.array,\n                         title: Optional[str] = \"Variable\",\n                         save_path: Optional[str] = \"./\",\n                         file_name: Optional[str] = \"spatial_surface.png\",\n                         grid_resolution: Optional[int] = 1000,\n                         method: Optional[str] = \"CloughTocher\",\n                         cmap: Optional[str] = 'viridis',\n                         show: Optional[bool] = False\n                         ) -&gt; None:\n    \"\"\"\n    Plots a smooth surface for spatial data.\n\n    The function interpolates scattered data onto a regular grid and generates\n    a smooth surface plot. The resulting plot is saved as a PNG file.\n\n    Parameters:\n        variable (array-like): Values to plot, corresponding to the coordinates.\n        coord (array-like): Coordinates of shape (n, 2) where each row represents a point (x, y).\n        title (str, optional): Title of the plot. Defaults to \"Variable\".\n        save_path (str, optional): Directory to save the plot. Defaults to \"./\".\n        file_name (str, optional): Name of the saved plot file. Defaults to \"spatial_surface.png\".\n        grid_resolution (int, optional): Resolution of the interpolation grid.\n            Higher values produce finer surfaces. Defaults to 100.\n        cmap (str, optional): Colormap to use for the plot. Defaults to 'viridis'.\n\n    Raises:\n        ValueError: If the lengths of `variable` and `coord` do not match.\n\n    Returns:\n        None: The function saves the plot to a file and does not return any value.\n\n    Example:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; coord = np.random.uniform(0, 10, (50, 2))\n        &gt;&gt;&gt; variable = np.sin(coord[:, 0]) + np.cos(coord[:, 1])\n        &gt;&gt;&gt; spatial_plot_surface(variable, coord, title=\"Example Plot\", grid_resolution=200)\n    \"\"\"\n    if len(variable) != len(coord):\n        raise ValueError(\"Length of 'variable' and 'coord' must match.\")\n\n    # Create grid for interpolation\n    xi = np.linspace(coord[:, 0].min(), coord[:, 0].max(), grid_resolution)\n    yi = np.linspace(coord[:, 1].min(), coord[:, 1].max(), grid_resolution)\n    X, Y = np.meshgrid(xi, yi)\n\n    # Interpolate data onto the grid\n\n    if method == \"CloughTocher\":\n        interp = CloughTocher2DInterpolator(list(zip(coord[:, 0], coord[:, 1])), variable)\n        Z = interp(X, Y)\n    elif method in ['linear', 'nearest', 'cubic']:\n        Z = griddata(coord, variable, (X, Y), method=method)\n    else:\n        warnings.warn(\"No interpolation method provided, use cubic spline as default!\")\n        Z = griddata(coord, variable, (X, Y), method=\"cubic\")\n\n    # Plot the interpolated surface\n    plt.clf()\n    fig = plt.figure(figsize=(8, 6))\n    surface = plt.pcolormesh(X, Y, Z, cmap=cmap, shading='auto')\n    plt.colorbar(surface, label='Variable')\n    plt.title(title)\n    plt.xlabel('Coord_X')\n    plt.ylabel('Coord_Y')\n    plt.savefig(f\"{save_path}/{file_name}\")\n    if show:\n        plt.show()\n\n    return fig\n</code></pre>"},{"location":"Example_GAM/Example_GAM/","title":"Example GAM","text":"<pre><code>import torch\nimport geospaNN\nimport numpy as np\nimport time\nimport pandas as pd\nimport seaborn as sns\nimport scipy\nimport lhsmdu\nimport random\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n\npath = '../data/Output/'\n</code></pre> <pre><code>def RMSE(x,y):\n    x = x.reshape(-1)\n    y = y.reshape(-1)\n    n = x.shape[0]\n    return(np.sqrt(np.sum(np.square(x-y))/n))\n\ndef f5(X): return (10 * np.sin(np.pi * X[:, 0] * X[:, 1]) + 20 * (X[:, 2] - 0.5) ** 2 + 10 * X[:, 3] + 5 * X[:, 4]) / 6\n\nsigma = 1\nphi = 3\ntau = 0.01\ntheta = torch.tensor([sigma, phi / np.sqrt(2), tau])\n\np = 5;\nfunXY = f5\n\nn = 1000\nnn = 20\nbatch_size = 50\n\nN = 1000\nn_small = int(N / 100)\nnp.random.seed(2021)\nX_MISE = np.array(lhsmdu.sample(p, n_small).reshape((n_small, p)))\nfor i in range(99):\n    temp = np.array(lhsmdu.sample(p, n_small).reshape((n_small, p)))\n    X_MISE = np.concatenate((X_MISE, temp))\n\nX_MISE_torch = torch.from_numpy(X_MISE).float()\n</code></pre> <pre><code>torch.manual_seed(2025)\nX, Y, coord, cov, corerr = geospaNN.Simulation(n, p, nn, funXY, theta, range=[0, 10])\n\nX, Y, coord, _ = geospaNN.spatial_order(X, Y, coord, method='max-min')\ndata = geospaNN.make_graph(X, Y, coord, nn)\n\ntorch.manual_seed(2025)\nnp.random.seed(0)\ndata_train, data_val, data_test = geospaNN.split_data(X, Y, coord, neighbor_size=nn,\n                                                      test_proportion=0.2)\n</code></pre> <pre><code>import utils_pygam\n</code></pre> <pre><code>torch.manual_seed(2024)\nmlp_nn = torch.nn.Sequential(\n    torch.nn.Linear(p, 50),\n    torch.nn.ReLU(),\n    torch.nn.Linear(50, 1)\n)\nnn_model = geospaNN.nn_train(mlp_nn, lr=0.01, min_delta=0.001)\ntraining_log = nn_model.train(data_train, data_val, data_test, seed = 2024)\ntheta0 = geospaNN.theta_update(mlp_nn(data_train.x).squeeze() - data_train.y,data_train.pos, neighbor_size=20)\n</code></pre> <pre><code>Epoch 00089: reducing learning rate of group 0 to 5.0000e-03.\nINFO: Early stopping\nEnd at epoch92\n---------------------------------------- \n    Ordering Coordinates \n----------------------------------------\n    Model description\n----------------------------------------\nBRISC model fit with 600 observations.\n\nNumber of covariates 1 (including intercept if specified).\n\nUsing the exponential spatial correlation model.\n\nUsing 15 nearest neighbors.\n\n\n\nSource not compiled with OpenMP support.\n----------------------------------------\n    Building neighbor index\n----------------------------------------\n    Performing optimization\n----------------------------------------\n    Processing optimizers\n----------------------------------------\nTheta estimated as\n[0.88725394 2.87048584 0.04966461]\n</code></pre> <pre><code>Est_NN = mlp_nn(X_MISE_torch).detach().numpy()\nRMSE(Est_NN, funXY(X_MISE))\n</code></pre> <pre><code>0.3801058505234924\n</code></pre> <pre><code>torch.manual_seed(2024)\nmlp_nngls = torch.nn.Sequential(\n    torch.nn.Linear(p, 50),\n    torch.nn.ReLU(),\n    torch.nn.Linear(50, 20),\n    torch.nn.ReLU(),\n    torch.nn.Linear(20, 1)\n)\nmodel_nngls = geospaNN.nngls(p=p, neighbor_size=nn, coord_dimensions=2, mlp=mlp_nngls, theta=torch.tensor(theta0))\nnngls_model = geospaNN.nngls_train(model_nngls, lr=0.01, min_delta=0.001)\ntraining_log = nngls_model.train(data_train, data_val, data_test,\n                                 Update_init=20, Update_step=10, seed = 2024)\n</code></pre> <pre><code>---------------------------------------- \n    Ordering Coordinates \n----------------------------------------\n    Model description\n----------------------------------------\nBRISC model fit with 600 observations.\n\nNumber of covariates 1 (including intercept if specified).\n\nUsing the exponential spatial correlation model.\n\nUsing 15 nearest neighbors.\n\n\n\nSource not compiled with OpenMP support.\n----------------------------------------\n    Building neighbor index\n----------------------------------------\n    Performing optimization\n----------------------------------------\n    Processing optimizers\n----------------------------------------\nTheta estimated as\n[0.93411162 2.81610403 0.00887926]\n...\nINFO: Early stopping\nEnd at epoch52\n</code></pre> <pre><code>Est_NNGLS = mlp_nngls(X_MISE_torch).detach().numpy()\nRMSE(Est_NNGLS, funXY(X_MISE))\n</code></pre> <pre><code>0.35517833944095023\n</code></pre> <pre><code>torch.manual_seed(2024)\nnp.random.seed(2024)\nX_train = data_train.x.detach().numpy()\nY_train = data_train.y.detach().numpy()\ngam = utils_pygam.my_LinearGAM()\ngam.fit(X_train, Y_train)\nXspline = gam._modelmat(X_train)\ngam.my_fit(X_train, Xspline, Y_train)\n</code></pre> <pre><code>my_LinearGAM(callbacks=[Deviance(), Diffs()], fit_intercept=True, \n   max_iter=100, scale=None, \n   terms=s(0) + s(1) + s(2) + s(3) + s(4) + intercept, tol=0.0001, \n   verbose=False)\n</code></pre> <pre><code>Est_GAM = gam.predict(X_MISE)\nRMSE(Est_GAM, funXY(X_MISE))\n</code></pre> <pre><code>0.3790614014474388\n</code></pre> <pre><code>torch.manual_seed(2024)\nnp.random.seed(2024)\nX_train = data_train.x.detach().numpy()\nY_train = data_train.y.detach().numpy()\ngam = utils_pygam.my_LinearGAM()\ngam.fit(X_train, Y_train)\nXspline = gam._modelmat(X_train)\nI_B_GAM, F_GAM = geospaNN.make_bf(data_train.pos, theta0, nn)\nI_B_GAM = I_B_GAM.to_dense()\nF_GAM = F_GAM.detach().numpy()\nFI_B_GAM = (I_B_GAM.T * np.sqrt(np.reciprocal(F_GAM))).T\ngam.my_fit(X_train,\n           scipy.sparse.csr_matrix(np.matmul(FI_B_GAM, Xspline.todense())),\n           np.array(np.matmul(FI_B_GAM, Y_train)))\n</code></pre> <pre><code>my_LinearGAM(callbacks=[Deviance(), Diffs()], fit_intercept=True, \n   max_iter=100, scale=None, \n   terms=s(0) + s(1) + s(2) + s(3) + s(4) + intercept, tol=0.0001, \n   verbose=False)\n</code></pre> <pre><code>Est_GAMGLS = gam.predict(X_MISE)\nRMSE(Est_GAMGLS, funXY(X_MISE))\n</code></pre> <pre><code>0.3538191720461421\n</code></pre> <pre><code>rho_vec = []\nMSE_GAMGLS = []\nMSE_NNGLS = []\nfor rho in list(np.array(range(21))/20):\n    print(rho)\n    def f5(X): return rho * (10 * np.sin(np.pi * X[:, 0] * X[:, 1]))/3 + \\\n                      (1-rho)*(20 * (X[:, 2] - 0.5) ** 2 + 10 * X[:, 3] + 5 * X[:,4]) / 3\n\n    funXY = f5\n    torch.manual_seed(2024)\n    X, Y, coord, cov, corerr = geospaNN.Simulation(n, p, nn, funXY, theta, range=[0, 10])\n    random.seed(2024)\n    X, Y, coord, _ = geospaNN.spatial_order(X, Y, coord, method='max-min')\n    data = geospaNN.make_graph(X, Y, coord, nn)\n    torch.manual_seed(2024)\n    np.random.seed(0)\n    data_train, data_val, data_test = geospaNN.split_data(X, Y, coord, neighbor_size=nn,\n                                                          test_proportion=0.2)\n    torch.manual_seed(2024)\n    mlp_nngls = torch.nn.Sequential(\n        torch.nn.Linear(p, 50),\n        torch.nn.ReLU(),\n        torch.nn.Linear(50, 20),\n        torch.nn.ReLU(),\n        torch.nn.Linear(20, 1)\n    )\n    model_nngls = geospaNN.nngls(p=p, neighbor_size=nn, coord_dimensions=2, mlp=mlp_nngls, theta=torch.tensor(theta0))\n    nngls_model = geospaNN.nngls_train(model_nngls, lr=0.02, min_delta=0.001)\n    training_log = nngls_model.train(data_train, data_val, data_test,\n                                     Update_init=10, Update_step=5, seed = 2024)\n    Est_NNGLS = mlp_nngls(X_MISE_torch).detach().numpy()\n    MSE_NNGLS.append(RMSE(Est_NNGLS, funXY(X_MISE)))\n\n    torch.manual_seed(2024)\n    np.random.seed(2024)\n    X_train = data_train.x.detach().numpy()\n    Y_train = data_train.y.detach().numpy()\n    gam = utils_pygam.my_LinearGAM()\n    gam.fit(X_train, Y_train)\n    Xspline = gam._modelmat(X_train)\n    I_B_GAM, F_GAM = geospaNN.make_bf(data_train.pos, theta0, nn)\n    I_B_GAM = I_B_GAM.to_dense()\n    F_GAM = F_GAM.detach().numpy()\n    FI_B_GAM = (I_B_GAM.T * np.sqrt(np.reciprocal(F_GAM))).T\n    gam.my_fit(X_train,\n               scipy.sparse.csr_matrix(np.matmul(FI_B_GAM, Xspline.todense())),\n               np.array(np.matmul(FI_B_GAM, Y_train)))\n\n    Est_GAMGLS = gam.predict(X_MISE)\n    MSE_GAMGLS.append(RMSE(Est_GAMGLS, funXY(X_MISE)))\n    rho_vec.append(rho) \n</code></pre> <pre><code>df_MSE = pd.DataFrame(\n    {'GAMGLS': np.array(MSE_GAMGLS), 'NNGLS': np.array(MSE_NNGLS), 'rho': np.array(rho_vec)})\n</code></pre> <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Plot using Matplotlib\nplt.figure(figsize=(8, 6))\nplt.scatter(df_MSE['rho'], df_MSE['GAMGLS'], label='GAMGLS', marker='o')\nplt.scatter(df_MSE['rho'], df_MSE['NNGLS'], label='NNGLS', marker='s')\n\n# Add titles and labels\nplt.title('MSE vs strength of interaction', fontsize=14)\nplt.xlabel('Strength of interaction (rho)', fontsize=12)\nplt.ylabel('Mean Squared Error (MSE)', fontsize=12)\nplt.legend(fontsize=12)\nplt.grid(True)\n\n# Show the plot\nplt.tight_layout()\nplt.savefig(path + \"GAM.png\")\n</code></pre> <pre><code>degree = 3  # Degree of the polynomial\nrho_smooth = np.linspace(df_MSE['rho'].min(), df_MSE['rho'].max(), 200)\n\n# Fit polynomials for GAMGLS and NNGLS\ngamgls_fit = np.polyfit(df_MSE['rho'], df_MSE['GAMGLS'], degree)\nnngls_fit = np.polyfit(df_MSE['rho'], df_MSE['NNGLS'], degree)\n\n# Evaluate the polynomial on a smooth grid\ngamgls_smooth = np.polyval(gamgls_fit, rho_smooth)\nnngls_smooth = np.polyval(nngls_fit, rho_smooth)\n\n# Plot using Matplotlib\nplt.clf()\nplt.figure(figsize=(8, 6))\n\n# Original points\nplt.plot(df_MSE['rho'], df_MSE['GAMGLS'], label='GAMGLS (Data)', marker='o', linestyle='none')\nplt.plot(df_MSE['rho'], df_MSE['NNGLS'], label='NNGLS (Data)', marker='s', linestyle='none')\n\n# Smoothed curves\nplt.plot(rho_smooth, gamgls_smooth, label='GAMGLS (Smoothed)', color='blue', linestyle='-', linewidth=1.5)\nplt.plot(rho_smooth, nngls_smooth, label='NNGLS (Smoothed)', color='orange', linestyle='-', linewidth=1.5)\n# Add titles and labels\nplt.title('MSE vs strength of interaction', fontsize=14)\nplt.xlabel('Strength of interaction (rho)', fontsize=12)\nplt.ylabel('Mean Squared Error (MSE)', fontsize=12)\nplt.legend(fontsize=12)\nplt.grid(True)\n\n# Show the plot\nplt.tight_layout()\nplt.savefig(path + \"GAM_fit.png\")\n</code></pre> <pre><code>&lt;Figure size 640x480 with 0 Axes&gt;\n</code></pre> <pre><code>\n</code></pre>"},{"location":"Example_addcovariates/Example_addcovariates/","title":"Example addcovariates","text":"<pre><code>import torch\nimport geospaNN\nimport numpy as np\nimport time\nimport pandas as pd\nimport seaborn as sns\nimport random\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n\npath = '../data/Output/'\n</code></pre> <pre><code>def RMSE(x,y):\n    x = x.reshape(-1)\n    y = y.reshape(-1)\n    n = x.shape[0]\n    return(np.sqrt(np.sum(np.square(x-y))/n))\n</code></pre> <pre><code>def f5(X): return (10 * np.sin(np.pi * X[:, 0] * X[:, 1]) + 20 * (X[:, 2] - 0.5) ** 2 + 10 * X[:, 3] + 5 * X[:, 4]) / 6\ndef f1(X): return 10 * np.sin(2 * np.pi * X)\n\nsigma = 5\nphi = 0.3\ntau = 0.01\ntheta = torch.tensor([sigma, phi / np.sqrt(2), tau])\n\np = 1;\nfunXY = f1\n\nn = 1000\nb = 10\nnn = 20\nbatch_size = 50\n</code></pre> <pre><code>torch.manual_seed(2025)\nX, Y, coord, cov, corerr = geospaNN.Simulation(n, p, nn, funXY, theta, range=[0, b])\n\nrandom.seed(2024)\nX, Y, coord, _ = geospaNN.spatial_order(X, Y, coord, method='max-min')\ndata = geospaNN.make_graph(X, Y, coord, nn)\n\ntorch.manual_seed(2024)\nnp.random.seed(0)\ndata_train, data_val, data_test = geospaNN.split_data(X, Y, coord, neighbor_size=nn,\n                                                      test_proportion=0.2)\n</code></pre> <pre><code>torch.manual_seed(2024)\nmlp_nn = torch.nn.Sequential(\n    torch.nn.Linear(p, 50),\n    torch.nn.ReLU(),\n    torch.nn.Linear(50, 20),\n    torch.nn.ReLU(),\n    torch.nn.Linear(20, 1)\n)\nnn_model = geospaNN.nn_train(mlp_nn, lr=0.01, min_delta=0.001)\ntraining_log = nn_model.train(data_train, data_val, data_test, seed = 2024)\ntheta0 = geospaNN.theta_update(mlp_nn(data_train.x).squeeze() - data_train.y,\n                               data_train.pos, neighbor_size=20)\nmodel = geospaNN.nngls(p=p, neighbor_size=nn, coord_dimensions=2, mlp=mlp_nn, theta=torch.tensor(theta0))\npredict_nn = model.predict(data_train, data_test)\n</code></pre> <pre><code>Epoch 00061: reducing learning rate of group 0 to 5.0000e-03.\nEpoch 00068: reducing learning rate of group 0 to 2.5000e-03.\nINFO: Early stopping\nEnd at epoch71\n---------------------------------------- \n    Ordering Coordinates \n----------------------------------------\n    Model description\n----------------------------------------\nBRISC model fit with 600 observations.\n\nNumber of covariates 1 (including intercept if specified).\n\nUsing the exponential spatial correlation model.\n\nUsing 15 nearest neighbors.\n\n\n\nSource not compiled with OpenMP support.\n----------------------------------------\n    Building neighbor index\n----------------------------------------\n    Performing optimization\n----------------------------------------\n    Processing optimizers\n----------------------------------------\nTheta estimated as\n[3.8977135  0.48003391 0.01919864]\n</code></pre> <pre><code>torch.manual_seed(2024)\nmlp_nngls = torch.nn.Sequential(\n    torch.nn.Linear(p, 50),\n    torch.nn.ReLU(),\n    torch.nn.Linear(50, 20),\n    torch.nn.ReLU(),\n    torch.nn.Linear(20, 1)\n)\nmodel_nngls = geospaNN.nngls(p=p, neighbor_size=nn, coord_dimensions=2, mlp=mlp_nngls, theta=torch.tensor(theta0))\nnngls_model = geospaNN.nngls_train(model_nngls, lr=0.1, min_delta=0.001)\ntraining_log = nngls_model.train(data_train, data_val, data_test,\n                                 Update_init=20, Update_step=10, seed = 2024)\ntheta_hat = geospaNN.theta_update(mlp_nngls(data_train.x).squeeze() - data_train.y,\n                                  data_train.pos, neighbor_size = 20)\nmodel = geospaNN.nngls(p=p, neighbor_size=nn, coord_dimensions=2, mlp=mlp_nngls, theta=torch.tensor(theta_hat))\npredict_nngls = model.predict(data_train, data_test)\n</code></pre> <pre><code>---------------------------------------- \n    Ordering Coordinates \n----------------------------------------\n    Model description\n----------------------------------------\nBRISC model fit with 600 observations.\n\nNumber of covariates 1 (including intercept if specified).\n\nUsing the exponential spatial correlation model.\n\nUsing 15 nearest neighbors.\n\n\n\nSource not compiled with OpenMP support.\n----------------------------------------\n    Building neighbor index\n----------------------------------------\n    Performing optimization\n----------------------------------------\n    Processing optimizers\n----------------------------------------\nTheta estimated as\n[3.74956068 0.58783613 0.05867977]\n...\n</code></pre> <pre><code>print(theta_hat)\nprint(theta0)\n</code></pre> <pre><code>[4.26710204 0.41945289 0.0132123 ]\n[3.8977135  0.48003391 0.01919864]\n</code></pre> <pre><code>data_add_train = geospaNN.make_graph(torch.concat([data_train.x, data_train.pos], axis = 1), \n                                     data_train.y, data_train.pos, nn)\ndata_add_val = geospaNN.make_graph(torch.concat([data_val.x, data_val.pos], axis = 1), \n                                   data_val.y, data_val.pos, nn)\ndata_add_test = geospaNN.make_graph(torch.concat([data_test.x, data_test.pos], axis = 1), \n                                    data_test.y, data_test.pos, nn)\n</code></pre> <pre><code>torch.manual_seed(2024)\nnp.random.seed(0)\ndata_add_train, data_add_val, data_add_test = geospaNN.split_data(torch.concat([X, coord], axis = 1), Y, coord, neighbor_size=nn,\n                                                                  test_proportion=0.2)\n</code></pre> <pre><code>torch.manual_seed(2025)\nmlp_nn_add = torch.nn.Sequential(\n    torch.nn.Linear(p+2, 50),\n    torch.nn.ReLU(),\n    torch.nn.Linear(50, 20),\n    torch.nn.ReLU(),\n    torch.nn.Linear(20, 1)\n)\nnn_add_model = geospaNN.nn_train(mlp_nn_add, lr=0.01, min_delta=0.001)\ntraining_log = nn_add_model.train(data_add_train, data_add_val, data_add_test, seed = 2024)\npredict_nn_add = mlp_nn_add(data_add_test.x).detach().numpy().reshape(-1)\n</code></pre> <pre><code>Epoch 00087: reducing learning rate of group 0 to 5.0000e-03.\n</code></pre> <pre><code>coord_np = coord.detach().numpy()\nnum_basis = [2 ** 2, 4 ** 2, 6 ** 2]\nknots_1d = [np.linspace(0, 1, int(np.sqrt(i))) for i in num_basis]\n##Wendland kernel\nK = 0\nphi_temp = np.zeros((n, sum(num_basis)))\nfor res in range(len(num_basis)):\n    theta_temp = 1 / np.sqrt(num_basis[res]) * 2.5\n    knots_s1, knots_s2 = np.meshgrid(knots_1d[res], knots_1d[res])\n    knots = np.column_stack((knots_s1.flatten(), knots_s2.flatten()))\n    for i in range(num_basis[res]):\n        d = np.linalg.norm(coord_np / b - knots[i, :], axis=1) / theta_temp\n        for j in range(len(d)):\n            if d[j] &gt;= 0 and d[j] &lt;= 1:\n                phi_temp[j, i + K] = (1 - d[j]) ** 6 * (35 * d[j] ** 2 + 18 * d[j] + 3) / 3\n            else:\n                phi_temp[j, i + K] = 0\n    K = K + num_basis[res]\n\ntorch.manual_seed(2024)\nnp.random.seed(0)\ndata_DK_train, data_DK_val, data_DK_test = geospaNN.split_data(torch.concat([X, torch.from_numpy(phi_temp)], axis = 1).float(), \n                                                               Y, coord, neighbor_size=nn, test_proportion=0.2)\n</code></pre> <pre><code>torch.manual_seed(2024)\nmlp_nn_DK = torch.nn.Sequential(\n    torch.nn.Linear(p+K, 50),\n    torch.nn.ReLU(), \n    torch.nn.Linear(50, 20), \n    torch.nn.ReLU(), \n    torch.nn.Linear(20, 1)) \nnn_DK_model = geospaNN.nn_train(mlp_nn_DK, lr=0.01, min_delta=0.001) \ntraining_log = nn_DK_model.train(data_DK_train, data_DK_val, data_DK_test, seed = 2024) \npredict_DK = mlp_nn_DK(data_DK_test.x).detach().numpy().reshape(-1)\n</code></pre> <pre><code>Epoch 00054: reducing learning rate of group 0 to 5.0000e-03.\nEpoch 00061: reducing learning rate of group 0 to 2.5000e-03.\nINFO: Early stopping\nEnd at epoch64\n</code></pre> <pre><code>plt.clf()\nplt.scatter(X.detach().numpy(), Y.detach().numpy(), s=1, label='data')\nplt.scatter(X.detach().numpy(), funXY(X.detach().numpy()), s=1, label='f(x)')\nplt.scatter(X.detach().numpy(), mlp_nn(X).detach().numpy(), s=1, label='NN')\nplt.scatter(X.detach().numpy(), mlp_nngls(X).detach().numpy(), s=1, label='NNGLS')\nplt.legend()\nplt.show()\n</code></pre> <pre><code>estimate_nn = mlp_nn(data_test.x).detach().numpy().reshape(-1)\nplt.clf()\nplt.scatter(data_test.y.detach().numpy(), data_test.y.detach().numpy(), s=1, alpha = 0.5, label='Truth')\nplt.scatter(data_test.y.detach().numpy(), predict_nngls, s=1, alpha = 0.5, label='NNGLS')\nplt.scatter(data_test.y.detach().numpy(), estimate_nn, s=1, alpha = 0.5, label='NN estimate')\nplt.scatter(data_test.y.detach().numpy(), predict_nn, s=1, alpha = 0.5, label='NN + kriging')\nplt.scatter(data_test.y.detach().numpy(), predict_nn_add, s=1, alpha = 0.5, label='NN-add-coords')\nplt.scatter(data_test.y.detach().numpy(), predict_DK, s=1, alpha = 0.5, label='NN-Deepkrig')\n#plt.scatter(data_test.y.detach().numpy(), predict3, s=1, alpha = 0.5, label='NNGLS nugget 97%')\nlgnd = plt.legend(fontsize=10)\nplt.xlabel('Observed y', fontsize=10)\nplt.ylabel('Predicted y from x and locations', fontsize=10)\nplt.title('Prediction')\n\nfor handle in lgnd.legend_handles:\n    handle.set_sizes([10.0])\nplt.savefig(path + 'Prediction_block5.png')\n\nprint(f\"RMSE nn-estimate:  {torch.mean((data_test.y - estimate_nn)**2):.2f}\")\nprint(f\"RMSE nngls:  {torch.mean((data_test.y - predict_nngls)**2):.2f}\")\nprint(f\"RMSE nn+kriging: {torch.mean((data_test.y - predict_nn)**2):.2f}\")\nprint(f\"RMSE nn-add-coordinates: {torch.mean((data_test.y - predict_nn_add)**2):.2f}\")\nprint(f\"RMSE nn-Deepkrig: {torch.mean((data_test.y - predict_DK)**2):.2f}\")\n</code></pre> <pre><code>RMSE nn-estimate:  2.98\nRMSE nngls:  0.45\nRMSE nn+kriging: 0.52\nRMSE nn-add-coordinates: 2.84\nRMSE nn-Deepkrig: 1.59\n</code></pre> <pre><code>colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\nlabels = [\"Truth\", \"NNGLS\", \"NN estimate\", \"NN + kriging\", \"NN-add-coords\", \"NN-DeepKrig\"]\ndata = [data_test.y.detach().numpy(), predict_nngls, estimate_nn, predict_nn,  predict_nn_add, predict_DK]\n\nfor i, label in enumerate(labels):\n    if i &lt;= 1:\n        continue\n    plt.clf()\n    plt.scatter(data_test.y.detach().numpy(), data_test.y.detach().numpy(), s=1.5, alpha = 0.8, label='Truth')\n    plt.scatter(data_test.y.detach().numpy(), predict_nngls, s=1.5, alpha = 0.8, label='NNGLS')\n    plt.scatter(data_test.y.detach().numpy(), data[i], s=1.5, alpha = 0.8, label=label, color = colors[i])\n    #plt.scatter(data_test.y.detach().numpy(), predict3, s=1, alpha = 0.5, label='NNGLS nugget 97%')\n    lgnd = plt.legend(fontsize=10)\n    plt.xlabel('Observed y', fontsize=10)\n    plt.ylabel('Predicted y from x and locations', fontsize=10)\n    plt.title('Prediction with ' + label)\n\n    for handle in lgnd.legend_handles:\n        handle.set_sizes([10.0])\n    #plt.show()\n    plt.savefig(path + 'Prediction_[5 03 01]_' + label + '.png')\n</code></pre> <pre><code>MSE_nngls = []\nMSE_nn = []\nMSE_nnkrig = []\nMSE_nnadd = []\nMSE_nnDK = []\nn_vec = [1000, 2000, 5000, 10000]\n\nfor n in n_vec:\n    b = 10\n    torch.manual_seed(2025)\n    X, Y, coord, cov, corerr = geospaNN.Simulation(n, p, nn, funXY, theta, range=[0, b])\n\n    random.seed(2024)\n    X, Y, coord, _ = geospaNN.spatial_order(X, Y, coord, method='max-min')\n    data = geospaNN.make_graph(X, Y, coord, nn)\n\n    torch.manual_seed(2024)\n    np.random.seed(0)\n    data_train, data_val, data_test = geospaNN.split_data(X, Y, coord, neighbor_size=nn,\n                                                          test_proportion=0.2)\n    torch.manual_seed(2024)\n    mlp_nn = torch.nn.Sequential(\n        torch.nn.Linear(p, 50),\n        torch.nn.ReLU(),\n        torch.nn.Linear(50, 20),\n        torch.nn.ReLU(),\n        torch.nn.Linear(20, 1)\n    )\n    nn_model = geospaNN.nn_train(mlp_nn, lr=0.01, min_delta=0.001)\n    training_log = nn_model.train(data_train, data_val, data_test, seed = 2024)\n    theta0 = geospaNN.theta_update(mlp_nn(data_train.x).squeeze() - data_train.y,\n                                   data_train.pos, neighbor_size=20)\n    model = geospaNN.nngls(p=p, neighbor_size=nn, coord_dimensions=2, mlp=mlp_nn, theta=torch.tensor(theta0))\n    predict_nn = model.predict(data_train, data_test)\n    estimate_nn = mlp_nn(data_test.x).detach().numpy().reshape(-1)\n\n    torch.manual_seed(2024)\n    mlp_nngls = torch.nn.Sequential(\n        torch.nn.Linear(p, 50),\n        torch.nn.ReLU(),\n        torch.nn.Linear(50, 20),\n        torch.nn.ReLU(),\n        torch.nn.Linear(20, 1)\n    )\n    model_nngls = geospaNN.nngls(p=p, neighbor_size=nn, coord_dimensions=2, mlp=mlp_nngls, theta=torch.tensor(theta0))\n    nngls_model = geospaNN.nngls_train(model_nngls, lr=0.1, min_delta=0.001)\n    training_log = nngls_model.train(data_train, data_val, data_test,\n                                     Update_init=20, Update_step=10, seed = 2024)\n    theta_hat = geospaNN.theta_update(mlp_nngls(data_train.x).squeeze() - data_train.y,\n                                      data_train.pos, neighbor_size = 20)\n    model = geospaNN.nngls(p=p, neighbor_size=nn, coord_dimensions=2, mlp=mlp_nngls, theta=torch.tensor(theta_hat))\n    predict_nngls = model.predict(data_train, data_test)\n\n    torch.manual_seed(2024)\n    np.random.seed(0)\n    data_add_train, data_add_val, data_add_test = geospaNN.split_data(torch.concat([X, coord], axis = 1), Y, coord, neighbor_size=nn,\n                                                                      test_proportion=0.2)\n    mlp_nn_add = torch.nn.Sequential(\n        torch.nn.Linear(p+2, 50),\n        torch.nn.ReLU(),\n        torch.nn.Linear(50, 20),\n        torch.nn.ReLU(),\n        torch.nn.Linear(20, 1)\n    )\n    nn_add_model = geospaNN.nn_train(mlp_nn_add, lr=0.01, min_delta=0.001)\n    training_log = nn_add_model.train(data_add_train, data_add_val, data_add_test, seed = 2024)\n    predict_nn_add = mlp_nn_add(data_add_test.x).detach().numpy().reshape(-1)\n\n    coord_np = coord.detach().numpy()\n    num_basis = [2 ** 2, 4 ** 2, 6 ** 2]\n    knots_1d = [np.linspace(0, 1, int(np.sqrt(i))) for i in num_basis]\n    ##Wendland kernel\n    K = 0\n    phi_temp = np.zeros((n, sum(num_basis)))\n    for res in range(len(num_basis)):\n        theta_temp = 1 / np.sqrt(num_basis[res]) * 2.5\n        knots_s1, knots_s2 = np.meshgrid(knots_1d[res], knots_1d[res])\n        knots = np.column_stack((knots_s1.flatten(), knots_s2.flatten()))\n        for i in range(num_basis[res]):\n            d = np.linalg.norm(coord_np / b - knots[i, :], axis=1) / theta_temp\n            for j in range(len(d)):\n                if d[j] &gt;= 0 and d[j] &lt;= 1:\n                    phi_temp[j, i + K] = (1 - d[j]) ** 6 * (35 * d[j] ** 2 + 18 * d[j] + 3) / 3\n                else:\n                    phi_temp[j, i + K] = 0\n        K = K + num_basis[res]\n\n    torch.manual_seed(2024)\n    np.random.seed(0)\n    data_DK_train, data_DK_val, data_DK_test = geospaNN.split_data(torch.concat([X, torch.from_numpy(phi_temp)], axis = 1).float(), \n                                                                   Y, coord, neighbor_size=nn, test_proportion=0.2)\n    mlp_nn_DK = torch.nn.Sequential(\n        torch.nn.Linear(p+K, 50),\n        torch.nn.ReLU(), \n        torch.nn.Linear(50, 20), \n        torch.nn.ReLU(), \n        torch.nn.Linear(20, 1)) \n    nn_DK_model = geospaNN.nn_train(mlp_nn_DK, lr=0.05, min_delta=0.001) \n    training_log = nn_DK_model.train(data_DK_train, data_DK_val, data_DK_test, seed = 2024) \n    predict_DK = mlp_nn_DK(data_DK_test.x).detach().numpy().reshape(-1)\n\n\n    MSE_nngls.append(torch.mean((data_test.y - predict_nngls)**2))\n    MSE_nn.append(torch.mean((data_test.y - estimate_nn)**2))\n    MSE_nnkrig.append(torch.mean((data_test.y - predict_nn)**2))\n    MSE_nnadd.append(torch.mean((data_test.y - predict_nn_add)**2))\n    MSE_nnDK.append(torch.mean((data_test.y - predict_DK)**2))\n</code></pre> <pre><code>Epoch 00061: reducing learning rate of group 0 to 5.0000e-03.\nEpoch 00068: reducing learning rate of group 0 to 2.5000e-03.\nINFO: Early stopping\nEnd at epoch71\n---------------------------------------- \n    Ordering Coordinates \n----------------------------------------\n    Model description\n----------------------------------------\nBRISC model fit with 600 observations.\n\nNumber of covariates 1 (including intercept if specified).\n\nUsing the exponential spatial correlation model.\n\nUsing 15 nearest neighbors.\n\n\n\nSource not compiled with OpenMP support.\n----------------------------------------\n    Building neighbor index\n----------------------------------------\n    Performing optimization\n----------------------------------------\n    Processing optimizers\n----------------------------------------\nTheta estimated as\n[3.8977135  0.48003391 0.01919864]\n...\n</code></pre> <pre><code>df_MSE = pd.DataFrame(\n    {'NN estimate': np.array(MSE_nn), 'NN+kriging': np.array(MSE_nnkrig), \n     'NNGLS': np.array(MSE_nngls), 'NN-add-coords': np.array(MSE_nnadd),\n     'NN-DeepKrig': np.array(MSE_nnDK),\n     'n': n_vec}\n)\ndf_MSE\n</code></pre> NN estimate NN+kriging NNGLS NN-add-coords NN-DeepKrig n 0 2.983742 0.523556 0.454370 3.699407 2.003776 1000 1 2.921750 0.460017 0.453934 3.302231 1.425005 2000 2 3.181067 0.370218 0.507125 2.772930 1.713040 5000 3 2.984987 0.291051 0.293493 3.140883 1.588460 10000 <pre><code>plt.figure(figsize=(8, 6))\nplt.plot(df_MSE['n'], df_MSE['NN estimate'], label='NN estimate', linestyle='-', marker='s')\nplt.plot(df_MSE['n'], df_MSE['NN+kriging'], label='NN+kriging', linestyle='-', marker='s')\nplt.plot(df_MSE['n'], df_MSE['NNGLS'], label='NNGLS', linestyle='-', marker='s')\nplt.plot(df_MSE['n'], df_MSE['NN-add-coords'], label='NN-add-coords', linestyle='-', marker='s')\nplt.plot(df_MSE['n'], df_MSE['NN-DeepKrig'], label='NN-DeepKrig', linestyle='-', marker='s')\n\n# Add titles and labels\nplt.title('MSE vs sample size', fontsize=14)\nplt.xlabel('Log 10 of sample size', fontsize=12)\nplt.ylabel('Log 10 Mean Squared Error (MSE)', fontsize=12)\nplt.xscale('log')\nplt.yscale('log')\nplt.legend(fontsize=12)\nplt.grid(True)\n\n# Show the plot\nplt.tight_layout()\nplt.savefig(path + \"MSE_vs_samplesize.png\")\n</code></pre> <pre><code>\n</code></pre>"},{"location":"Example_architecture/Example_architecture/","title":"Example architecture","text":"<pre><code>import torch\nimport geospaNN\nimport numpy as np\nimport time\nimport pandas as pd\nimport seaborn as sns\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n\npath = '../data/Output/'\n</code></pre> <pre><code>def f1(X): return 10 * np.sin(np.pi * 2 * X)\n\n\nsigma = 1\nphi = 0.1\ntau = 0.01\ntheta = torch.tensor([sigma, phi / np.sqrt(2), tau])\n\np = 1;\nfunXY = f1\n\nn = 1000\nnn = 20\nbatch_size = 50\n\ntorch.manual_seed(2025)\n_, _, _, _, X = geospaNN.Simulation(n, p, nn, funXY, torch.tensor([1, 5, 0.01]), range=[0, 1])\nX = X.reshape(-1,1)\nX = (X - X.min())/(X.max() - X.min())\ntorch.manual_seed(2025)\n_, _, coord, cov, corerr = geospaNN.Simulation(n, p, nn, funXY, theta, range=[0, 1])\nY = funXY(X).reshape(-1) + corerr\n\ntorch.manual_seed(2024)\nnp.random.seed(0)\ndata_train, data_val, data_test = geospaNN.split_data(X, Y, coord, neighbor_size=nn,\n                                                      test_proportion=0.2)\n</code></pre> <pre><code>torch.manual_seed(2025)\nmlp_nn = torch.nn.Sequential(\n    torch.nn.Linear(p, 5),\n    torch.nn.ReLU(),\n    torch.nn.Linear(5, 1)\n)\ntrainer_nn = geospaNN.nn_train(mlp_nn, lr=0.01, min_delta=0.001)\ntraining_log = trainer_nn.train(data_train, data_val, data_test)\ntheta0 = geospaNN.theta_update(mlp_nn(data_train.x).squeeze() - data_train.y, data_train.pos, neighbor_size=20)\n</code></pre> <pre><code>Epoch 00081: reducing learning rate of group 0 to 5.0000e-03.\nINFO: Early stopping\nEnd at epoch84\n---------------------------------------- \n    Ordering Coordinates \n----------------------------------------\n    Model description\n----------------------------------------\nBRISC model fit with 600 observations.\n\nNumber of covariates 1 (including intercept if specified).\n\nUsing the exponential spatial correlation model.\n\nUsing 15 nearest neighbors.\n\n\n\nSource not compiled with OpenMP support.\n----------------------------------------\n    Building neighbor index\n----------------------------------------\n    Performing optimization\n----------------------------------------\n    Processing optimizers\n----------------------------------------\nTheta estimated as\n[ 4.46885169 29.18811903  0.06182564]\n</code></pre> <pre><code>torch.manual_seed(2025)\nmlp_linear = torch.nn.Sequential(\n    torch.nn.Linear(p, 1),\n)\nmodel_linear = geospaNN.nngls(p=p, neighbor_size=nn, coord_dimensions=2, mlp=mlp_linear, theta = torch.tensor(theta0))\ntrainer_linear = geospaNN.nngls_train(model_linear, lr=0.01, min_delta=0.001)\ntraining_log = trainer_linear.train(data_train, data_val, data_test,\n                                        Update_init=10, Update_step=5)\n</code></pre> <pre><code>---------------------------------------- \n    Ordering Coordinates \n----------------------------------------\n    Model description\n----------------------------------------\nBRISC model fit with 600 observations.\n\nNumber of covariates 1 (including intercept if specified).\n\nUsing the exponential spatial correlation model.\n\nUsing 15 nearest neighbors.\n\n\n\nSource not compiled with OpenMP support.\n----------------------------------------\n    Building neighbor index\n----------------------------------------\n    Performing optimization\n----------------------------------------\n    Processing optimizers\n----------------------------------------\nTheta estimated as\n[3.66738529e+01 1.28390384e+01 1.00000000e-03]\n...\n</code></pre> <pre><code>torch.manual_seed(2025)\nmlp_l1_n5 = torch.nn.Sequential(\n    torch.nn.Linear(p, 5),\n    torch.nn.ReLU(),\n    torch.nn.Linear(5, 1)\n)\nnngls_l1_n5 = geospaNN.nngls(p=p, neighbor_size=nn, coord_dimensions=2, mlp=mlp_l1_n5, theta=torch.tensor(theta0))\ntrainer_l1_n5 = geospaNN.nngls_train(nngls_l1_n5, lr=0.1, min_delta=0.001)\ntraining_log = trainer_l1_n5.train(data_train, data_val, data_test,\n                                   Update_init=20, Update_step=5, seed = 2024)\n</code></pre> <pre><code>Epoch 00011: reducing learning rate of group 0 to 5.0000e-02.\nINFO: Early stopping\nEnd at epoch14\n</code></pre> <pre><code>torch.manual_seed(2025)\nmlp_l1_n20 = torch.nn.Sequential(\n    torch.nn.Linear(p, 20),\n    torch.nn.ReLU(),\n    torch.nn.Linear(20, 1)\n)\nnngls_l1_n20 = geospaNN.nngls(p=p, neighbor_size=nn, coord_dimensions=2, mlp=mlp_l1_n20, \n                              theta=torch.tensor(theta0))\ntrainer_l1_n20 = geospaNN.nngls_train(nngls_l1_n20, lr=0.01, min_delta=0.001)\ntraining_log = trainer_l1_n20.train(data_train, data_val, data_test,\n                                    Update_init=20, Update_step=5, seed = 2024)\n</code></pre> <pre><code>---------------------------------------- \n    Ordering Coordinates \n----------------------------------------\n    Model description\n----------------------------------------\nBRISC model fit with 600 observations.\n\nNumber of covariates 1 (including intercept if specified).\n\nUsing the exponential spatial correlation model.\n\nUsing 15 nearest neighbors.\n\n\n\nSource not compiled with OpenMP support.\n----------------------------------------\n    Building neighbor index\n----------------------------------------\n    Performing optimization\n----------------------------------------\n    Processing optimizers\n----------------------------------------\nTheta estimated as\n[5.88382895e+00 3.57918116e+01 2.09636078e-03]\n...\n</code></pre> <pre><code>torch.manual_seed(2025)\nmlp_l1_n50 = torch.nn.Sequential(\n    torch.nn.Linear(p, 50),\n    torch.nn.ReLU(),\n    torch.nn.Linear(50, 1)\n)\nnngls_l1_n50 = geospaNN.nngls(p=p, neighbor_size=nn, coord_dimensions=2, mlp=mlp_l1_n50, \n                              theta=torch.tensor(theta0))\ntrainer_l1_n50 = geospaNN.nngls_train(nngls_l1_n50, lr=0.1, min_delta=0.001)\ntraining_log = trainer_l1_n50.train(data_train, data_val, data_test,\n                                    Update_init=10, Update_step=5, seed = 2024)\n</code></pre> <pre><code>---------------------------------------- \n    Ordering Coordinates \n----------------------------------------\n    Model description\n----------------------------------------\nBRISC model fit with 600 observations.\n\nNumber of covariates 1 (including intercept if specified).\n\nUsing the exponential spatial correlation model.\n\nUsing 15 nearest neighbors.\n\n\n\nSource not compiled with OpenMP support.\n----------------------------------------\n    Building neighbor index\n----------------------------------------\n    Performing optimization\n----------------------------------------\n    Processing optimizers\n----------------------------------------\nTheta estimated as\n[ 2.74893171 16.64279543  0.09969586]\n...\nINFO: Early stopping\nEnd at epoch33\n</code></pre> <pre><code>torch.manual_seed(2025)\nmlp_l2_n5_2 = torch.nn.Sequential(\n    torch.nn.Linear(p, 5),\n    torch.nn.ReLU(),\n    torch.nn.Linear(5, 2),\n    torch.nn.ReLU(),\n    torch.nn.Linear(2, 1)\n)\nnngls_l2_n5_2 = geospaNN.nngls(p=p, neighbor_size=nn, coord_dimensions=2, mlp=mlp_l2_n5_2, theta=torch.tensor(theta0))\ntrainer_l2_n5_2 = geospaNN.nngls_train(nngls_l2_n5_2, lr=0.1, min_delta=0.001)\ntraining_log = trainer_l2_n5_2.train(data_train, data_val, data_test,\n                                     Update_init=10, Update_step=5, seed = 2024)\n</code></pre> <pre><code>---------------------------------------- \n    Ordering Coordinates \n----------------------------------------\n    Model description\n----------------------------------------\nBRISC model fit with 600 observations.\n\nNumber of covariates 1 (including intercept if specified).\n\nUsing the exponential spatial correlation model.\n\nUsing 15 nearest neighbors.\n\n\n\nSource not compiled with OpenMP support.\n----------------------------------------\n    Building neighbor index\n----------------------------------------\n    Performing optimization\n----------------------------------------\n    Processing optimizers\n----------------------------------------\nTheta estimated as\n[ 1.58066505 32.42580587  0.19364144]\n...\nEpoch 00041: reducing learning rate of group 0 to 1.2500e-02.\nINFO: Early stopping\nEnd at epoch44\n</code></pre> <pre><code>torch.manual_seed(2025)\nmlp_l2_n50_20 = torch.nn.Sequential(\n    torch.nn.Linear(p, 50),\n    torch.nn.ReLU(),\n    torch.nn.Linear(50, 20),\n    torch.nn.ReLU(),\n    torch.nn.Linear(20, 1)\n)\nnngls_l2_n50_20 = geospaNN.nngls(p=p, neighbor_size=nn, coord_dimensions=2, mlp=mlp_l2_n50_20, theta=torch.tensor(theta0))\ntrainer_l2_n50_20 = geospaNN.nngls_train(nngls_l2_n50_20, lr=0.1, min_delta=0.001)\ntraining_log = trainer_l2_n50_20.train(data_train, data_val, data_test,\n                                       Update_init=10, Update_step=5, seed = 2024)\n</code></pre> <pre><code>---------------------------------------- \n    Ordering Coordinates \n----------------------------------------\n    Model description\n----------------------------------------\nBRISC model fit with 600 observations.\n\nNumber of covariates 1 (including intercept if specified).\n\nUsing the exponential spatial correlation model.\n\nUsing 15 nearest neighbors.\n\n\n\nSource not compiled with OpenMP support.\n----------------------------------------\n    Building neighbor index\n----------------------------------------\n    Performing optimization\n----------------------------------------\n    Processing optimizers\n----------------------------------------\nTheta estimated as\n[ 0.84932386 21.7438003   0.12269381]\n...\nEpoch 00036: reducing learning rate of group 0 to 2.5000e-02.\nINFO: Early stopping\nEnd at epoch39\n</code></pre> <pre><code>plt.clf()\nplt.scatter(X.detach().numpy(), Y.detach().numpy(), s=1, label='data')\nplt.scatter(X.detach().numpy(), funXY(X.detach().numpy()), s=1, label='f(x)')\nplt.scatter(X.detach().numpy(), mlp_l1_n5(X).detach().numpy(), s=1, label='NNGLS 1 layer 5 nodes')\n#plt.scatter(X.detach().numpy(), mlp_l1_n20(X).detach().numpy(), s=1, label='NNGLS 1 layer 20 nodes')\n#plt.scatter(X.detach().numpy(), mlp_l1_n50(X).detach().numpy(), s=1, label='NNGLS 1 layer 50 nodes')\nlgnd = plt.legend()\nfor handle in lgnd.legend_handles:\n    handle.set_sizes([10.0])\nplt.savefig(path + 'l1n5.png')\n</code></pre> <pre><code>plt.clf()\nplt.scatter(X.detach().numpy(), Y.detach().numpy(), s=1, label='data')\nplt.scatter(X.detach().numpy(), funXY(X.detach().numpy()), s=1, label='f(x)')\n#plt.scatter(X.detach().numpy(), mlp_l1_n5(X).detach().numpy(), s=1, label='NNGLS 1 layer 5 nodes')\nplt.scatter(X.detach().numpy(), mlp_l1_n20(X).detach().numpy(), s=1, label='NNGLS 1 layer 20 nodes')\n#plt.scatter(X.detach().numpy(), mlp_l1_n50(X).detach().numpy(), s=1, label='NNGLS 1 layer 50 nodes')\nlgnd = plt.legend()\nfor handle in lgnd.legend_handles:\n    handle.set_sizes([10.0])\nplt.savefig(path + 'l1n20.png')\n</code></pre> <pre><code>plt.clf()\nplt.scatter(X.detach().numpy(), Y.detach().numpy(), s=1, label='data')\nplt.scatter(X.detach().numpy(), funXY(X.detach().numpy()), s=1, label='f(x)')\n#plt.scatter(X.detach().numpy(), mlp_l1_n5(X).detach().numpy(), s=1, label='NNGLS 1 layer 5 nodes')\n#plt.scatter(X.detach().numpy(), mlp_l1_n20(X).detach().numpy(), s=1, label='NNGLS 1 layer 20 nodes')\nplt.scatter(X.detach().numpy(), mlp_l1_n50(X).detach().numpy(), s=1, label='NNGLS 1 layer 50 nodes')\nlgnd = plt.legend()\nfor handle in lgnd.legend_handles:\n    handle.set_sizes([10.0])\nplt.savefig(path + 'l1n50.png')\n</code></pre> <pre><code>plt.clf()\nplt.scatter(X.detach().numpy(), Y.detach().numpy(), s=1, label='data')\nplt.scatter(X.detach().numpy(), funXY(X.detach().numpy()), s=1, label='f(x)')\nplt.scatter(X.detach().numpy(), mlp_l2_n5_2(X).detach().numpy(), s=1, label='NNGLS 2 layers 5 2 nodes')\n#plt.scatter(X.detach().numpy(), mlp_l2_n50_20(X).detach().numpy(), s=1, label='NNGLS 2 layers 50 20 nodes')\nlgnd = plt.legend()\nfor handle in lgnd.legend_handles:\n    handle.set_sizes([10.0])\nplt.savefig(path + 'l2n5_2.png')\n</code></pre> <pre><code>plt.clf()\nplt.scatter(X.detach().numpy(), Y.detach().numpy(), s=1, label='data')\nplt.scatter(X.detach().numpy(), funXY(X.detach().numpy()), s=1, label='f(x)')\n#plt.scatter(X.detach().numpy(), mlp_l2_n5_2(X).detach().numpy(), s=1, label='NNGLS 2 layers 5 2 nodes')\nplt.scatter(X.detach().numpy(), mlp_l2_n50_20(X).detach().numpy(), s=1, label='NNGLS 2 layers 50 20 nodes')\nlgnd = plt.legend()\nfor handle in lgnd.legend_handles:\n    handle.set_sizes([10.0])\nplt.savefig(path + 'l2n50_20.png')\n</code></pre> <pre><code>plt.clf()\nplt.scatter(X.detach().numpy(), Y.detach().numpy(), s=1, label='data')\nplt.scatter(X.detach().numpy(), funXY(X.detach().numpy()), s=1, label='f(x)')\nplt.scatter(X.detach().numpy(), mlp_l1_n5(X).detach().numpy(), s=1, label='NNGLS 1 layer 5 nodes')\nplt.scatter(X.detach().numpy(), mlp_l2_n5_2(X).detach().numpy(), s=1, label='NNGLS 2 layers 5 2 nodes')\nplt.scatter(X.detach().numpy(), mlp_l1_n50(X).detach().numpy(), s=1, label='NNGLS 1 layer 50 nodes')\nplt.scatter(X.detach().numpy(), mlp_l2_n50_20(X).detach().numpy(), s=1, label='NNGLS 2 layers 50 20 nodes')\nlgnd = plt.legend()\nfor handle in lgnd.legend_handles:\n    handle.set_sizes([10.0])\nplt.show()\n</code></pre> <pre><code>\n</code></pre> <pre><code>\n</code></pre>"},{"location":"Example_utils/Example_utils/","title":"Example utils","text":"<pre><code>import torch\nimport geospaNN\nimport numpy as np\nimport time\nimport pandas as pd\nimport seaborn as sns\nimport random\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n\npath = '../data/Output/'\n</code></pre> <pre><code>def f5(X): return (10 * np.sin(np.pi * X[:, 0] * X[:, 1]) + 20 * (X[:, 2] - 0.5) ** 2 + 10 * X[:, 3] + 5 * X[:, 4]) / 6\n\n\ndef f1(X): return 10 * np.sin(2*np.pi * X)\n\np = 1;\nfunXY = f1\n\nn = 1000\nnn = 20\nbatch_size = 50\n\nsigma = 1\nphi = 0.3\ntau = 0.01\ntheta = torch.tensor([sigma, phi / np.sqrt(2), tau])\n\nX, Y, coord, cov, corerr = geospaNN.Simulation(n, p, nn, funXY, theta, range=[0, 1])\n</code></pre> <pre><code>torch.manual_seed(2025)\n_, _, _, _, X = geospaNN.Simulation(n, p, nn, funXY, torch.tensor([1, 5, 0.01]), range=[0, 1])\nX = X.reshape(-1,p)\nX = (X - X.min())/(X.max() - X.min())\ntorch.manual_seed(2025)\n_, _, coord, cov, corerr = geospaNN.Simulation(n, p, nn, funXY, theta, range=[0, 1])\nY = funXY(X).reshape(-1) + corerr\n</code></pre> <pre><code>dict = {\"Covariate\": X, \"Response\": Y, \"Spatial_effect\":corerr}\n\nfor index, (name, variable) in enumerate(dict.items()):\n    geospaNN.spatial_plot_surface(variable.detach().numpy().reshape(-1), coord.detach().numpy(),\n                             grid_resolution = 50, method = \"CloughTocher\", cmap = \"RdBu\",\n                             title = name, save_path = path, file_name = name + \"_RdBu.png\")\n</code></pre> <pre><code>&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n&lt;Figure size 800x600 with 0 Axes&gt;\n\n\n\n&lt;Figure size 800x600 with 0 Axes&gt;\n</code></pre> <pre><code>plt.figure(figsize=(6, 6))\nsc = plt.scatter(coord[:,0], coord[:,1], c = range(n), cmap='viridis', s=50, edgecolor='k')\n\n# Customize the plot\nplt.title('Random order of spots', fontsize=16)\nplt.xlabel('coord X', fontsize=12)\nplt.ylabel('coord Y', fontsize=12)\nplt.grid(alpha=0.3)\nplt.savefig(path + \"order_none.png\")\n</code></pre> <pre><code>_, _, _, order = geospaNN.spatial_order(X, Y, coord, method='coord-sum')\nplt.figure(figsize=(6, 6))\nsc = plt.scatter(coord[order,0], coord[order,1], c = range(n), cmap='viridis', s=50, edgecolor='k')\n# Customize the plot\nplt.title('Order of spots by coordinates sum', fontsize=16)\nplt.xlabel('coord X', fontsize=12)\nplt.ylabel('coord Y', fontsize=12)\nplt.grid(alpha=0.3)\nplt.savefig(path + \"order_coord-sum.png\")\n</code></pre> <pre><code>random.seed(2024)\n_, _, _, order = geospaNN.spatial_order(X, Y, coord, method='max-min')\nplt.figure(figsize=(7.5, 6))\nsc = plt.scatter(coord[order,0], coord[order,1], c = range(n), cmap='viridis', s=50, edgecolor='k')\ncbar = plt.colorbar(sc)\ncbar.set_label('Color', fontsize=12)\n# Customize the plot\nplt.title('Order of spots by coordinates max-min', fontsize=16)\nplt.xlabel('coord X', fontsize=12)\nplt.ylabel('coord Y', fontsize=12)\nplt.grid(alpha=0.3)\nplt.savefig(path + \"order_max-min.png\")\n</code></pre> <pre><code>data = geospaNN.make_graph(X, Y, coord, nn, Ind_list = None)\n\ntorch.manual_seed(2024)\nnp.random.seed(0)\ndata_train, data_val, data_test = geospaNN.split_data(X, Y, coord, neighbor_size=nn, \n                                                      test_proportion=0.2, val_proportion=0.2)\n</code></pre> <pre><code>torch.manual_seed(2025)\n_, _, coord_simp, _, corerr_simp = geospaNN.Simulation(n, p, nn, funXY, \n                                                       torch.tensor([1,1.5,0.01]), range=[0, 10])\ntheta_hat = geospaNN.theta_update(corerr_simp, coord_simp, neighbor_size=20)\n</code></pre> <pre><code>---------------------------------------- \n    Ordering Coordinates \n----------------------------------------\n    Model description\n----------------------------------------\nBRISC model fit with 1000 observations.\n\nNumber of covariates 1 (including intercept if specified).\n\nUsing the exponential spatial correlation model.\n\nUsing 15 nearest neighbors.\n\n\n\nSource not compiled with OpenMP support.\n----------------------------------------\n    Building neighbor index\n----------------------------------------\n    Performing optimization\n----------------------------------------\n    Processing optimizers\n----------------------------------------\nTheta estimated as\n[0.88942914 1.74742522 0.01030131]\n</code></pre> <pre><code>torch.manual_seed(2024)\nmlp_nn = torch.nn.Sequential(\n    torch.nn.Linear(p, 100),\n    torch.nn.ReLU(),\n    torch.nn.Linear(100, 50),\n    torch.nn.ReLU(),\n    torch.nn.Linear(50, 20),\n    torch.nn.ReLU(),\n    torch.nn.Linear(20, 1),\n)\ntrainer_nn = geospaNN.nn_train(mlp_nn, lr=0.01, min_delta=0.001)\ntraining_log = trainer_nn.train(data_train, data_val, data_test, seed = 2025)\ntheta0 = geospaNN.theta_update(mlp_nn(data_train.x).squeeze() - data_train.y, \n                               data_train.pos, neighbor_size=20)\n</code></pre> <pre><code>Epoch 00030: reducing learning rate of group 0 to 5.0000e-03.\nINFO: Early stopping\nEnd at epoch33\n---------------------------------------- \n    Ordering Coordinates \n----------------------------------------\n    Model description\n----------------------------------------\nBRISC model fit with 600 observations.\n\nNumber of covariates 1 (including intercept if specified).\n\nUsing the exponential spatial correlation model.\n\nUsing 15 nearest neighbors.\n\n\n\nSource not compiled with OpenMP support.\n----------------------------------------\n    Building neighbor index\n----------------------------------------\n    Performing optimization\n----------------------------------------\n    Processing optimizers\n----------------------------------------\nTheta estimated as\n[0.182681   6.67337675 0.13378453]\n</code></pre> <pre><code>torch.manual_seed(2024)\nmlp_nngls = torch.nn.Sequential(\n    torch.nn.Linear(p, 100),\n    torch.nn.ReLU(),\n    torch.nn.Linear(100, 50),\n    torch.nn.ReLU(),\n    torch.nn.Linear(50, 20),\n    torch.nn.ReLU(),\n    torch.nn.Linear(20, 1),\n)\nmodel = geospaNN.nngls(p=p, neighbor_size=nn, coord_dimensions=2, mlp=mlp_nngls, \n                       theta=torch.tensor(theta0))\ntrainer_nngls = geospaNN.nngls_train(model, lr=0.1, min_delta=0.001)\ntraining_log = trainer_nngls.train(data_train, data_val, data_test, epoch_num= 200, \n                                   Update_init=10, Update_step=2, seed = 2025)\ntheta1 = geospaNN.theta_update(mlp_nngls(data_train.x).squeeze() - data_train.y,\n                               data_train.pos, neighbor_size=20)\n</code></pre> <pre><code>---------------------------------------- \n    Ordering Coordinates \n----------------------------------------\n    Model description\n----------------------------------------\nBRISC model fit with 600 observations.\n\nNumber of covariates 1 (including intercept if specified).\n\nUsing the exponential spatial correlation model.\n\nUsing 15 nearest neighbors.\n\n\n\nSource not compiled with OpenMP support.\n----------------------------------------\n    Building neighbor index\n----------------------------------------\n    Performing optimization\n----------------------------------------\n    Processing optimizers\n----------------------------------------\nTheta estimated as\n[ 1.24232833 15.51145856  0.07333078]\n...\n</code></pre> <pre><code>estimate = model.estimate(X)\nplt.clf()\nplt.scatter(X.detach().numpy(), Y.detach().numpy(), s=1, label='data')\nplt.scatter(X.detach().numpy(), funXY(X.detach().numpy()), s=1, label='f(x)')\nplt.scatter(X.detach().numpy(), estimate, s=1, label='NNGLS')\nplt.scatter(X.detach().numpy(), mlp_nn(X).detach().numpy(), s=1, label='NN')\nlgnd = plt.legend()\nfor handle in lgnd.legend_handles:\n    handle.set_sizes([10.0])\nplt.savefig(path + 'Estimation.png')\n</code></pre> <pre><code>[test_predict, test_CI_U, test_CI_L] = model.predict(data_train, data_test, CI = True)\nx_np = data_test.x.detach().numpy().reshape(-1)\nx_smooth = np.linspace(x_np.min(), x_np.max(), 200)  # Create finer x-points\ndegree = 4\nU_fit = np.polyfit(x_np, test_CI_U, degree)\nL_fit = np.polyfit(x_np, test_CI_L, degree)\nPred_fit = np.polyfit(x_np, test_predict, degree)\n\n# Evaluate the polynomial on a smooth grid\ny_smooth_U = np.polyval(U_fit, x_smooth)\ny_smooth_L = np.polyval(L_fit, x_smooth)\ny_smooth = np.polyval(Pred_fit, x_smooth)\n\nplt.clf()\nplt.scatter(data_test.x.detach().numpy(), data_test.y.detach().numpy(), s=1, label='data')\nplt.scatter(data_test.x.detach().numpy(), funXY(data_test.x.detach().numpy()), s=1, label='f(x)')\nplt.scatter(data_test.x.detach().numpy(), test_predict.detach().numpy(), s=1, label='NNGLS prediction')\n#plt.plot(x_smooth, y_smooth_U, linestyle='--', label='NNGLS CI_U', color = 'red', alpha = 0.5)\n#plt.plot(x_smooth, y_smooth_L, linestyle='--', label='NNGLS CI_L', color = 'red', alpha = 0.5)\nplt.xlabel(\"X\", fontsize=15)\nplt.ylabel(\"Y\", fontsize=15)\nlgnd = plt.legend()\nfor handle in lgnd.legend_handles[:3]:\n    handle.set_sizes([10.0])\nplt.savefig(path + \"Prediction_no_CI.png\")\n</code></pre> <pre><code>plt.clf()\nplt.scatter(data_test.x.detach().numpy(), data_test.y.detach().numpy(), s=1, label='data')\nplt.scatter(data_test.x.detach().numpy(), funXY(data_test.x.detach().numpy()), s=1, label='f(x)')\nplt.scatter(x_smooth, y_smooth, s=1, label='NNGLS prediction')\nplt.plot(x_smooth, y_smooth_U, linestyle='--', label='NNGLS CI_U', color = 'red', alpha = 0.5)\nplt.plot(x_smooth, y_smooth_L, linestyle='--', label='NNGLS CI_L', color = 'red', alpha = 0.5)\nplt.xlabel(\"X\", fontsize=15)\nplt.ylabel(\"Y\", fontsize=15)\nlgnd = plt.legend()\nfor handle in lgnd.legend_handles[:3]:\n    handle.set_sizes([10.0])\nplt.savefig(path + \"Prediction_smoothed_CI.png\")\n</code></pre> <pre><code>x_np = data_test.y.detach().numpy().reshape(-1)\ndegree = 4\nx_smooth = np.linspace(x_np.min(), x_np.max(), 200)  # Create finer x-points\nU_fit = np.polyfit(x_np, test_CI_U, degree)\nL_fit = np.polyfit(x_np, test_CI_L, degree)\n# Evaluate the polynomial on a smooth grid\ny_smooth_U = np.polyval(U_fit, x_smooth)\ny_smooth_L = np.polyval(L_fit, x_smooth)\n\nplt.clf()\nplt.scatter(data_test.y.detach().numpy(), data_test.y.detach().numpy(), s=1, label='data')\nplt.scatter(data_test.y.detach().numpy(), test_predict.detach().numpy(), s=1, label='NNGLS prediction')\nplt.plot(x_smooth, y_smooth_U, linestyle='--', label='CI Upper', color = 'red', alpha = 0.5)\nplt.plot(x_smooth, y_smooth_L, linestyle='--', label='CI Lower', color = 'red', alpha = 0.5)\nplt.xlabel(\"Prediction\", fontsize=15)\nplt.ylabel(\"Truth\", fontsize=15)\nlgnd = plt.legend()\nfor handle in lgnd.legend_handles[:2]:\n    handle.set_sizes([10.0])\nplt.savefig(path + \"Prediction_vs_no_CI.png\")\n</code></pre> <pre><code>geospaNN.spatial_plot_surface(data_test.y.detach().numpy(), data_test.pos.detach().numpy(),\n                              grid_resolution = 50, method = \"CloughTocher\",\n                              title = \"Y test\", save_path = path, file_name = \"Y_test\" + \".png\")\ngeospaNN.spatial_plot_surface(test_predict.detach().numpy(), data_test.pos.detach().numpy(),\n                              grid_resolution = 50, method = \"CloughTocher\",\n                              title = \"Y test predicted\", save_path = path, file_name = \"Y_test_predicted\" + \".png\")\n</code></pre> <pre><code>&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n&lt;Figure size 800x600 with 0 Axes&gt;\n</code></pre> <pre><code>def plot_log(training_log, theta):\n    epoch = len(training_log[\"val_loss\"])\n    training_log[\"epoch\"] = list(range(1, epoch + 1))\n    training_log = pd.DataFrame(training_log)\n\n    # Melting the dataframe to make it suitable for seaborn plotting\n    training_log_melted = training_log[[\"epoch\", \"val_loss\"]].melt(id_vars='epoch', var_name='Variable', value_name='Value')\n    # Plotting with seaborn\n    # Creating two subplots side by side\n    fig, axes = plt.subplots(1, 2, figsize=(20, 6))\n    sns.lineplot(ax=axes[0], data=training_log_melted, x='epoch', y='Value', hue='Variable', style='Variable', markers=False, dashes=False)\n\n    axes[0].set_title('Validation and prediction loss over Epochs (Log Scale) with Benchmark', fontsize=14)\n    axes[0].set_xlabel('Epoch', fontsize=15)\n    axes[0].set_ylabel('Value (Log Scale)', fontsize=15)\n    axes[0].set_yscale('log')\n    axes[0].legend(prop={'size': 15})\n    axes[0].tick_params(labelsize=14)\n    axes[0].grid(True)\n\n    # Second plot (sigma, phi, tau)\n    kernel_params_melted = training_log[[\"epoch\", \"sigma\", \"phi\", \"tau\"]].melt(id_vars='epoch', var_name='Variable', value_name='Value')\n    ground_truth = {'sigma': theta[0], 'phi': theta[1], 'tau': theta[2]}\n    sns.lineplot(ax=axes[1], data=kernel_params_melted, x='epoch', y='Value', hue='Variable', style='Variable', markers=False, dashes=False)\n    palette = sns.color_palette()\n    for i, (param, gt_value) in enumerate(ground_truth.items()):\n        axes[1].hlines(y=gt_value, xmin=1, xmax=epoch, color=palette[i], linestyle='--')\n    axes[1].set_title('Parameter Values over Epochs with Ground Truth', fontsize=14)\n    axes[1].set_xlabel('Epoch', fontsize=15)\n    axes[1].set_ylabel('Value', fontsize=15)\n    axes[1].set_yscale('log')\n    axes[1].legend(prop={'size': 15})\n    axes[1].tick_params(labelsize=14)\n    axes[1].grid(True)\n\n    plt.tight_layout()\n\n</code></pre> <pre><code>geospaNN.plot_log(training_log, theta, path, save = True)\n</code></pre> <pre><code>\n</code></pre>"}]}